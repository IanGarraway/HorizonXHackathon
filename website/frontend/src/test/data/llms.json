[
  {
    "id": 0,
    "name": "Aya 23",
    "organization": "Cohere",
    "description": "Aya 23 is an open weights research release of an instruction fine-tuned model with multilingual capabilities. It focuses on pairing a highly performant pre-trained Command family of models with the recently released Aya Collection. This model supports 23 languages.",
    "createdDate": "May 31, 2024",
    "url": "https://arxiv.org/pdf/2405.15032",
    "modelCard": "https://huggingface.co/CohereForAI/aya-23-35B",
    "modality": "text; text",
    "analysis": "Evaluated across 23 languages with the highest results in all tasks and languages compared to other multilingual language models.",
    "size": "35B parameters",
    "dependencies": "",
    "trainingEmissions": "unknown",
    "trainingTime": "unknown",
    "trainingHardware": "TPUv4 chips with up to 128 pod slices",
    "qualityControl": "unknown",
    "access": "Open",
    "license": "CC-BY-NC",
    "intendedUses": "This model is designed to be used for multilingual tasks covering 23 languages.",
    "prohibitedUses": "unknown",
    "monitoring": "unknown",
    "feedback": "https://huggingface.co/CohereForAI/aya-23-35B/discussions",
    "logo": "https://upload.wikimedia.org/wikipedia/en/thumb/0/0c/Cohere_logo.svg/1920px-Cohere_logo.svg.png",
    "scoreX": "58",
    "scoreY": "81"
  },
  {
    "id": 1,
    "name": "Gemini 1.5 Flash",
    "organization": "Google DeepMind",
    "description": "Gemini Flash is a lightweight model, optimized for speed and efficiency. It features multimodal reasoning and a breakthrough long context window of up to one million tokens. It's designed to serve at scale and is efficient on cost, providing quality results at a fraction of the cost of larger models.",
    "createdDate": "May 30, 2024",
    "url": "https://deepmind.google/technologies/gemini/flash/",
    "modelCard": "none",
    "modality": "audio, image, text, video; text",
    "analysis": "The model was evaluated on various benchmarks like General MMLU, Code Natural2Code, MATH, GPQA, Big-Bench, WMT23, MMMU, and MathVista providing performance across various domains like multilingual translation, image processing, and code generation.",
    "size": "unknown",
    "dependencies": "",
    "trainingEmissions": "unknown",
    "trainingTime": "unknown",
    "trainingHardware": "unknown",
    "qualityControl": "The research team is continually exploring new ideas at the frontier of AI and building innovative products for consistent progress.",
    "access": "limited",
    "license": "Googles Terms and Conditions",
    "intendedUses": "The model is intended for developer and enterprise use cases. It can process hours of video and audio, and hundreds of thousands of words or lines of code, making it beneficial for a wide range of tasks.",
    "prohibitedUses": "",
    "monitoring": "unknown",
    "feedback": "none",
    "logo": "https://upload.wikimedia.org/wikipedia/commons/thumb/6/6a/DeepMind_new_logo.svg/1920px-DeepMind_new_logo.svg.png",
    "scoreX": "60",
    "scoreY": "79"
  },
  {
    "id": 2,
    "name": "Aurora",
    "organization": "Microsoft",
    "description": "Aurora is a large-scale foundation model of the atmosphere trained on over a million hours of diverse weather and climate data.",
    "createdDate": "May 28, 2024",
    "url": "https://arxiv.org/pdf/2405.13063",
    "modelCard": "none",
    "modality": "text; climate forecasts",
    "analysis": "Evaluated by comparing climate predictions to actual happened events.",
    "size": "1.3B parameters",
    "dependencies": "",
    "trainingEmissions": "unknown",
    "trainingTime": "unknown",
    "trainingHardware": "32 A100 GPUs",
    "qualityControl": "",
    "access": "closed",
    "license": "unknown",
    "intendedUses": "",
    "prohibitedUses": "",
    "monitoring": "unknown",
    "feedback": "none",
    "logo": "https://upload.wikimedia.org/wikipedia/commons/thumb/9/96/Microsoft_logo_%282012%29.svg/250px-Microsoft_logo_%282012%29.svg.png",
    "scoreX": "72",
    "scoreY": "39"
  },
  {
    "id": 3,
    "name": "GPT-4o",
    "organization": "OpenAI",
    "description": "GPT-4o is OpenAI's new flagship model, as of release, that can reason across audio, vision, and text in real time.",
    "createdDate": "May 13, 2024",
    "url": "https://openai.com/index/hello-gpt-4o/",
    "modelCard": "none",
    "modality": "audio, image, text, video; audio, image, text",
    "analysis": "When evaluated on standard performance benchmarks, achieves similar levels of performance to GPT-4 Turbo.",
    "size": "unknown",
    "dependencies": "",
    "trainingEmissions": "unknown",
    "trainingTime": "unknown",
    "trainingHardware": "unknown",
    "qualityControl": "Training data filtering and post-training refinement act as additional guardrails for preventing harmful outputs.",
    "access": "limited",
    "license": "unknown",
    "intendedUses": "",
    "prohibitedUses": "",
    "monitoring": "Internal monitoring of risk for non-text outputs before a public release (currently only image, text inputs and text outputs are available).",
    "feedback": "none",
    "logo": "https://framerusercontent.com/images/2Nh130axBS9gZnzdi1GSCmkOTcA.png?scale-down-to=1024",
    "scoreX": "44",
    "scoreY": "78"
  },
  {
    "id": 4,
    "name": "DALL·E 3",
    "organization": "OpenAI",
    "description": "DALL·E 3 is an artificial intelligence model that takes a text prompt and/or existing image as an input and generates a new image as an output The model is now in research preview, and will be available to ChatGPT Plus and Enterprise customers in October.",
    "createdDate": "Sep 20, 2023",
    "url": "https://openai.com/dall-e-3",
    "modelCard": "none",
    "modality": "text; image",
    "analysis": "The model is capable of generating explicit content and the researchers found limited amount of spurious content generated.",
    "size": "unknown",
    "dependencies": "DALL·E 2 dataset | CLIP dataset (https://crfm.stanford.edu/ecosystem-graphs/index.html?asset=CLIP%20dataset) | ChatGPT (https://crfm.stanford.edu/ecosystem-graphs/index.html?asset=ChatGPT)",
    "trainingEmissions": "unknown",
    "trainingTime": "unknown",
    "trainingHardware": "unknown",
    "qualityControl": "DALL·E 3 has mitigations to decline requests that ask for a public figure by name. We improved safety performance in risk areas like generation of public figures and harmful biases related to visual over/under-representation, in partnership with red teamers—domain experts who stress-test the model—to help inform our risk assessment and mitigation efforts in areas like propaganda and misinformation.",
    "access": "limited",
    "license": "custom, License information can be found at https://openai.com/policies/terms-of-use",
    "intendedUses": "The intended use of the DALL·E 3 Preview at this time is for personal, non-commercial exploration and research purposes by people who are interested in understanding the potential uses of these capabilities",
    "prohibitedUses": "Use of the model is governed by the OpenAI Content Policy, which prohibits posting of G rated content. Users are not allowed to utilize the model in commercial products in the preview version.",
    "monitoring": "Internal monitoring of risk for non-text outputs before a public release (currently only image, text inputs and text outputs are available).",
    "feedback": "Feedback can be provided at openai.com",
    "logo": "https://framerusercontent.com/images/2Nh130axBS9gZnzdi1GSCmkOTcA.png?scale-down-to=1024",
    "scoreX": "30",
    "scoreY": "29"
  },
  {
    "id": 5,
    "name": "Llama 3",
    "organization": "Meta",
    "description": "Llama 3 is the third generation of Meta AI's open-source large language model. It comes with pretrained and instruction-fine-tuned language models with 8B and 70B parameters that can support a broad range of use cases.",
    "createdDate": "Apr 18, 2024",
    "url": "https://llama.meta.com/llama3/",
    "modelCard": "https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md",
    "modality": "text; text",
    "analysis": "The models were evaluated based on their performance on standard benchmarks and real-world scenarios. These evaluations were performed using a high-quality human evaluation set containing 1,800 prompts covering multiple use cases. The models also went through red-teaming for safety, where human experts and automated methods were used to generate adversarial prompts to test for problematic responses.",
    "size": "70B parameters",
    "dependencies": "",
    "trainingEmissions": "unknown",
    "trainingTime": "unknown",
    "trainingHardware": "2 custom-built Meta 24K GPU clusters",
    "qualityControl": "DALL·E 3 has mitigations to decline requests that ask for a public figure by name. We improved safety performance in risk areas like generation of public figures and harmful biases related to visual over/under-representation, in partnership with red teamers—domain experts who stress-test the model—to help inform our risk assessment and mitigation efforts in areas like propaganda and misinformation.",
    "access": "Open",
    "license": "Llama 3, Can be found at https://github.com/meta-llama/llama3/blob/main/LICENSE",
    "intendedUses": "Llama 3 is intended for a broad range of use cases, including AI assistance, content creation, learning, and analysis.",
    "prohibitedUses": "Unknown",
    "monitoring": "Extensive internal and external performance evaluation and red-teaming approach for safety testing.",
    "feedback": "Feedback is encouraged from users to improve the model, but the feedback mechanism is not explicitly described.",
    "logo": "https://media.licdn.com/dms/image/D4D12AQGSDHcylNVfcA/article-cover_image-shrink_600_2000/0/1713710970597?e=2147483647&v=beta&t=FV__dZLzmCHa6Fm6-eqDzGa4KNLie6MFDC6SQ1FGiQI",
    "scoreX": "85",
    "scoreY": "46"
  }
]
