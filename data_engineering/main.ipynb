{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>organization</th>\n",
       "      <th>description</th>\n",
       "      <th>created_date</th>\n",
       "      <th>url</th>\n",
       "      <th>datasheet</th>\n",
       "      <th>modality</th>\n",
       "      <th>size</th>\n",
       "      <th>sample</th>\n",
       "      <th>...</th>\n",
       "      <th>model_card</th>\n",
       "      <th>training_emissions</th>\n",
       "      <th>training_time</th>\n",
       "      <th>training_hardware</th>\n",
       "      <th>adaptation</th>\n",
       "      <th>output_space</th>\n",
       "      <th>terms_of_service</th>\n",
       "      <th>monthly_active_users</th>\n",
       "      <th>user_distribution</th>\n",
       "      <th>failures</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dataset</td>\n",
       "      <td>ToyMix</td>\n",
       "      <td>Mila-Quebec AI Institute</td>\n",
       "      <td>ToyMix is the smallest dataset of three extens...</td>\n",
       "      <td>2023-10-09</td>\n",
       "      <td>https://arxiv.org/pdf/2310.04292.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>molecules, tasks</td>\n",
       "      <td>13B labels of quantum and biological nature.</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dataset</td>\n",
       "      <td>LargeMix</td>\n",
       "      <td>Mila-Quebec AI Institute</td>\n",
       "      <td>LargeMix is the middle-sized dataset of three ...</td>\n",
       "      <td>2023-10-09</td>\n",
       "      <td>https://arxiv.org/pdf/2310.04292.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>molecules, tasks</td>\n",
       "      <td>13B labels of quantum and biological nature.</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dataset</td>\n",
       "      <td>UltraLarge</td>\n",
       "      <td>Mila-Quebec AI Institute</td>\n",
       "      <td>UltraLarge is the largest dataset of three ext...</td>\n",
       "      <td>2023-10-09</td>\n",
       "      <td>https://arxiv.org/pdf/2310.04292.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>molecules, tasks</td>\n",
       "      <td>13B labels of quantum and biological nature.</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>model</td>\n",
       "      <td>Lag-LLaMA</td>\n",
       "      <td>Morgan Stanley, ServiceNow Research, Universit...</td>\n",
       "      <td>Lag-LLaMA is a general-purpose foundation mode...</td>\n",
       "      <td>2024-02-08</td>\n",
       "      <td>https://time-series-foundation-models.github.i...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>text; text</td>\n",
       "      <td>unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>https://huggingface.co/time-series-foundation-...</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>A single NVIDIA Tesla-P100 GPU</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>model</td>\n",
       "      <td>Prithvi</td>\n",
       "      <td>IBM</td>\n",
       "      <td>Prithvi is a first-of-its-kind temporal Vision...</td>\n",
       "      <td>2023-08-03</td>\n",
       "      <td>https://github.com/NASA-IMPACT/hls-foundation-os</td>\n",
       "      <td>NaN</td>\n",
       "      <td>text, video; text, video</td>\n",
       "      <td>100M parameters (dense)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>https://huggingface.co/ibm-nasa-geospatial/Pri...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>563</th>\n",
       "      <td>model</td>\n",
       "      <td>Amber</td>\n",
       "      <td>LLM360</td>\n",
       "      <td>Amber is the first model in the LLM360 family,...</td>\n",
       "      <td>2023-12-12</td>\n",
       "      <td>https://www.llm360.ai/</td>\n",
       "      <td>NaN</td>\n",
       "      <td>text; text</td>\n",
       "      <td>7B parameters (dense)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>https://huggingface.co/LLM360/Amber</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>56 DGX A100 nodes, each equipped with 4 80GB A...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>model</td>\n",
       "      <td>CrystalCoder</td>\n",
       "      <td>LLM360</td>\n",
       "      <td>CrystalCoder is a language model with a balanc...</td>\n",
       "      <td>2023-12-12</td>\n",
       "      <td>https://www.llm360.ai/</td>\n",
       "      <td>NaN</td>\n",
       "      <td>text; code, text</td>\n",
       "      <td>7B parameters (dense)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>https://huggingface.co/LLM360/CrystalCoder</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Trained on the Cerebras Condor Galaxy 1 (CG-1)...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>application</td>\n",
       "      <td>Duolingo Explain My Answer</td>\n",
       "      <td>Duolingo</td>\n",
       "      <td>Explain My Answer offers learners the chance t...</td>\n",
       "      <td>2023-03-14</td>\n",
       "      <td>https://blog.duolingo.com/duolingo-max/</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.duolingo.com/terms</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>application</td>\n",
       "      <td>Duolingo Max</td>\n",
       "      <td>Duolingo</td>\n",
       "      <td>Duolingo Max is a new subscription tier above ...</td>\n",
       "      <td>2023-03-14</td>\n",
       "      <td>https://blog.duolingo.com/duolingo-max/</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>application</td>\n",
       "      <td>Duolingo Role Play</td>\n",
       "      <td>Duolingo</td>\n",
       "      <td>Roleplay allows learners to practice real-worl...</td>\n",
       "      <td>2023-03-14</td>\n",
       "      <td>https://blog.duolingo.com/duolingo-max/</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.duolingo.com/terms</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>568 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            type                        name  \\\n",
       "0        dataset                      ToyMix   \n",
       "1        dataset                    LargeMix   \n",
       "2        dataset                  UltraLarge   \n",
       "3          model                   Lag-LLaMA   \n",
       "4          model                     Prithvi   \n",
       "..           ...                         ...   \n",
       "563        model                       Amber   \n",
       "564        model                CrystalCoder   \n",
       "565  application  Duolingo Explain My Answer   \n",
       "566  application                Duolingo Max   \n",
       "567  application          Duolingo Role Play   \n",
       "\n",
       "                                          organization  \\\n",
       "0                             Mila-Quebec AI Institute   \n",
       "1                             Mila-Quebec AI Institute   \n",
       "2                             Mila-Quebec AI Institute   \n",
       "3    Morgan Stanley, ServiceNow Research, Universit...   \n",
       "4                                                  IBM   \n",
       "..                                                 ...   \n",
       "563                                             LLM360   \n",
       "564                                             LLM360   \n",
       "565                                           Duolingo   \n",
       "566                                           Duolingo   \n",
       "567                                           Duolingo   \n",
       "\n",
       "                                           description created_date  \\\n",
       "0    ToyMix is the smallest dataset of three extens...   2023-10-09   \n",
       "1    LargeMix is the middle-sized dataset of three ...   2023-10-09   \n",
       "2    UltraLarge is the largest dataset of three ext...   2023-10-09   \n",
       "3    Lag-LLaMA is a general-purpose foundation mode...   2024-02-08   \n",
       "4    Prithvi is a first-of-its-kind temporal Vision...   2023-08-03   \n",
       "..                                                 ...          ...   \n",
       "563  Amber is the first model in the LLM360 family,...   2023-12-12   \n",
       "564  CrystalCoder is a language model with a balanc...   2023-12-12   \n",
       "565  Explain My Answer offers learners the chance t...   2023-03-14   \n",
       "566  Duolingo Max is a new subscription tier above ...   2023-03-14   \n",
       "567  Roleplay allows learners to practice real-worl...   2023-03-14   \n",
       "\n",
       "                                                   url datasheet  \\\n",
       "0                 https://arxiv.org/pdf/2310.04292.pdf       NaN   \n",
       "1                 https://arxiv.org/pdf/2310.04292.pdf       NaN   \n",
       "2                 https://arxiv.org/pdf/2310.04292.pdf       NaN   \n",
       "3    https://time-series-foundation-models.github.i...       NaN   \n",
       "4     https://github.com/NASA-IMPACT/hls-foundation-os       NaN   \n",
       "..                                                 ...       ...   \n",
       "563                             https://www.llm360.ai/       NaN   \n",
       "564                             https://www.llm360.ai/       NaN   \n",
       "565            https://blog.duolingo.com/duolingo-max/       NaN   \n",
       "566            https://blog.duolingo.com/duolingo-max/       NaN   \n",
       "567            https://blog.duolingo.com/duolingo-max/       NaN   \n",
       "\n",
       "                     modality                                          size  \\\n",
       "0            molecules, tasks  13B labels of quantum and biological nature.   \n",
       "1            molecules, tasks  13B labels of quantum and biological nature.   \n",
       "2            molecules, tasks  13B labels of quantum and biological nature.   \n",
       "3                  text; text                                       unknown   \n",
       "4    text, video; text, video                       100M parameters (dense)   \n",
       "..                        ...                                           ...   \n",
       "563                text; text                         7B parameters (dense)   \n",
       "564          text; code, text                         7B parameters (dense)   \n",
       "565                       NaN                                           NaN   \n",
       "566                       NaN                                           NaN   \n",
       "567                       NaN                                           NaN   \n",
       "\n",
       "    sample  ...                                         model_card  \\\n",
       "0       []  ...                                                NaN   \n",
       "1       []  ...                                                NaN   \n",
       "2       []  ...                                                NaN   \n",
       "3      NaN  ...  https://huggingface.co/time-series-foundation-...   \n",
       "4      NaN  ...  https://huggingface.co/ibm-nasa-geospatial/Pri...   \n",
       "..     ...  ...                                                ...   \n",
       "563    NaN  ...                https://huggingface.co/LLM360/Amber   \n",
       "564    NaN  ...         https://huggingface.co/LLM360/CrystalCoder   \n",
       "565    NaN  ...                                                NaN   \n",
       "566    NaN  ...                                                NaN   \n",
       "567    NaN  ...                                                NaN   \n",
       "\n",
       "    training_emissions training_time  \\\n",
       "0                  NaN           NaN   \n",
       "1                  NaN           NaN   \n",
       "2                  NaN           NaN   \n",
       "3              unknown       unknown   \n",
       "4                  NaN           NaN   \n",
       "..                 ...           ...   \n",
       "563            unknown       unknown   \n",
       "564            unknown       unknown   \n",
       "565                NaN           NaN   \n",
       "566                NaN           NaN   \n",
       "567                NaN           NaN   \n",
       "\n",
       "                                     training_hardware adaptation  \\\n",
       "0                                                  NaN        NaN   \n",
       "1                                                  NaN        NaN   \n",
       "2                                                  NaN        NaN   \n",
       "3                       A single NVIDIA Tesla-P100 GPU        NaN   \n",
       "4                                                  NaN        NaN   \n",
       "..                                                 ...        ...   \n",
       "563  56 DGX A100 nodes, each equipped with 4 80GB A...        NaN   \n",
       "564  Trained on the Cerebras Condor Galaxy 1 (CG-1)...        NaN   \n",
       "565                                                NaN        NaN   \n",
       "566                                                NaN        NaN   \n",
       "567                                                NaN        NaN   \n",
       "\n",
       "    output_space                terms_of_service monthly_active_users  \\\n",
       "0            NaN                             NaN                  NaN   \n",
       "1            NaN                             NaN                  NaN   \n",
       "2            NaN                             NaN                  NaN   \n",
       "3            NaN                             NaN                  NaN   \n",
       "4            NaN                             NaN                  NaN   \n",
       "..           ...                             ...                  ...   \n",
       "563          NaN                             NaN                  NaN   \n",
       "564          NaN                             NaN                  NaN   \n",
       "565          NaN  https://www.duolingo.com/terms                  NaN   \n",
       "566          NaN                             NaN                  NaN   \n",
       "567          NaN  https://www.duolingo.com/terms                  NaN   \n",
       "\n",
       "    user_distribution failures  \n",
       "0                 NaN      NaN  \n",
       "1                 NaN      NaN  \n",
       "2                 NaN      NaN  \n",
       "3                 NaN      NaN  \n",
       "4                 NaN      NaN  \n",
       "..                ...      ...  \n",
       "563               NaN      NaN  \n",
       "564               NaN      NaN  \n",
       "565               NaN      NaN  \n",
       "566               NaN      NaN  \n",
       "567               NaN      NaN  \n",
       "\n",
       "[568 rows x 31 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the path to the Excel file\n",
    "file_path = 'assets.csv'\n",
    "\n",
    "# Read the Excel file\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get unique values for each column in a DataFrame\n",
    "def get_unique_values(df):\n",
    "    unique_values = {}\n",
    "    for column in df.columns:\n",
    "        unique_values[column] = df[column].unique()\n",
    "    return unique_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type\n",
      "name\n",
      "organization\n",
      "description\n",
      "created_date\n",
      "url\n",
      "datasheet\n",
      "modality\n",
      "size\n",
      "sample\n",
      "analysis\n",
      "dependencies\n",
      "included\n",
      "excluded\n",
      "quality_control\n",
      "access\n",
      "license\n",
      "intended_uses\n",
      "prohibited_uses\n",
      "monitoring\n",
      "feedback\n",
      "model_card\n",
      "training_emissions\n",
      "training_time\n",
      "training_hardware\n",
      "adaptation\n",
      "output_space\n",
      "terms_of_service\n",
      "monthly_active_users\n",
      "user_distribution\n",
      "failures\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loop through each DataFrame and print unique values\n",
    "unique_values = get_unique_values(df)\n",
    "for column, values in unique_values.items():\n",
    "    print(f\"{column}\")\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type: 3\n",
      "name: 567\n",
      "organization: 193\n",
      "description: 467\n",
      "created_date: 304\n",
      "url: 443\n",
      "datasheet: 39\n",
      "modality: 74\n",
      "size: 200\n",
      "sample: 10\n",
      "analysis: 249\n",
      "dependencies: 303\n",
      "included: 34\n",
      "excluded: 26\n",
      "quality_control: 85\n",
      "access: 3\n",
      "license: 54\n",
      "intended_uses: 161\n",
      "prohibited_uses: 71\n",
      "monitoring: 22\n",
      "feedback: 187\n",
      "model_card: 166\n",
      "training_emissions: 22\n",
      "training_time: 76\n",
      "training_hardware: 121\n",
      "adaptation: 10\n",
      "output_space: 33\n",
      "terms_of_service: 40\n",
      "monthly_active_users: 4\n",
      "user_distribution: 3\n",
      "failures: 2\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loop through each DataFrame and print unique values\n",
    "unique_values = get_unique_values(df)\n",
    "for column, values in unique_values.items():\n",
    "    print(f\"{column}: {len(values)}\")\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: type\n",
      "Unique values: ['dataset' 'model' 'application']\n",
      "Column: name\n",
      "Unique values: ['ToyMix' 'LargeMix' 'UltraLarge' 'Lag-LLaMA' 'Prithvi' 'Watsonx.ai'\n",
      " 'Granite' 'Animagine XL 3.1' 'Portkey' 'Viable' 'Auto-GPT' 'Bark'\n",
      " 'ChatGPT powered by OBO' 'GPT-JT' 'GPT-NeoXT-Chat-Base'\n",
      " 'OpenChatKit moderation model' 'OIG-43M' 'OIG-moderation'\n",
      " 'RedPajama-Data' 'Llama-2-7B-32K-Instruct' 'RedPajama-Data-v2'\n",
      " 'StripedHyena' 'StripedHyena Nous' 'MediTron' 'XVERSE' 'Otter' 'EXMODD'\n",
      " 'MiniMA' 'ChatGLM' 'OpenFold' 'Ferret' 'Guanaco' 'Llark' 'InternLM'\n",
      " 'BioMistral' 'Khanmigo' 'GAIA-1' 'GreenBit LLaMA' 'Ocean-1' 'Aurora-M'\n",
      " 'CodeParrot' 'Zephyr' 'IDEFICS' 'OBELICS' 'FinGPT' 'BLUUMI'\n",
      " 'Cosmopedia v0.1' 'Idefics2' 'The Cauldron' 'DeepFloyd IF' 'StableLM'\n",
      " 'Stable Diffusion' 'Stable Diffusion XL' 'Stable Video Diffusion'\n",
      " 'Large Video Dataset' 'Sky Replacer' 'StableLM 2' 'Stable Cascade'\n",
      " 'Stable Video 3D' 'Stable Audio 2.0' 'Reka Flash' 'Reka Core' 'FuseChat'\n",
      " 'MoMo' 'Mistral' 'Mistral Large' 'Le Chat' 'Reexpress One'\n",
      " 'Dolphin 2.2 Yi' 'WizardLM Uncensored' 'DuckAssist' 'Perplexity Ask'\n",
      " 'Bird SQL' 'Perplexity Chat' 'Vulture' 'DeciLM' 'The Pile' 'GPT-J'\n",
      " 'GPT-Neo' 'GPT-NeoX' 'GooseAI API' 'VQGAN-CLIP' 'Pythia' 'Llemma'\n",
      " 'Proof Pile 2' 'Pile-T5' 'Virtual Volunteer' 'CodeGeeX' 'CogView'\n",
      " 'CogView 2' 'CogVideo' 'GLM-130B' 'CogVLM' 'UltraLM' 'UltraChat'\n",
      " 'PolyCoder' 'Moment' 'OpenAssistant LLaMA 2' 'Inflection-1' 'Pi'\n",
      " 'Inflection-2' 'Inflection-2.5' 'RWKV World 4' 'RWKV 4 Pile'\n",
      " 'RWKV World 5' 'ERNIE 3.0 Titan' 'ERNIE-ViLG' 'ERNIE-ViLG 2.0'\n",
      " 'ERNIE 4.0' 'Q-Chat' 'Bedrock' 'FalconLite2' 'Chronos' 'Prism'\n",
      " 'InternVideo' 'Lego-MT' 'MathCoder' 'InternVideo2' 'CosmicMan'\n",
      " 'CosmicMan-HQ 1.0' 'Nucleus' 'Devin' 'Konan LLM' 'LinkedIn' 'Character'\n",
      " 'Open X-Embodiment dataset' 'RT-1-X' 'RT-2-X' 'Taiyi Diffusion XL'\n",
      " 'Pegasus-1' 'Marengo 2.6' 'GodziLLa 2' 'BiomedGPT' 'MM1' 'OpenELM'\n",
      " 'StarCoder' 'SantaCoder' 'The Stack' 'StarCoder2-15B' 'StarCoder2-7B'\n",
      " 'StarCoder2-3B' 'h2oGPT' 'H2O Danube' 'ARES' 'C4'\n",
      " 'Internal Google BERT dataset' 'Conceptual Captions' 'Conceptual 12M'\n",
      " 'T5' 'Internal Google BERT' 'Google Search' 'Infiniset' 'LaMDA'\n",
      " 'PaLM dataset' 'Flan-T5' 'UL2' 'Parti' 'Imagen' 'VATT' 'PaLM' 'PaLM API'\n",
      " 'Med-PaLM' 'Med-PaLM Multimodal' 'MultiMedQA' 'Flan-PaLM' 'Flan-U-PaLM'\n",
      " 'Muffin' 'U-PaLM' 'PaLM-SayCan' 'GLaM' 'GLaM Web dataset'\n",
      " 'GLaM Conversations dataset' 'GLaM Forums dataset' 'GLaM News dataset'\n",
      " 'MUM' 'MUM dataset' 'Phenaki' 'Phenaki Video-Text Corpus' 'Flan-UL2'\n",
      " 'Flan Collection' 'MusicLM' 'SoundStream' 'w2v-BERT' 'MuLan'\n",
      " 'MuLan dataset' 'MusicLM dataset' 'MusicLM semantic model'\n",
      " 'MusicLM acoustic model' 'Noise2Music' 'LaMDA-LF' 'Rater-LF' 'Rater-SF'\n",
      " 'Noise2Music pseudolabeler' 'Noise2Music audio dataset'\n",
      " 'Noise2Music pseudolabel dataset' 'AI Test Kitchen' 'Bard' 'Minerva'\n",
      " 'Minerva Math Web Pages dataset' 'USM' 'YouTube' 'PaLM-E' 'ViT-22B'\n",
      " 'AudioLM' 'PaLI' 'ViT-e' 'WebLI' 'Vid2Seq' 'Google Joint SLM' 'PaLM 2'\n",
      " 'MedLM' 'Gemini' 'TimesFM' 'Gemma' 'Med-Gemini' 'HyperCLOVA'\n",
      " 'HyperCLOVA X' 'Crisis Contact Simulator' 'Ask Instacart'\n",
      " 'Firefly Image 2' 'Firefly Vector' 'Firefly Design' 'Firefly' 'CulturaX'\n",
      " 'GenSLM' 'Moonhub Recruiter' 'Skywork' 'COYO-700M' 'Kotoba Speech'\n",
      " 'Shop Assistant' 'GPT-3 dataset' 'HumanEval' 'Codex dataset'\n",
      " 'CLIP dataset' 'DALL·E dataset' 'Whisper dataset' 'WebText' 'GPT-2'\n",
      " 'GPT-3' 'Codex' 'InstructGPT' 'Whisper' 'CLIP' 'DALL·E' 'Jukebox'\n",
      " 'DALL·E 2' 'OpenAI API' 'VPT' 'web_clean' 'ChatGPT' 'gpt-3.5-turbo'\n",
      " 'GPT-4 Turbo' 'gpt-3.5-turbo dataset' 'code-davinci-002 dataset'\n",
      " 'code-davinci-002' 'text-davinci-002' 'text-davinci-003' 'Whisper API'\n",
      " 'ChatGPT API' 'OpenAI Moderation API' 'OpenAI toxicity classifier'\n",
      " 'OpenAI toxicity dataset' 'Sage API' 'Dragonfly API' 'Sage' 'Dragonfly'\n",
      " 'ChatGPT for Slack' 'GPT-4' 'GPT-4 API' 'ChatGPT Enterprise' 'DALL·E 3'\n",
      " 'Sora' 'Ideogram 1.0' 'FinPile' 'BloombergGPT' 'Common Corpus' 'Cformers'\n",
      " 'Platypus' 'UFOGen' 'Nextdoor Assistant' 'You dataset' 'You model'\n",
      " 'You Search' 'SBU Captions' 'MassiveText' 'M3W' 'Gato dataset'\n",
      " 'AlphaFold2' 'Flamingo' 'AlphaCode' 'Gopher' 'Chinchilla' 'Gato'\n",
      " 'Sparrow' 'RETRO' 'Sparrow Rule reward model'\n",
      " 'Sparrow Preference reward model' 'Sparrow adversarial probing dataset'\n",
      " 'Sparrow response preference dataset' 'GopherCite'\n",
      " 'GopherCite reward model' 'GopherCite Preference dataset' 'Dramatron'\n",
      " 'RT-2' 'Lyria' 'Genie' 'YT-Temporal-1B' 'WebVid-10M' 'WebVid-2M' 'Sana'\n",
      " 'NaturalInstructions-v2' 'SODA' 'Multimodal C4' 'COSMO' 'Dolma'\n",
      " 'Tulu-V2-mix' 'Tulu 2' 'Tulu 2 DPO' 'Code Tulu 2' 'OLMo' 'MADLAD-400'\n",
      " 'VARCO-LLM' 'UnderwriteGPT' 'Cerebras-GPT' 'Jais' 'Jais Chat'\n",
      " 'Bittensor Language Model' 'SlimPajama' 'CodeGen' 'BLIP' 'LAION-115M'\n",
      " 'EinsteinGPT' 'BLIP-2' 'Moirai' 'LOTSA' 'Neeva dataset' 'Neeva model'\n",
      " 'NeevaAI' 'Jurassic-1 dataset' 'Jurassic-1 Instruct dataset' 'Jurassic-1'\n",
      " 'Jurassic-1 Instruct' 'Jurassic-2' 'AI21 Playground'\n",
      " 'AI21 Paraphrase API' 'AI21 Summarization API' 'Wordtune' 'Wordtune Read'\n",
      " 'Jamba' 'MPT' 'CommonCanvas' 'CommonCatalog' 'AI Dungeon'\n",
      " 'Conformer-1 dataset' 'Conformer-1' 'AssemblyAI' 'Conformer-1 API'\n",
      " 'Xwin-LM' 'JARVIS-1' 'MAmmoTH' 'A.X' 'Yi' 'Yi-VL' 'HowTo100M' 'Lemur'\n",
      " 'Lemur-Chat' 'ACT-1' 'Persimmon' 'Fuyu' 'Fuyu Heavy' 'CPM Bee'\n",
      " 'UltraFeedback' 'MiniCPM' 'Eurus' '10k_prompts_ranked' 'ESM-2' 'PMD'\n",
      " 'FLAVA' 'The Galactica Corpus' 'Galactica' 'InCoder' 'OPT'\n",
      " 'Make-A-Video dataset' 'Make-A-Video' 'LLaMA' 'Llama 2' 'OPT-IML' 'SA-1B'\n",
      " 'SAM' 'Voicebox' 'PEER' 'MusicGen' 'AudioGen' 'Emu' 'Code LLaMA'\n",
      " 'Emu Video' 'Emu Edit' 'MetaCLIP' 'Llama 3' 'HyperWrite' 'Midm'\n",
      " 'Anthropic Helpfulness dataset' 'Anthropic Harmlessness dataset'\n",
      " 'Anthropic RLHF models' 'Anthropic Human Feedback Interface'\n",
      " 'Anthropic API' 'Claude' 'Claude Instant' 'Claude 2' 'Claude 2.1'\n",
      " 'Claude for Sheets' 'Claude 3' 'ROOTS' 'P3' 'xP3' 'T0++' 'BLOOM' 'mT0'\n",
      " 'BLOOMZ' 'CausalLM' 'Bain Chat' 'SauerkrautLM' 'Transformify Automate'\n",
      " 'Palmyra' 'Camel' 'Dolly' 'DBRX' 'Vicuna' 'EXAONE 2.0' 'RakutenAI'\n",
      " 'OpenBA' 'AI DJ' 'Koala' 'Gorilla' 'OpenLLaMA' 'SaiLY' 'Poe' 'Notion AI'\n",
      " 'Deepseek' 'Deepseek Chat' 'Deepseek Coder' 'Starling' 'Falcon-40B'\n",
      " 'RefinedWeb' 'Falcon-180B' 'My AI for Snapchat' 'Brex Chat' 'LAION-400M'\n",
      " 'LAION-5B' 'LAION-2B-en' 'OpenFlamingo' 'SALMONN' 'SDXL-Lightning' 'VLMo'\n",
      " 'T-ULRv5' 'Turing NLR-v5' 'Megatron-Turing NLG' 'VALL-E' 'GitHub CoPilot'\n",
      " 'BioGPT' 'Bing Search' 'KOSMOS-1' 'Prometheus' 'Florence' 'FLD-900M'\n",
      " 'Azure Cognitive Services for Vision' 'VisualChatGPT'\n",
      " 'Microsoft 365 Copilot' 'Microsoft Business Chat' 'Microsoft Excel'\n",
      " 'Microsoft Outlook' 'Microsoft Power Platform' 'Microsoft PowerPoint'\n",
      " 'Microsoft Teams' 'Microsoft Word' 'Microsoft Inside Look'\n",
      " 'Microsoft Suggested Replies' 'Microsoft Security Copilot' 'UniLM'\n",
      " 'Docugami' 'BEiT-3' 'WizardLM' 'WizardCoder' 'Florence-2' 'FLD-5B'\n",
      " 'OpenOrca' 'LlongOrca' 'Phi-1.5' 'Orca 2' 'Phi-3 Mini' 'TigerBot'\n",
      " 'coheretext' 'Cohere Base' 'Cohere Command' 'Cohere Embed (English)'\n",
      " 'Cohere Embed (Multilingual)' 'Cohere API' 'Cohere Generate Endpoint'\n",
      " 'Cohere Embed Endpoint' 'Cohere Classify Endpoint'\n",
      " 'Cohere Summarize Endpoint' 'Cohere Embedv3 (English)' 'Aya' 'Command-R'\n",
      " 'Aya Dataset' 'Rerank 3' 'Grok-1' 'Grok-1.5V' 'Speak' 'OceanGPT'\n",
      " 'BioMedLM' 'RoentGen' 'CORGI' 'Alpaca dataset' 'Alpaca' 'AutoMathText'\n",
      " 'Nous Hermes 2' 'YaRN LLaMA 2' 'Nous Capybara' 'YaRN Mistral'\n",
      " 'OpenHermes 2.5 Mistral' 'Hermes 2 Pro-Mistral' 'Genstruct' 'Megatron-LM'\n",
      " 'MineDojo' 'VIMA dataset' 'VIMA' 'Nemotron 4' 'BigTrans' 'YAYI 2' 'YaLM'\n",
      " 'Yandex Search' 'Continue' 'GOAT' 'OpenMoE' 'Baichuan 2' 'Wu Dao dataset'\n",
      " 'Wu Dao 2.0' 'JudgeLM' 'JudgeLM Dataset' 'SegMamba' 'BGE M3 Embedding'\n",
      " 'EVA-CLIP' 'Luminous dataset' 'Luminous' 'Aleph Alpha API' 'MAGMA'\n",
      " 'Robin AI' 'Juni Tutor Bot' 'LAION-1B' 'Composer' 'Qwen' 'Qwen 1.5'\n",
      " 'Qwen 1.5 MoE' 'SeaLLM v2.5' 'OpenWebMath' 'Orion' 'SambaLingo' 'Samba 1'\n",
      " 'LP-MusicCaps' 'SciPhi Mistral' 'Notus' 'Amber' 'CrystalCoder'\n",
      " 'Duolingo Explain My Answer' 'Duolingo Max' 'Duolingo Role Play']\n",
      "Column: organization\n",
      "Unique values: ['Mila-Quebec AI Institute'\n",
      " 'Morgan Stanley, ServiceNow Research, University of Montreal, Mila-Quebec AI Institute'\n",
      " 'IBM' 'Cagliostro Research Lab' 'Portkey' 'Viable' 'Auto-GPT' 'Suno'\n",
      " 'HubSpot' 'Together' 'Together, LAION, Ontocord'\n",
      " 'EPFL, Idiap Research Institute, OpenAssistant, Yale' 'Xverse'\n",
      " 'Nanyang Technological University' 'Beijing Institute of Technology'\n",
      " 'ChatGLM' 'Columbia' 'Columbia, Apple AI' 'University of Washington'\n",
      " 'University of Washington, Spotify' 'InternLM'\n",
      " 'Avignon University, Nantes University' 'Khan Academy' 'Wayve'\n",
      " 'GreenBit AI' 'Cresta'\n",
      " 'Tokyo Institute of Technology, MIT-IBM Watson Lab, Sapienza University of Rome'\n",
      " 'HuggingFace'\n",
      " 'University of Turku, HuggingFace, National Library of Finland'\n",
      " 'Hugging Face' 'Stability AI' 'Reka' 'FuseAI' 'Moreh' 'Mistral AI'\n",
      " 'Reexpress AI' 'Cognitive Computations' 'DuckDuckGo' 'Perplexity'\n",
      " 'Virtual Interactive' 'Deci' 'EleutherAI' 'GooseAI' 'Eleuther AI'\n",
      " 'Princeton University, Eleuther AI' 'Be My Eyes' 'Tsinghua University'\n",
      " 'Zhipu AI, Tsinghua University' 'Carnegie Mellon University'\n",
      " 'Carnegie Mellon University, University of Pennsylvania' 'OpenAssistant'\n",
      " 'Inflection AI' 'RWKV' 'Baidu, PengCheng Laboratory' 'Baidu' 'Quizlet'\n",
      " 'Amazon' 'Toyota Research Institute' 'Shanghai AI Laboratory'\n",
      " 'Shanghai AI Laboratory, Nanjing University, Zhejiang University'\n",
      " 'Nucleus.AI' 'Cognition Labs' 'Konan' 'LinkedIn' 'Character AI'\n",
      " 'Open X-Embodiment' 'Open X-Embodiment, Google Deepmind'\n",
      " 'International Digital Economy Academy, South China University of Technology, University of Science and Technology of China'\n",
      " 'Twelve Labs' 'Maya Philippines' 'Lehigh University' 'Apple' 'BigCode'\n",
      " 'H2O AI' 'Faraday Lab' 'Google' 'NAVER' 'The Trevor Project' 'Instacart'\n",
      " 'Adobe' 'University of Oregon, Adobe' 'Argonne National Laboratory'\n",
      " 'Moonhub' 'Kunlun Inc.' 'Kakao Brain' 'Kotoba Tech' 'Shop' 'OpenAI'\n",
      " 'OpenAI, Salesforce' 'Ideogram AI' 'Bloomberg' 'Pleias' 'Nolano'\n",
      " 'Boston University' 'Nextdoor' 'You' 'Stony Brook University'\n",
      " 'Google Deepmind' 'Google DeepMind' 'University of Oxford' 'Sana' 'AI2'\n",
      " 'NCSOFT' 'Paladin Group and Dais Technology' 'Cerebras'\n",
      " 'Inception Institute of Artificial Intelligence, Cerebras, Mohamed bin Zayed University of Artificial Intelligence'\n",
      " 'Salesforce' 'Neeva' 'AI21 Labs' 'Mosaic' 'Cornell University, Mosaic'\n",
      " 'Latitude' 'AssemblyAI' 'Xwin'\n",
      " 'Peking University Institute for Artificial Intelligence'\n",
      " 'Ohio State University' 'SK Telecom' '01 AI'\n",
      " 'École Normale Supérieure, Inria' 'OpenLemur' 'Adept' 'OpenBMB'\n",
      " 'Data is Better Together' 'Meta'\n",
      " 'Meta, CMU, TTI-Chicago, UC Berkeley, University of Washington'\n",
      " 'OthersideAI' 'KT Corporation' 'Anthropic' 'BigScience' 'CausalLM' 'Bain'\n",
      " 'VAGO Solutions' 'Transformify' 'Writer' 'Databricks' 'LMSYS'\n",
      " 'LG AI Research' 'Rakuten' 'Soochow University' 'Spotify' 'Berkeley'\n",
      " 'Deepnight Research' 'Quora' 'Notion' 'Deepseek AI' 'Ollama'\n",
      " 'UAE Technology Innovation Institute' 'Snap' 'Brex' 'LAION'\n",
      " 'ByteDance, Tsinghua University' 'ByteDance' 'Microsoft'\n",
      " 'Microsoft, NVIDIA' 'TigerResearch' 'Cohere'\n",
      " 'Cohere for AI, Cohere, Brown University, Carnegie Mellon University, MIT'\n",
      " 'Cohere for AI, Beijing Academy of Artificial Intelligence, Cohere, Binghamton University'\n",
      " 'xAI' 'Speak' 'Zhejiang University' 'Stanford' 'Math AI' 'Nous Research'\n",
      " 'Nous Research, EleutherAI, University of Geneva' 'Nous' 'NVIDIA'\n",
      " 'NVIDIA, Stanford' 'Nvidia'\n",
      " 'Institute of Automation Chinese Academy of Sciences' 'Yandex'\n",
      " 'Continue Dev, Inc.' 'National University of Singapore'\n",
      " 'National University of Singapore, University of Edinburgh, ETH Zurich'\n",
      " 'Baichuan Inc.' 'Beijing Academy of Artificial Intelligence'\n",
      " 'Hong Kong University of Science and Technology (Guangzhou + original), Beijing Academy of Artificial Intelligence'\n",
      " 'Beijing Academy of Artificial Intelligence, University of Science and Technology of China'\n",
      " 'Beijing Academy of Artificial Intelligence, Tsinghua University'\n",
      " 'Aleph Alpha' 'Robin AI' 'Juni Learning' 'Alibaba' 'Qwen Team'\n",
      " 'DAMO Academy, Alibaba' 'University of Toronto' 'OrionStarAI'\n",
      " 'Samba Nova Systems' 'South Korea Graduate School of Culture Technology'\n",
      " 'SciPhi' 'Argilla' 'LLM360' 'Duolingo']\n",
      "Column: description\n",
      "Unique values: ['ToyMix is the smallest dataset of three extensive and meticulously curated multi-label datasets that cover nearly 100 million molecules and over 3000 sparsely defined tasks.'\n",
      " 'LargeMix is the middle-sized dataset of three extensive and meticulously curated multi-label datasets that cover nearly 100 million molecules and over 3000 sparsely defined tasks.'\n",
      " 'UltraLarge is the largest dataset of three extensive and meticulously curated multi-label datasets that cover nearly 100 million molecules and over 3000 sparsely defined tasks.'\n",
      " 'Lag-LLaMA is a general-purpose foundation model for univariate probabilistic time series forecasting based on a decoder-only transformer architecture that uses lags as covariates.'\n",
      " 'Prithvi is a first-of-its-kind temporal Vision transformer pre-trained by the IBM and NASA team on contiguous US Harmonised Landsat Sentinel 2 (HLS) data. The model adopts a self-supervised encoder developed with a ViT architecture and Masked AutoEncoder (MAE) learning strategy, with an MSE loss function.'\n",
      " 'Watsonx.ai is part of the IBM watsonx platform that brings together new generative AI capabilities, powered by foundation models and traditional machine learning into a powerful studio spanning the AI lifecycle.'\n",
      " 'Granite is a set of multi-size foundation models that apply generative AI to both language and code.'\n",
      " 'An open-source, anime-themed text-to-image model enhanced to generate higher quality anime-style images with a broader range of characters from well-known anime series, an optimized dataset, and new aesthetic tags for better image creation.'\n",
      " 'Portkey is a hosted middleware that allows users to create generative AI applications'\n",
      " 'Viable analyzes qualitative consumer feedback and provides summary feedback to companies.\\n'\n",
      " 'Auto-GPT is an experimental open-source application showcasing the capabilities of the GPT-4 language model.'\n",
      " 'Bark is a text-to-audio model that can generate multilingual speech as well as other noises.'\n",
      " 'Give your sales, marketing, and customer service teams one of the most powerful AI tools available - ChatGPT priority access, no timeout limits, company wide access managed through a single account, incorporate into your existing processes without leaving HubSpot'\n",
      " nan\n",
      " 'The RedPajama base dataset is a 1.2 trillion token fully-open dataset created by following the recipe described in the LLaMA paper'\n",
      " 'Llama-2-7B-32K-Instruct is an open-source, long-context chat model finetuned from Llama-2-7B-32K, over high-quality instruction and chat data.'\n",
      " 'RedPajama-Data-v2 is a new version of the RedPajama dataset, with 30 trillion filtered and deduplicated tokens (100+ trillions raw) from 84 CommonCrawl dumps covering 5 languages, along with 40+ pre-computed data quality annotations that can be used for further filtering and weighting.'\n",
      " 'StripedHyena is an LLM and the first alternative model competitive with the best open-source Transformers in short and long-context evaluations, according to Together.'\n",
      " 'StripedHyena Nous is an LLM and chatbot, along with the first alternative model competitive with the best open-source Transformers in short and long-context evaluations, according to Together.'\n",
      " 'Meditron is a large-scale medical LLM that remains open-source.'\n",
      " 'XVERSE is a multilingual large language model for over 40 languages.'\n",
      " 'Otter is a multi-modal model based on OpenFlamingo (open-sourced version of DeepMind’s Flamingo), trained on MIMIC-IT and showcasing improved instruction-following ability and in-context learning.'\n",
      " 'EXMODD (Explanatory Multimodal Open-Domain Dialogue dataset) is a dataset built off the proposed MDCF (Multimodal Data Construction Framework).'\n",
      " 'MiniMA is a smaller finetuned Llama 2 model adapted for Chinese.'\n",
      " 'ChatGLM is a Chinese-English language model with question and answer and dialogue functions, and is aimed at a Chinese audience.'\n",
      " 'OpenFold is an open source recreation of AlphaFold2.'\n",
      " 'Ferret is a Multimodal Large Language Model (MLLM) capable of understanding spatial referring of any shape or granularity within an image and accurately grounding open-vocabulary descriptions.'\n",
      " 'Guanaco is a model family trained with QLORA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance.'\n",
      " 'Llark is an instruction-tuned multimodal model for music understanding.'\n",
      " 'InternLM is an LLM pre-trained on over 2.3T Tokens containing high-quality English, Chinese, and code data.'\n",
      " 'BioMistral is an open-source Large Language Model tailored for the biomedical domain, utilizing Mistral as its foundation model and further pre-trained on PubMed Central.'\n",
      " 'An AI-powered assistant that functions as both a virtual tutor for students and a classroom assistant for teachers.'\n",
      " 'GAIA-1 (‘Generative AI for Autonomy’) is a generative world model that leverages video, text, and action inputs to generate realistic driving scenarios while offering fine-grained control over ego-vehicle behavior and scene features.'\n",
      " 'GreenBit LLaMA is a series of fine-tuned LLaMA models.'\n",
      " \"Ocean-1 is the culmination of Cresta's experience in deploying generative AI systems for large enterprises and signifies their latest milestone in advancing the cutting edge AI technology for customer facing conversations.\"\n",
      " 'Aurora-M is a 15B parameter multilingual open-source model trained on English, Finnish, Hindi, Japanese, Vietnamese, and code.'\n",
      " 'CodeParrot is an autoregressive language model trained on code'\n",
      " 'Zephyr is a series of language models that are trained to act as helpful assistants.'\n",
      " 'IDEFICS is an open-access visual language model, based on Flamingo.'\n",
      " 'OBELICS is a dataset consisting of 141 million interleaved image-text documents scraped from the web and contains 353 million images.'\n",
      " 'FinGPT is a series of Finnish LLMs trained from scratch.'\n",
      " 'BLUUMI is a multilingual fine-tuned version of BLOOM.'\n",
      " 'Cosmopedia is a dataset of synthetic textbooks, blogposts, stories, posts, and WikiHow articles generated by Mixtral-8x7B-Instruct-v0.1. The dataset contains over 30 million files and 25 billion tokens, making it the largest open synthetic dataset to date. It covers a variety of topics, mapping worldwide knowledge from Web datasets like RefinedWeb and RedPajama, to generate synthetic content.'\n",
      " 'Idefics2 is a general multimodal model that takes as input arbitrary sequences of text and images, generating text responses. It has the capability to describe visual content, answer questions about images, perform basic arithmetic operations, create stories grounded in multiple images, and extract information from documents.'\n",
      " 'The Cauldron is an open compilation of 50 manually-curated datasets formatted for multi-turn conversations.'\n",
      " 'A text-to-image cascaded pixel diffusion model released in conjunction with AI research lab DeepFloyd.'\n",
      " 'Large language models trained on up to 1.5 trillion tokens.'\n",
      " 'Stable Diffusion is a generative software that creates images from text prompts.'\n",
      " 'Stable Diffusion XL is an updated version of Stable Diffusion, and creates descriptive images with shorter prompts and generate words within images.'\n",
      " 'Stable Video Diffusion is a latent diffusion model trained to generate short video clips from an image conditioning.'\n",
      " 'Large Video Dataset is the dataset that trained Stable Video Diffusion, consisting of over 212 years of content.'\n",
      " 'Sky Replacer is an exciting new tool that allows users to replace the color and aesthetic of the sky in their original photos with a selection of nine alternatives to improve the overall look and feel of the image.'\n",
      " 'StableLM 2 is a state-of-the-art 1.6 billion parameter small language model trained on multilingual data in English, Spanish, German, Italian, French, Portuguese, and Dutch.'\n",
      " 'Stable Cascade is built upon the Würstchen architecture and its main difference to other models, like Stable Diffusion, is that it is working at a much smaller latent space.'\n",
      " 'Stable Video 3D (SV3D) is a generative model based on Stable Video Diffusion that takes in a still image of an object as a conditioning frame, and generates an orbital video of that object.'\n",
      " 'Stable Audio 2.0 sets a new standard in AI-generated audio, producing high-quality, full tracks with coherent musical structure up to three minutes in length at 44.1kHz stereo.'\n",
      " 'Reka Flash is a multimodal, multilingual, state-of-the-art 21B model trained entirely from scratch.'\n",
      " 'Reka Core is a frontier-class multimodal language model comparable to industry leaders. It has powerful capabilities including multimodal understanding (including images, videos, and audio), superb reasoning abilities, code generation, and multilinguality with proficiency in 32 languages.'\n",
      " 'FuseChat is a powerful chat Language Learning Model (LLM) that integrates multiple structure and scale-varied chat LLMs using a fuse-then-merge strategy. The fusion is done using two stages'\n",
      " 'MoMo is a large language model fine-tuned from Qwen.'\n",
      " 'Mistral is a compact language model.'\n",
      " 'Mistral Large is Mistral AI’s new cutting-edge text generation model.'\n",
      " 'Le Chat is a first demonstration of what can be built with Mistral models and what can deployed in the business environment.'\n",
      " 'Reexpress One offers a means of document classification, semantic search, and uncertainty analysis on-device.'\n",
      " 'Dolphin 2.2 Yi is an LLM based off Yi.'\n",
      " 'WizardLM Uncensored is WizardLM trained with a subset of the dataset - responses that contained alignment / moralizing were removed.'\n",
      " 'The first Instant Answer in DuckDuckGo search results to use natural language technology to generate answers to search queries using Wikipedia and other related sources'\n",
      " 'Perplexity Ask is a new search interface that uses advanced artificial intelligence technologies'\n",
      " \"Twitter search interface that is powered by Perplexity's structured search engine.\"\n",
      " 'Perplexity chat is an AI chatbot trained in-house by Perplexity.'\n",
      " 'Vulture is a further fine-tuned causal Decoder-only LLM built by Virtual Interactive (VILM) on top of Falcon.'\n",
      " 'DeciLM is a LLM that on release ranks as the fastest and most accurate model of its size.'\n",
      " 'A large language model training dataset, used to train GPT-NeoX-20B.\\n'\n",
      " 'GPT-J is an open-source autoregressive language model.'\n",
      " 'GPT-NeoX (20B) is an open-sourced autoregressive language model.\\n'\n",
      " 'GooseAI API is an API service providing access to NLP services.\\n'\n",
      " 'VQGAN-CLIP is a model that better generates and edits images using a multimodal encoder to guide image generation.'\n",
      " 'A suite of 16 LLMs all trained on public data seen in the exact same order and ranging in size from 70M to 12B parameters'\n",
      " 'Llemma is a large language model for mathematics.'\n",
      " 'Proof Pile 2 is a corpus for language modeling of mathematics.'\n",
      " 'Pile-T5 is a version of the broadly used T5 model, but improved to eliminate weaknesses such as the omission of crucial code-related tokens. It utilizes LLaMA tokenizer and is trained on the Pile, offering enhancements for finetuning on downstream tasks, particularly those involving code.'\n",
      " 'The first-ever digital visual assistant powered by OpenAI’s new GPT-4 language model.'\n",
      " 'CodeGeeX is an autoregressive language model trained on code'\n",
      " 'CogView is a transformer model for text-to-image generation'\n",
      " 'CogView 2 is a hierarchical transformer for text-to-image generation'\n",
      " 'CogVideo is a transformer model for text-to-video generation'\n",
      " 'GLM-130B is a bidirectional language model trained on English and Chinese'\n",
      " 'CogVLM is a powerful open-source visual language foundation model'\n",
      " 'UltraLM is a series of chat language models trained on UltraChat.'\n",
      " 'UltraChat is an open-source, large-scale, and multi-round dialogue data powered by Turbo APIs.'\n",
      " 'PolyCoder is a code model trained on 2.7B parameters based on the GPT-2 architecture, which was trained on 249GB of code across 12 programming languages on a single machine.'\n",
      " 'Moment is a family of open-source foundation models for general-purpose time-series analysis.'\n",
      " \"OpenAssistant LLaMA 2 is an Open-Assistant fine-tuning of Meta's LLaMA 2.\"\n",
      " \"Inflection AI's first version of its in-house LLM. via Inflection AI's conversational API.\"\n",
      " 'Personal AI chatbot designed to be conversational and specialized in emotional intelligence.'\n",
      " 'Inflection-2 is the best model in the world for its compute class and the second most capable LLM in the world, according to benchmark evaluation, as of its release.'\n",
      " \"Inflection-2.5 is an upgraded in-house model that is competitive with all the world's leading LLMs, as of release, like GPT-4 and Gemini.\"\n",
      " 'RWKV World 4 is an RNN with GPT-level LLM performance, which can also be directly trained like a GPT transformer (parallelizable).'\n",
      " 'RWKV 4 Pile is an RNN with GPT-level LLM performance, which can also be directly trained like a GPT transformer (parallelizable).'\n",
      " 'RWKV World 5 is an RNN with GPT-level LLM performance, which can also be directly trained like a GPT transformer (parallelizable).'\n",
      " 'ERNIE 3.0 Titan is a language model'\n",
      " 'ERNIE-ViLG is a model for text-to-image generation'\n",
      " 'ERNIE-4.0 is a multimodal generalist foundation model.'\n",
      " 'Quizlet is introducing Q-Chat, a fully-adaptive AI tutor that engages students with adaptive questions based on relevant study materials delivered through a fun chat experience.'\n",
      " 'Bedrock is a new service that makes FMs from AI21 Labs, Anthropic, Stability AI, and Amazon accessible via an API. Bedrock is intended for customers to build and scale generative AI-based applications using FMs, democratizing access for all builders. using an API.'\n",
      " 'FalconLite2 is a fine-tuned and quantized Falcon language model, capable of processing long (up to 24K tokens) input sequences.'\n",
      " 'Chronos is a family of pretrained time series forecasting models based on language model architectures. A time series is transformed into a sequence of tokens via scaling and quantization, and a language model is trained on these tokens using the cross-entropy loss. Once trained, probabilistic forecasts are obtained by sampling multiple future trajectories given the historical context.'\n",
      " 'Prism is a family of VLMs trained using new analyses about key vision design axes.'\n",
      " 'Lego-MT is a multilingual large language model which uses a more efficient approach of being an effective detachable model.'\n",
      " 'MathCoder is a family of models capable of generating code-based solutions for solving challenging math problems.'\n",
      " 'InternLM is a high-quality language model proficient in English, Chinese, and code.'\n",
      " 'InternVideo2 is a new video foundation model (ViFM) that achieves the state-of-the-art performance in action recognition, video-text tasks, and video-centric dialogue.'\n",
      " 'CosmicMan is a text-to-image foundation model specialized for generating high-fidelity human images with meticulous appearance, reasonable structure, and precise text-image alignment.'\n",
      " 'CosmicMan-HQ 1.0 is a large-scale dataset with 6 million high-quality, real-world human images.'\n",
      " 'Nucleus is a 22B parameters causal decoder-only model built by Nucleus.AI and trained on 500B tokens of RefinedWeb along with curated corpora.'\n",
      " 'Devin is the world’s first fully autonomous AI software engineer.'\n",
      " 'Konan LLM is a Large Language Model developed in-house by Konan Technology. Optimized for super-large AI training, it leverages high-quality, large-scale data and over 20 years of expertise in natural language processing.'\n",
      " \"More than 40 percent of LinkedIn's feed posts include at least one image. We want every member to have equal access to opportunity and are committed to ensuring that we make images accessible to our members who are blind or who have low vision so they can be a part of the online conversation. With Azure Cognitive Service for Vision, we can provide auto-captioning to edit and support alt. text descriptions.\"\n",
      " 'Character allows users to converse with various chatbot personas.'\n",
      " 'The Open X-Embodiment dataset is a dataset of robot movements assembled from 22 different robots collected through a collaboration between 21 institutions, demonstrating 527 skills (160266 tasks)'\n",
      " 'RT-1-X is a model trained on the Open X-Embodiment dataset that exhibits better generalization and new capabilities compared to its predecessor RT-1, an efficient Transformer-based architecture designed for robotic control.'\n",
      " 'RT-2-X is a model trained on the Open X-Embodiment dataset that exhibits better generalization and new capabilities compared to its predecessor RT-2, a large vision-language model co-fine-tuned to output robot actions as natural language tokens.'\n",
      " 'Taiyi Diffusion XL is a new Chinese and English bilingual text-to-image model which is developed by extending the capabilities of CLIP and Stable-DiffusionXL.'\n",
      " 'Pegasus-1 is a video-language foundation model.'\n",
      " 'Marengo 2.6 is a new state-of-the-art (SOTA) multimodal foundation model capable of performing any-to-any search tasks, including Text-To-Video, Text-To-Image, Text-To-Audio, Audio-To-Video, Image-To-Video, and more.\\xa0'\n",
      " 'GodziLLa 2 is an experimental combination of various proprietary LoRAs from Maya Philippines and Guanaco LLaMA 2 1K dataset, with LLaMA 2.'\n",
      " 'BiomedGPT leverages self-supervision on large and diverse datasets to accept multi-modal inputs and perform a range of downstream tasks.'\n",
      " 'MM1 is a family of multimodal models, including both dense variants up to 30B and mixture-of-experts (MoE) variants up to 64B.'\n",
      " 'OpenELM is a family of Open-source Efficient Language Models. It uses a layer-wise scaling strategy to efficiently allocate parameters within each layer of the transformer model, leading to enhanced accuracy.'\n",
      " 'StarCoder is a Large Language Model for Code (Code LLM) trained on permissively licensed data from GitHub, including from 80+ programming languages, Git commits, GitHub issues, and Jupyter notebooks.'\n",
      " \"Multilingual code model derived from the findings of BigCode Project analysis on Github stars' association to data quality.\"\n",
      " 'The Stack contains over 6TB of permissively-licensed source code files covering 358 programming languages. The Stack serves as a pre-training dataset for Code LLMs, i.e., code-generating AI systems which enable the synthesis of programs from natural language descriptions as well as other from code snippets.'\n",
      " 'StarCoder2-15B model is a 15B parameter model trained on 600+ programming languages from The Stack v2, with opt-out requests excluded. The training was carried out using the Fill-in-the-Middle objective on 4+ trillion tokens.'\n",
      " 'StarCoder2-7B model is a 7B parameter model trained on 17 programming languages from The Stack v2, with opt-out requests excluded. The model uses Grouped Query Attention, a context window of 16,384 tokens with a sliding window attention of 4,096 tokens, and was trained using the Fill-in-the-Middle objective on 3.5+ trillion tokens.'\n",
      " 'StarCoder2-3B model is a 3B parameter model trained on 17 programming languages from The Stack v2, with opt-out requests excluded. The model uses Grouped Query Attention, a context window of 16,384 tokens with a sliding window attention of 4,096 tokens, and was trained using the Fill-in-the-Middle objective on 3+ trillion tokens.'\n",
      " 'Series of models fine-tuned on well-known LLMs using the h2oGPT repositories.'\n",
      " 'H2O Danube is a language model trained on 1T tokens following the core principles of LLaMA 2 and Mistral.'\n",
      " 'ARES is a text-to-image generator based on Stable Diffusion. The goal is to provide a simple tool with a user interface allowing mainstream AI access for artists and creators.'\n",
      " 'The Colossal Clean Crawled Corpus (C4) is a processed version of Common Crawl to facilitate transfer learning in NLP.'\n",
      " 'The dataset used to train Internal Google BERT models.\\n'\n",
      " 'A dataset containing 3 million (image-URL, caption) pairs designed for the training and evaluation of machine learned image captioning systems.\\n'\n",
      " 'A dataset with 12 million image-text pairs specifically meant to be used for vision-and-language pre-training.\\n'\n",
      " 'Text-To-Text Transfer Transformer (T5) is a model that unifies all NLP tasks under the text-to-text format.'\n",
      " 'Internal Google BERT model used to power Google Search products.\\n'\n",
      " \"Google Search is Google's search engine.\\n\"\n",
      " 'Infiniset \"is a combination of dialog data from public dialog data and other public web documents\" [[Appendix E]](https://arxiv.org/pdf/2201.08239.pdf#appendix.E).\\n'\n",
      " 'LaMDA stands for Language Models for Dialog Application. It is a transformer based language model trained on dialogue data.\\n'\n",
      " 'PaLM dataset \"was created for pre-training language models\" [[Datasheet]](https://arxiv.org/pdf/2204.02311.pdf#appendix.D).\\n'\n",
      " 'Flan-T5 is a version of the T5 language model fine-tuned on instruction data'\n",
      " 'UL2 is a language model trained with a new pretraining objective'\n",
      " 'Parti is a text-to-image diffusion model'\n",
      " 'Imagen is a text-to-image diffusion model'\n",
      " 'VATT is a family of models trained on multimodal data'\n",
      " 'PaLM stands Pathways Language Model, \"dense decoder-only Transformer model trained with the Pathways system\" [[Google ai Blog]](https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html).\\n'\n",
      " 'a new developer offering that makes it easy and safe to experiment with Google’s language models.'\n",
      " 'MUM (Multitask Unified Model) is a multimodal model that is specialized for more complex queries.'\n",
      " 'AI Test Kitchen provides a new way for people to learn about, experience, and give feedback on emerging AI technology, like LaMDA.'\n",
      " 'Conversational AI service, powered by LaMDA'\n",
      " 'Universal Speech Model (USM) is a family of state-of-the-art speech models with 2B parameters trained on 12 million hours of speech and 28 billion sentences of text, spanning 300+ languages. USM, which is for use in YouTube (e.g., for closed captions), can perform automatic speech recognition (ASR) on widely-spoken languages like English and Mandarin, but also languages like Punjabi, Assamese, Santhali, Balinese, Shona, Malagasy, Luganda, Luo, Bambara, Soga, Maninka, Xhosa, Akan, Lingala, Chichewa, Nkore, Nzema to name a few. Some of these languages are spoken by fewer than twenty million people, making it very hard to find the necessary training data.'\n",
      " 'YouTube is a global online video sharing and social media platform'\n",
      " 'Joint speech and language model using a Speech2Text adapter and using a CTC-based blank-filtering.'\n",
      " 'PaLM 2 is a new state-of-the-art language model that has better multilingual and reasoning capabilities and is more compute-efficient than its predecessor PaLM. PaLM 2 is a Transformer-based model trained using a mixture of objectives similar to UL2.'\n",
      " 'MedLM is a collection of foundation models tuned to follow natural language instructions for tasks in medicine, such as question answering and creating draft summaries.'\n",
      " \"As of release, Gemini is Google's most capable and flexible AI model, proficient in multimodal domains.\"\n",
      " 'TimesFM is a single forecasting model pre-trained on a large time-series corpus of 100 billion real world time-points.'\n",
      " 'Gemma is a family of lightweight, state-of-the-art open models from Google, based on the Gemini models. They are text-to-text, decoder-only large language models, available in English.'\n",
      " 'Med-Gemini is a family of highly capable multimodal models that are specialized in medicine with the ability to seamlessly integrate the use of web search, and that can be efficiently tailored to novel modalities using custom encoders.'\n",
      " 'HyperClova is an autoregressive language model'\n",
      " 'HyperCLOVA X is a family of large language models (LLMs) tailored to the Korean language and culture, along with competitive capabilities in English, math, and coding.'\n",
      " \"Crisis Contact Simulator, developed as part of a collaboration with Google.org, helps train The Trevor Project counselors by mimicking to be a teen in crisis. Crisis Contact Simulator is used as part of the training programs for the Trevor Project's 24/7 digital crisis services that supports LGBTQ youth [[Trevor Project Blog]](https://www.thetrevorproject.org/blog/the-trevor-project-launches-new-ai-tool-to-support-crisis-counselor-training/).\\n\"\n",
      " 'Instacart is augmenting the Instacart app to enable customers to ask about food and get inspirational, shoppable answers. This uses ChatGPT alongside Instacart’s own AI and product data from their 75,000+ retail partner store locations to help customers discover ideas for open-ended shopping goals, such as “How do I make great fish tacos?” or “What’s a healthy lunch for my kids?” Instacart plans to launch “Ask Instacart” later this year.'\n",
      " 'Firefly Image 2 is the next generation of generative AI for imaging, bringing significant advancements to creative control and quality, including new Text to Image capabilities now available in the popular Firefly web app where 90% of users are new to Adobe products.'\n",
      " \"Firefly Vector is the world’s first generative AI focused on producing vector graphics, bringing Adobe's vector graphic and generative AI expertise directly into Adobe Illustrator workflows with Text to Vector Graphic.\"\n",
      " 'Firefly Design powers instant generation of amazing quality template designs in Adobe Express with the new Text to Template capability.'\n",
      " 'Adobe Firefly is a standalone web application. It offers new ways to ideate, create, and communicate while significantly improving creative workflows using generative AI.'\n",
      " 'CulturaX is a substantial multilingual dataset with 6.3 trillion tokens in 167 languages, tailored for LLM development.'\n",
      " \"Moonhub Recruiter is the world's first AI-powered recruiter providing sourcing and recruiting services for startups and growing businesses.\"\n",
      " 'The Skywork series is a family of large language models (LLMs) trained on a corpus of over 3.2 trillion tokens drawn from both English and Chinese texts.'\n",
      " 'COYO-700M is a large-scale dataset that contains 747M image-text pairs as well as many other meta-attributes to increase the usability to train various models.\\n'\n",
      " 'Kotoba-Speech is a Transformer-based speech generative model that supports fluent text-to-speech generation in Japanese and one-shot voice cloning through speech prompt.'\n",
      " 'When shoppers search for products, the shopping assistant makes personalized recommendations based on their requests. Shop’s new AI-powered shopping assistant will streamline in-app shopping by scanning millions of products to quickly find what buyers are looking for—or help them discover something new.'\n",
      " 'The GPT-3 dataset is the text corpus that was used to train the GPT-3 model. Information on the GPT-3 dataset is limited to discussion in the paper introducing GPT-3 [[Section 2.2]](https://arxiv.org/pdf/2005.14165.pdf#subsection.2.2).'\n",
      " 'HumanEval is a dataset of 164 programming problems hand-written to evaluate their Codex model.\\n'\n",
      " 'The dataset used to train the Codex model.\\n'\n",
      " 'CLIP dataset contains text-image pairs crawled from the internet.\\n'\n",
      " 'DALL·E dataset is the training set consisting of image and text pairs collected to train the DALL·E model.\\n'\n",
      " 'The Whisper dataset is the speech corpus that was used to train the Whisper model. Information on the dataset is limited to discussion in the paper introducing Whisper. [[Section 2.1]](https://cdn.openai.com/papers/whisper.pdf).\\n'\n",
      " 'GPT-3 is an autoregressive language model.\\n'\n",
      " 'Codex is a GPT language model fine-tuned on publicly available code from GitHub.\\n'\n",
      " 'InstructGPT is a family of GPT-3 based models fine-tuned on human feedback, which allows for better instruction following capabilities than GPT-3.\\n'\n",
      " 'Whisper is an audio transcription software.'\n",
      " '\"CLIP (Contrastive Language-Image Pre-Training) is a neural network trained on a variety of (image, text) pairs. It can be instructed in natural language to predict the most relevant text snippet, given an image, without directly optimizing for the task, similarly to the zero-shot capabilities of GPT-2 and 3. We found CLIP matches the performance of the original ResNet50 on ImageNet “zero-shot” without using any of the original 1.28M labeled examples, overcoming several major challenges in computer vision\" [[CLIP Repository]](https://github.com/openai/CLIP).\\n'\n",
      " 'DALL·E is a GPT-3 based model trained to generate images from text descriptions. The authors found that it had \"a diverse set of capabilities, including creating anthropomorphized versions of animals and objects, combining unrelated concepts in plausible ways, rendering text, and applying transformations to existing images\" [[OpenAI Blog Post]](https://openai.com/blog/dall-e/).\\n'\n",
      " 'Jukebox is a generative model that produces music'\n",
      " '\"DALL·E 2 is an artificial intelligence model that takes a text prompt and/or existing image as an input and generates a new image as an output\" [[System Card]] (https://github.com/openai/dalle-2-preview/blob/main/system-card.md). The model wasn\\'t fully released, but OpenAI released a version of the model (DALL·E 2 Preview) to a select group of testers.\\n'\n",
      " 'OpenAI API is a general purpose \"text in, text out\" interface connecting users with a suite of language models. The API was initially released as a gateway to GPT-3, but it now supports access to other, more specialized OpenAI models. [[Open AI Blog Post]](https://openai.com/blog/openai-api/)\\n'\n",
      " 'ChatGPT is an artificial intelligence chatbot developed by OpenAI.'\n",
      " 'GPT-4 Turbo is a more capable version of GPT-4 and has knowledge of world events up to April 2023. It has a 128k context window so it can fit the equivalent of more than 300 pages of text in a single prompt.'\n",
      " \"API to query OpenAI's Whisper model.\"\n",
      " \"API to query OpenAI's ChatGPT model.\"\n",
      " 'This endpoint provides OpenAI API developers with free access to GPT-based classifiers that detect undesired content—an instance of using AI systems to assist with human supervision of these systems.'\n",
      " \"A chatbot language model available via Quora's Poe\"\n",
      " 'The app integrates ChatGPT’s powerful AI technology to deliver instant conversation summaries, research tools, and writing assistance directly in Slack to help millions of companies work more productively.'\n",
      " 'GPT-4 is OpenAI’s most advanced system, producing safer and more useful responses'\n",
      " \"ChatGPT Enterprise offers enterprise-grade security and privacy, unlimited higher-speed GPT-4 access, longer context windows for processing longer inputs, advanced data analysis capabilities, and customization options compared to OpenAI's previous offerings.\"\n",
      " 'DALL·E 3 is an artificial intelligence model that takes a text prompt and/or existing image as an input and generates a new image as an output The model is now in research preview, and will be available to ChatGPT Plus and Enterprise customers in October.'\n",
      " 'Sora is an AI model that can create realistic and imaginative scenes from text instructions.'\n",
      " 'Ideogram 1.0 is Ideogram’s most advanced text-to-image model, as of release.'\n",
      " 'A comprehensive dataset consisting of a range of English financial documents including news, filings, press releases, web-scraped financial documents, and social media drawn from the Bloomberg archives that was used to train the BloombergGPT model.'\n",
      " 'BloombergGPT is a 50 billion parameter large language model that is specifically trained on a wide range of financial data to support a diverse set of natural language processing tasks within the financial industry.'\n",
      " 'Common Corpus is the largest public domain dataset released for training Large Language Models (LLMs). This dataset includes 500 billion words from a diverse range of cultural heritage initiatives and is the largest corpus in English, French, Dutch, Spanish, German and Italian. It supports efforts to train fully open LLMs on sources without copyright concerns.'\n",
      " 'Cformers is a set of transformers that act as an API for AI inference in code.'\n",
      " 'Platypus is a family of fine-tuned and merged Large Language Models (LLMs).'\n",
      " 'UFOGen is a novel generative model designed for ultra-fast, one-step text-to-image synthesis.'\n",
      " 'AI chatbot on Nextdoor that helps users write more clear and conscientious posts.'\n",
      " 'You.com is a search engine built on artificial intelligence that provides users with a customized search experience while keeping their data 100% private.'\n",
      " 'SBU Captions Dataset is a collection of 1 million images and associated captions from Flickr, filtered so that the descriptions are likely to refer to visual content.\\n'\n",
      " 'The MassiveText dataset was used to train the Gopher model.\\n'\n",
      " 'M3W (MassiveWeb) is dataset used to train Flamingo, and other vision-language models and was created by researchers and engineers.\\n'\n",
      " 'The Gato datasets are a collection of data used to train the Gato model.\\n'\n",
      " 'AlphaFold2 is a protein language model trained on protein sequences'\n",
      " 'Flamingo is a Visual Language Model using the Transformer architecture that is intended for few-shot learning.\\n'\n",
      " 'AlphaCode is an autoregressive language model trained on code'\n",
      " 'Gopher is an autoregressive language model based on the Transformer architecture with two modifications: using RMSNorm instead of LayerNorm and using relative positional encoding scheme instead of absolute positional encodings [[Section 3]](https://arxiv.org/pdf/2112.11446.pdf#subsection.3.1).\\n'\n",
      " 'Chinchilla is an autoregressive language model based on the Transformer architecture with improved scaling laws.\\n'\n",
      " 'Gato is a generalist agent based on sequence modeling using the Transformer architecture to implement multi-modal, multi-task, multi-embodiment generalist policy.\\n'\n",
      " 'RT-2 is a vision-language-action model for robotic actions that incorporates chain of thought reasoning.'\n",
      " \"Lyria is DeepMind's most advanced AI music generation model to date.\"\n",
      " 'Gene is a foundation world model\\xa0trained from Internet videos\\xa0that can generate an endless variety of playable (action-controllable) worlds from synthetic images, photographs, and even sketches.'\n",
      " 'WebVid-10M is a large-scale dataset of short videos with textual descriptions sourced from stock footage sites.\\n'\n",
      " 'WebVid-2M is a large-scale dataset of 2.5M short videos with textual descriptions sourced from stock footage sites. A subset of the WebVid-10M dataset.\\n'\n",
      " '\"Sana is your all-in-one, AI-assisted, online learning platform (LMS). Author employee training courses and measure team development with Sana\\'s powerful analytics. Sana partners with the world\\'s most important organizations and fastest-growing startups to make personalized, adaptive learning available for everyone, everywhere\" [[Sana GPT-3 Demo]](https://gpt3demo.com/apps/sanalabs).\\n'\n",
      " 'SODA is the first publicly available, million-scale, high-quality dialogue dataset covering a wide range of social interactions.'\n",
      " 'An augmentation of C4 with images added and made openly available.'\n",
      " 'COSMO is a conversation agent with greater generalizability on both in- and out-of-domain chitchat datasets'\n",
      " 'Dolma is a dataset of 3 trillion tokens from a diverse mix of web content, academic publications, code, books, and encyclopedic materials'\n",
      " 'Tulu-V2-mix is a dataset composed of many high-quality instruction datasets that results in stronger performance across a variety of reasoning and knowledge-probing tasks.'\n",
      " 'Tulu 2 is a language model trained on the new Tulu-v2-mix dataset and fine-tuned on more state of the art language models.'\n",
      " 'Tulu 2 DPO is created in a similar manner to Tulu 2, but with Direct Preference Optimization (DPO).'\n",
      " 'Code Tulu 2 is a fine-tuned version of Code LLaMA that was trained on a mix of publicly available, synthetic and human datasets.'\n",
      " 'Open Language Model (OLMo) is designed to provide access to data, training code, models, and evaluation code necessary to advance AI through open research to empower academics and researchers to study the science of language models collectively.'\n",
      " 'MADLAD-400 is a document-level multilingual dataset based on Common Crawl, covering 419 languages in total.'\n",
      " 'VARCO-LLM is NCSOFT’s large language model and is trained on English and Korean.'\n",
      " \"UnderwriteGPT is the world's first generative AI underwriting tool.\"\n",
      " 'A Family of Open, Compute-efficient, Large Language Models. The family includes 111M, 256M, 590M, 1.3B, 2.7B, 6.7B, and 13B models. All models in the Cerebras-GPT family have been trained in accordance with Chinchilla scaling laws (20 tokens per model parameter). [[Cerebras Blog Post]](https://www.cerebras.net/blog/cerebras-gpt-a-family-of-open-compute-efficient-large-language-models)\\n'\n",
      " 'Jais is the world’s most advanced Arabic LLM as of its release.'\n",
      " 'Jais Chat is an instruction-tuned version of Jais, optimized for dialog interaction.'\n",
      " 'Bittensor Language Model is a 3 billion parameter language model with an 8k context length trained on 627B tokens of SlimPajama.'\n",
      " 'As of release, SlimPajama is the largest extensively deduplicated, multi-corpora, open-source dataset for training large language models.'\n",
      " 'CodeGen is a language model for code'\n",
      " 'EinsteinGPT is generative AI for customer relationship management (CRFM).'\n",
      " 'BLIP-2 is a model that employs a generic and efficient pretraining strategy that bootstraps vision-language pre-training from off-the-shelf frozen pre-trained image encoders and frozen large language models.'\n",
      " 'Moirai is a cutting-edge time series foundation model, offering universal forecasting capabilities. It stands out as a versatile time series forecasting model capable of addressing diverse forecasting tasks across multiple domains, frequencies, and variables in a zero-shot manner.'\n",
      " 'LOTSA is the largest collection of open time series datasets with 27B observations across nine domains.'\n",
      " \"NeevaAI is an AI-powered search tool that combines the capabilities of LLMs with Neeva's independent in-house search stack to create a unique and transformative search experience.\"\n",
      " 'The dataset used to train the Jurassic-1 models, based on publicly available data.'\n",
      " 'The dataset used to instruction-tune the Jurassic-1 Instruct models.'\n",
      " 'Jurassic-1 is a family of autoregressive language models (Large, Grande, Jumbo).'\n",
      " 'Jurassic-1 Instruct is an instruction-tuned autoregressive language model.'\n",
      " 'Jurassic-2 is a family of language models designed to replace Jurassic-1.'\n",
      " 'The AI21 Labs Playground supports several task-specific APIs in addition to a variety of models.'\n",
      " \"AI21 Studio's Paraphrase API offers access to our world-class paraphrasing engine. It has been specifically developed for suggesting alternative ways to convey the same message using different words.\"\n",
      " \"AI21 Studio's Summarize API offers access to our world-class summarization engine. It has been specifically developed for reading long texts and providing a faithful summary of the original document.\"\n",
      " 'Wordtune, the first AI-based writing companion that understands context and meaning.'\n",
      " 'Wordtune Read is an AI reader that summarizes long documents so you can understand more, faster.'\n",
      " 'Jamba is a state-of-the-art, hybrid SSM-Transformer LLM. Jamba is the world’s first production-grade Mamba based model.'\n",
      " 'MPT is a series of large language models seeking to address the limitations of other open source models like LLaMA and Pythia.'\n",
      " 'CommonCanvas is a text-to-image model trained solely on Creative Commons licensed images.'\n",
      " 'CommonCatalog is a curated dataset of CommonCrawl images and synthetic captions.'\n",
      " 'AI Dungeon is a single-player text adventure game that uses AI to generate content.\\n'\n",
      " \"The dataset used to train AssemblyAI's Conformer-1 model.\"\n",
      " 'Conformer-1 is a state-of-the-art speech recognition model trained on 650K hours of audio data that achieves near human-level performance and robustness across a variety of data, making up to 43% fewer errors on noisy data than other ASR models.'\n",
      " \"AssemblyAI uses Claude and Anthropic's model to transcribe and understand audio data at scale.\"\n",
      " \"API to access the AssemblyAI's Conformer-1 model.\"\n",
      " 'Xwin-LM is a LLM, which on release, ranked top 1 on AlpacaEval, becoming the first to surpass GPT-4 on this benchmark.'\n",
      " 'JARVIS-1 is an open-world agent that can perceive multimodal input (visual observations and human instructions), generate sophisticated plans, and perform embodied control, all within the popular yet challenging open-world Minecraft universe.'\n",
      " 'MAmmoTH is a series of open-source large language models (LLMs) specifically tailored for general math problem-solving.'\n",
      " \"A.X is SK Telecom's proprietary LLM, which has been trained on the Korean language.\"\n",
      " 'The Yi series models are large language models trained from scratch by developers at 01 AI.'\n",
      " 'The Yi Vision Language (Yi-VL) model is the open-source, multimodal version of the Yi Large Language Model (LLM) series, enabling content comprehension, recognition, and multi-round conversations about images.'\n",
      " 'HowTo100M is a large-scale dataset of narrated videos with an emphasis on instructional videos where content creators teach complex tasks with an explicit intention of explaining the visual content on screen. HowTo100M features a total of 136M video clips with captions sourced from 1.2M Youtube videos (15 years of video) and 23k activities from domains such as cooking, hand crafting, personal care, gardening or fitness.'\n",
      " 'Lemur is an openly accessible language model optimized for both natural language and coding capabilities to serve as the backbone of versatile language agents.'\n",
      " 'Lemur-Chat is an openly accessible language model optimized for both natural language and coding capabilities to serve as the backbone of versatile language agents.'\n",
      " \"ACT-1 (ACtion Transformer) is a large-scale transformer model designed and trained specifically for taking actions on computers (use software tools APIs and websites) in response to the user's natural language commands.\"\n",
      " 'Persimmon is the most capable open-source, fully permissive model with fewer than 10 billion parameters, as of its release date.'\n",
      " \"Fuyu is a small version of the multimodal model that powers Adept's core product.\"\n",
      " 'Fuyu Heavy is a new multimodal model designed specifically for digital agents.'\n",
      " 'CPM-Bee is a fully open-source, commercially-usable Chinese-English bilingual base model with a capacity of ten billion parameters.'\n",
      " 'UltraFeedback is a large-scale, fine-grained, diverse preference dataset, used for training powerful reward models and critic models.'\n",
      " 'MiniCPM is an End-Side LLM developed by ModelBest Inc. and TsinghuaNLP, with only 2.4B parameters excluding embeddings (2.7B in total).'\n",
      " 'Eurus is a suite of large language models (LLMs) optimized for reasoning.'\n",
      " '10k_prompts_ranked is a dataset of prompts with quality rankings created by 314 members of the open-source ML community using Argilla, an open-source tool to label data.'\n",
      " 'ESM-2 is a series of protein language models trained on protein sequences'\n",
      " 'PMD (Public Multimodal Datasets) is a collection of image-text datasets introduced in the FLAVA work.'\n",
      " 'FLAVA is a multimodal model composed of an image encoder, text encoder, and multimodal encoder.'\n",
      " 'The Galactica Corpus is a collection of scientific datasets introduced in the Galactica work.'\n",
      " 'Galactica is a family of autoregressive language models.'\n",
      " 'InCoder is a language model trained on code with a causal masking objective'\n",
      " 'OPT is a family of autoregressive language models.'\n",
      " 'The Make-A-Video dataset is the dataset used to train Make-A-Video, which includes both image-text and video-only datasets with specific and significant filtering.\\n'\n",
      " 'Make-A-Video is a model for Text-to-Video Generation without Text-Video Data.\\n'\n",
      " 'LLaMA is a collection of foundation language models ranging from 7B to 65B parameters trained our on trillions of tokens. The LLaMA models show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets.'\n",
      " 'Llama 2 is an updated version of LLaMA trained on a new mix of publicly available data.'\n",
      " 'SA-1B (Segment Anything 1 Billion) is a dataset designed for training general-purpose object segmentation models from open world images. It consists of 11M diverse, high-resolution, privacy protecting images and 1.1B high-quality segmentation masks.\\n'\n",
      " 'SAM (Segment Anything Model) is a foundation model for image segmentation. The model is designed and trained to be promptable, and supports flexible prompts (point, box, mask and free-form text) to compute masks in real-time to allow interactive use.'\n",
      " 'Voicebox is the first generative AI model for speech to generalize across tasks with state-of-the-art performance.'\n",
      " 'PEER is a collaborative language model that is trained to imitate the entire writing process itself. PEER can write drafts, add suggestions, propose edits and provide explanations for its actions.'\n",
      " \"MusicGen is a simple and controllable model for music generation that doesn't require self-supervised semantic representation\"\n",
      " 'AudioGen is an auto-regressive generative model that generates audio samples conditioned on text inputs'\n",
      " 'Emu is a pre-trained latent diffusion model on 1.1 billion image-text pairs and fine-tuned with only a few thousand carefully selected high-quality images.'\n",
      " 'Code Llama is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 34 billion parameters.'\n",
      " 'Emu Video is a text-to-video generation model that factorizes the generation into two steps, first generating an image conditioned on the text, and then generating a video conditioned on the text and the generated image.'\n",
      " 'Emu Edit is a multi-task image editing model which sets state-of-the-art results in instruction-based image editing.'\n",
      " \"MetaCLIP is a more transparent rendition of CLIP that aims to reveal CLIP's training data curation methods.\"\n",
      " \"Llama 3 is the third generation of Meta AI's open-source large language model. It comes with pretrained and instruction-fine-tuned language models with 8B and 70B parameters that can support a broad range of use cases.\"\n",
      " \"HyperWrite is a writing assistant that generates text based on a user's request, as well as style and tone choices.\\n\"\n",
      " 'Midm is a pre-trained Korean-English language model developed by KT. It takes text as input and creates text. The model is based on Transformer architecture for an auto-regressive language model.'\n",
      " 'One of the datasets used to train Anthropic RLHF models. The dataset was collected by asking crowdworkers to have open-ended conversations with Anthropic models, \"asking for help, advice, or for the model to accomplish a task\", then choose the model answer that was more helpful for their given task, via the Anthropic Human Feedback Interface [[Section 2.2]](https://arxiv.org/pdf/2204.05862.pdf#subsection.2.2).\\n'\n",
      " 'One of the datasets used to train Anthropic RLHF models. The dataset was collected by asking crowdworkers to have open-ended conversations with Anthropic models, aiming to elicit harmful responses, then choose the model answer that was more harmful for their given task, via the Anthropic Human Feedback Interface [[Section 2.2]](https://arxiv.org/pdf/2204.05862.pdf#subsection.2.2).\\n'\n",
      " 'Anthropic RLHF models are models trained using reinforcement learning from human feedback (RLHF). For Anthropic RLHF models, authors started with a set of base models, and asked humans to rank model generated prompts based on a specific tasks. They then trained preference models (PM) on the prompt pairs, and use the PM scores as rewards for training the RLHF models.\\n'\n",
      " 'The feedback interface used to collect preference datasets to train Anthropic RLHF models [[Paper]](https://arxiv.org/pdf/2204.05862.pdf).\\n'\n",
      " 'API is designed to be a backend that incorporates Claude into any application you’ve developed. Our application sends text to our API, then receives a response via server-sent events, a streaming protocol for the web.'\n",
      " 'Claude 2 is a more evolved and refined version of Claude, which is a general purpose large language model using a transformer architecture and trained via unsupervised learning.'\n",
      " 'Claude 2.1 is an updated version of Claude 2, with an increased context window, less hallucination and tool use.'\n",
      " 'Claude for Sheets is a Google Sheets add-on that allows the usage of Claude directly in Google Sheets.'\n",
      " 'The Claude 3 model family is a collection of models which sets new industry benchmarks across a wide range of cognitive tasks.'\n",
      " 'The Responsible Open-science Open-collaboration Text Sources (ROOTS) corpus, a 1.6TB dataset spanning 59 languages that was used to train the 176-billion-parameter BigScience Large Open-science Open-access Multilingual (BLOOM) language model.'\n",
      " 'The Public Pool of Prompts (P3) are prompts written in an unified format use to train T0++.'\n",
      " 'xP3 (Crosslingual Public Pool of Prompts) is a collection of prompts and datasets across 46 of languages & 16 NLP tasks. It is used for the training of BLOOMZ and mT0, multilingual language models capable of following human instructions in dozens of languages zero-shot.'\n",
      " 'T0++ is an multitask fine-tuned language model based on T5.'\n",
      " 'BLOOM is an autoregressive multilingual language model.'\n",
      " 'mT0 is an multitask fine-tuned multilingual language model based on mT5.'\n",
      " 'BLOOMZ is an multitask fine-tuned autoregressive multilingual language model.'\n",
      " 'CausalLM is an LLM based on the model weights of Qwen and trained on a model architecture identical to LLaMA 2.'\n",
      " 'With the alliance, Bain will combine its deep digital implementation capabilities and strategic expertise with OpenAI’s AI tools and platforms, including ChatGPT, to help its clients around the world identify and implement the value of AI to maximize business potential.'\n",
      " 'SauerkrautLM is a German language model merged from two Mistral derivatives.'\n",
      " 'Transformify Automate is a platform for automated task integration using natural language prompts.'\n",
      " 'Palmyra is a family of privacy-first LLMs for enterprises trained on business and marketing writing.'\n",
      " 'Camel is an instruction-following large language model tailored for advanced NLP and comprehension capabilities.'\n",
      " '\"Databricks’ Dolly, a large language model trained on the Databricks\\n Machine Learning Platform, demonstrates that a two-years-old open source\\n model (GPT-J) can, when subjected to just 30 minutes of fine tuning on a\\n focused corpus of 50k records (Stanford Alpaca), exhibit surprisingly\\n high quality instruction following behavior not characteristic of the\\n foundation model on which it is based.\"\\n [[Dolly Repository]](https://github.com/databrickslabs/dolly).\\n'\n",
      " 'DBRX is a transformer-based decoder-only large language model (LLM) that was trained using next-token prediction by Databricks. It uses a fine-grained mixture-of-experts (MoE) architecture with 132B total parameters of which 36B parameters are active on any input. DBRX only accepts text-based inputs and produces text-based outputs.'\n",
      " 'An open-source chatbot trained by fine-tuning LLaMA on user-shared conversations collected from ShareGPT.'\n",
      " 'EXAONE 2.0 is a multimodal artificial intelligence that can be used to help develop new materials and medicines.'\n",
      " 'RakutenAI-7B is a model developed with a focus on Japanese language understanding. It offers competitive performance on English tests as well.'\n",
      " 'OpenBA is an open-sourced 15B bilingual (English + Chinese) asymmetric seq2seq model.'\n",
      " 'The DJ is a personalized AI guide that knows you and your music taste so well that it can choose what to play for you. This feature, first rolling out in beta, will deliver a curated lineup of music alongside commentary around the tracks and artists we think you’ll like in a stunningly realistic voice.'\n",
      " 'A relatively small chatbot trained by fine-tuning Meta’s LLaMA on dialogue data gathered from the web.'\n",
      " 'Gorilla is a finetuned LLaMA-based model that surpasses the performance of GPT-4 on writing API calls.'\n",
      " \"OpenLlama is an open source reproduction of Meta's LLaMA model.\"\n",
      " 'SaiLy is a series/collection of AI Models by Deepnight Research which are highly experimental and uncensored.'\n",
      " 'Poe lets people ask questions, get instant answers, and have back-and-forth conversations with several AI-powered bots. It is initially available on iOS, but we will be adding support for all major platforms in the next few months, along with more bots.'\n",
      " 'Notion AI is a connected assistant that helps you think bigger, work faster, and augments your creativity, right inside the functional workspace you’re already familiar with.'\n",
      " 'Deepseek is a 67B parameter model with Grouped-Query Attention trained on 2 trillion tokens from scratch.'\n",
      " 'Deepseek Chat is a 67B parameter model initialized from Deepseek and fine-tuned on extra instruction data.'\n",
      " 'Deepseek Coder is composed of a series of code language models, each trained from scratch on 2T tokens, with a composition of 87% code and 13% natural language in both English and Chinese.'\n",
      " 'Starling is a large language model trained by reinforcement learning from AI feedback focused on improving chatbot helpfulness.'\n",
      " 'Falcon-40B is a 40B parameters causal decoder-only model built by TII and trained on 1,000B tokens of\\xa0RefinedWeb enhanced with curated corpora.'\n",
      " 'RefinedWeb is a high-quality five trillion tokens web-only English pretraining dataset.'\n",
      " 'Falcon-180B is a 180B parameters causal decoder-only model built by TII and trained on 3,500B tokens of RefinedWeb enhanced with curated corpora.'\n",
      " 'My AI offers Snapchatters a friendly, customizable chatbot at their fingertips that offers recommendations, and can even write a haiku for friends in seconds. Snapchat, where communication and messaging is a daily behavior, has 750 million monthly Snapchatters.'\n",
      " 'Brex Inc., a highly valued startup that makes software for finance professionals, is turning to the same artificial intelligence tool behind ChatGPT for a service that can answer questions about corporate budgets, policy and spending.'\n",
      " 'LAION-400M is a dataset with CLIP-filtered 400 million image-text pairs, their CLIP embeddings and kNN indices that allow efficient similarity search. This dataset is entirely openly, freely accessible.'\n",
      " 'LAION is a dataset of 5 billion image-text pairs from the Internet'\n",
      " 'LAION-2B-en is a subset of the LAION-5B dataset and contains 2.3 billion English image-text pairs.'\n",
      " \"An open-source reproduction of DeepMind's Flamingo model. At its core, OpenFlamingo is a framework that enables training and evaluation of large multimodal models (LMMs).\"\n",
      " 'SALMONN is a large language model (LLM) enabling speech, audio event, and music inputs.'\n",
      " 'SDXL-Lightning is a lightning-fast text-to-image generation model. It can generate high-quality 1024px images in a few steps. The models are distilled from stabilityai/stable-diffusion-xl-base-1.0. This repository contains checkpoints for 1-step, 2-step, 4-step, and 8-step distilled models.'\n",
      " 'VLMo is a model for text-to-image generation'\n",
      " 'T-ULRv5 is a language model trained with two unique training objectives'\n",
      " 'Megatron-Turing NLG is a 530B parameter autoregressive language model.\\n'\n",
      " 'Vall-E is a neural code model for text-to-speech synthesis'\n",
      " 'GitHub CoPilot is a coding pair programmer assisting programmers as they write code.\\n'\n",
      " 'AI-powered Bing search engine and Edge browser, available in preview now at Bing.com, to deliver better search, more complete answers, a new chat experience and the ability to generate content. We think of these tools as an AI copilot for the web.'\n",
      " 'KOSMOS-1 is a multimodal language model that is capable of perceiving multimodal input, following instructions, and performing in-context learning for not only language tasks but also multimodal tasks.'\n",
      " 'In the context of Bing, we have developed a proprietary way of working with the OpenAI model that allows us to best leverage its power. We call this collection of capabilities and techniques the Prometheus model. This combination gives you more relevant, timely and targeted results, with improved safety.'\n",
      " 'Cost-effective, production-ready computer vision services in Azure Cognitive Service for Vision. The improved Vision Services enables developers to create cutting-edge, market-ready, responsible computer vision applications across various industries.'\n",
      " 'It combines the power of language models with your data in the Microsoft Graph and the Microsoft 365 apps to turn your words into the most powerful productivity tool on the planet.'\n",
      " 'Business Chat works across the langugae model, the Microsoft 365 apps, and your data — your calendar, emails, chats, documents, meetings and contacts — to do things you’ve never been able to do before. You can give it natural language prompts like “Tell my team how we updated the product strategy,” and it will generate a status update based on the morning’s meetings, emails and chat threads.'\n",
      " 'Microsoft Excel is the industry leading spreadsheet software program, a powerful data visualization and analysis tool.'\n",
      " 'Microsoft Outlook is a personal information manager software system from Microsoft, available as a part of the Microsoft Office and Microsoft 365 software suites.'\n",
      " 'Microsoft Power Platform is a line of business intelligence, app development, and app connectivity software applications.'\n",
      " 'Microsoft PowerPoint empowers you to create clean slideshow presentations and intricate pitch decks and gives you a powerful presentation maker.'\n",
      " 'Microsoft Teams is a proprietary business communication platform developed by Microsoft, as part of the Microsoft 365 family of products.'\n",
      " 'Microsoft Word is a word processing software developed by Microsoft'\n",
      " 'Inside look is a Microsoft Office feature, composing document insights highlighting key points, expected time to read, and popularity among others.\\n'\n",
      " 'Suggested replies is a Microsoft Outlook feature that suggests responses to emails, available in: English, Spanish, Italian, French, German, Portuguese Chinese Simplified, Chinese Traditional, Swedish, Russian, Korean, Czech, Hungarian, Arabic, Hebrew, Thai, Turkish, Japanese, Dutch, Norwegian, Danish, and Polish.\\n'\n",
      " 'Microsoft Security Copilot is an AI-powered security analysis tool that enables analysts to respond to threats quickly, process signals at machine speed, and assess risk exposure in minutes.\\n'\n",
      " 'UniLM is a unified language model that can be fine-tuned for both natural language understanding and generation tasks.'\n",
      " 'Docugami is a LLM focused on writing business documents and data using generative AI.'\n",
      " 'BEiT-3 is a general-purpose multimodal foundation model for vision and vision-language tasks.'\n",
      " 'Starting with an initial set of instructions, we use our proposed Evol-Instruct to rewrite them step by step into more complex instructions. Then, we mix all generated instruction data to fine-tune LLaMA. We call the resulting model WizardLM.'\n",
      " 'WizardCoder empowers Code LLMs with complex instruction fine-tuning, by adapting the Evol-Instruct method to the domain of code.'\n",
      " 'FLD-5B is the dataset that powers Florence-2'\n",
      " 'The OpenOrca dataset is a collection of augmented FLAN Collection data. Currently ~1M GPT-4 completions, and ~3.2M GPT-3.5 completions. It is tabularized in alignment with the distributions presented in the ORCA paper and currently represents a partial completion of the full intended dataset, with ongoing generation to expand its scope.'\n",
      " 'LlongOrca is an attempt to make OpenOrca able to function in a Llong context.'\n",
      " 'Phi-1.5 is a large language transformer model.'\n",
      " 'Orca 2 is a finetuned version of LLAMA-2 for research purposes.'\n",
      " 'Phi-3 Mini is a 3.8 billion-parameter, lightweight, state-of-the-art open model trained using the Phi-3 datasets.'\n",
      " 'TigerBot is an open source multilingual multitask LLM.'\n",
      " 'The family of datasets used to train Cohere models, which come in two forms: coheretext-filtered and coheretext-unfiltered. The former is used to train the Representation models, while the latter one is used to train the Generation models.\\n'\n",
      " 'The Generations model is a language model trained by Cohere for generation tasks.\\n'\n",
      " 'This model is a generative model optimized to follow commands in the prompt.\\n'\n",
      " 'The Embedding Large (English) model is a language model trained by Cohere for tasks requiring embeddings.\\n'\n",
      " 'This model maps text from 100+ languages to a semantic vector space, positioning text with a similar meaning (regardless of language) in close proximity.\\n'\n",
      " 'Cohere API allows users to access the cohere language models and utilize them in their applications.\\n'\n",
      " 'This endpoint generates realistic text conditioned on a given input.\\n'\n",
      " 'This endpoint returns text embeddings. An embedding is a list of floating point numbers that captures semantic information about the text that it represents.\\n'\n",
      " 'This endpoint makes a prediction about which label best fits a specified text input. To make a prediction, Classify uses the provided examples of text + label pairs as a reference.\\n'\n",
      " 'This endpoint generates a succinct version of the original text that relays the most important information.\\n'\n",
      " \"As of release, Cohere Embedv3 is Cohere's latest and most advanced embeddings model.\"\n",
      " 'Aya is a massively multilingual generative language model that follows instructions in 101 languages of which over 50% are considered as lower-resourced.'\n",
      " 'Command-R is a scalable generative model targeting RAG and Tool Use to enable production-scale AI for enterprise.'\n",
      " 'The Aya Dataset is a dataset that consists of original human-curated prompt-completion pairs written by fluent speakers of 65 languages.'\n",
      " 'Rerank 3 is a new foundation model for efficient enterprise search and retrieval with 4k context length.'\n",
      " 'Grok is an AI modeled after the Hitchhiker’s Guide to the Galaxy,'\n",
      " 'Grok-1.5V is a first-generation multimodal model which can process a wide variety of visual information, including documents, diagrams, charts, screenshots, and photographs.'\n",
      " 'Speak is an AI-powered language learning app focused on building the best path to spoken fluency and is the the fastest-growing English app in South Korea.'\n",
      " 'OceanGPT is the first-ever LLM in the ocean domain and displays expertise in various ocean science tasks.'\n",
      " 'RoentGen is a generative medical imaging model that can create visually convincing X-ray images.'\n",
      " 'Model trained to generate language corrections for physical control tasks.'\n",
      " \"Alpaca dataset consistes of 52,000 instruction-following demonstrations generated in the style of the [Self-Instruct framework](https://github.com/yizhongw/self-instruct) using OpenAI's text-davinci-003 engine. This instruction data can be used to conduct instruction-tuning for language models and make the language model follow instruction better.\\n\"\n",
      " 'Alpaca-7B is an instruction-following model fine-tuned from the LLaMA 7B model on 52K instruction-following demonstrations.\\n'\n",
      " 'AutoMathText is an extensive and carefully curated dataset encompassing around 200 GB of mathematical texts.'\n",
      " 'Nous Hermes 2 Mixtral 8x7B DPO is the new flagship Nous Research model trained over the\\xa0Mixtral 8x7B MoE LLM.'\n",
      " 'YaRN LLaMA 2 is an adapted version of LLaMA 2 using the YaRN extension method.'\n",
      " 'The Capybara series is a series of LLMs and the first Nous collection of models made by fine-tuning mostly on data created by Nous in-house.'\n",
      " 'YaRN Mistral is an adapted version of Mistral using the YaRN extension method.'\n",
      " 'OpenHermes 2.5 Mistral 7B is a state of the art Mistral Fine-tune, a continuation of OpenHermes 2 model, trained on additional code datasets.'\n",
      " 'Hermes 2 Pro on Mistral 7B is an upgraded, retrained version of Nous Hermes 2. This improved version excels at function calling, JSON Structured Outputs, and several other areas, scoring positively on various benchmarks.'\n",
      " 'Genstruct is an instruction-generation model, designed to create valid instructions given a raw text corpus. This enables the creation of new, partially synthetic instruction finetuning datasets from any raw-text corpus. This work was inspired by Ada-Instruct and the model is also trained to generate questions involving complex scenarios that require detailed reasoning.'\n",
      " 'Megatron-LM is an autoregressive language model'\n",
      " 'Nemotron 4 is a 15-billion-parameter large multilingual language model trained on 8 trillion text tokens.'\n",
      " 'BigTrans is a model which adapts LLaMA that covers only 20 languages and enhances it with multilingual translation capability on more than 100 languages'\n",
      " 'YAYI 2 is an open source large language model trained in both English and Chinese.'\n",
      " 'YaLM is a 100B parameter autoregressive model trained on 25% English and 75% Russian text.'\n",
      " 'Yandex is a search engine and web portal. Yandex offers internet search and other services'\n",
      " 'Continue is the open-source autopilot for software development. It is an IDE extension that brings the power of ChatGPT to VS Code and JetBrains. It’s built to be deeply customizable and continuously learn from development data.'\n",
      " 'GOAT is a fine-tuned LLaMA model which uses the tokenization of numbers to significantly outperform benchmark standards on a range of arithmetic tasks.'\n",
      " 'OpenMoE is a series of fully open-sourced and reproducible decoder-only MoE LLMs.'\n",
      " 'Baichuan 2 is a series of large-scale multilingual language models containing 7 billion and 13 billion parameters, trained from scratch, on 2.6 trillion tokens.'\n",
      " 'JudgeLM is a fine-tuned to be a scalable judge to evaluate LLMs efficiently and effectively in open-ended benchmarks.'\n",
      " 'JudgeLM Dataset is a novel dataset replete with a rich variety of seed tasks, comprehensive answers from modern LLMs, answers’ grades from the teacher judge, and detailed reasons for judgments.'\n",
      " 'SegMamba is a novel 3D medical image Segmentation Mamba model, designed to effectively capture long-range dependencies within whole volume features at every scale.'\n",
      " 'BGE M3 Embedding is a new embedding model that can support more than 100 working languages, leading to new state-of-the-art performances on multi-lingual and cross-lingual retrieval tasks.'\n",
      " 'As of release, EVA-CLIP is the largest and most powerful open-source CLIP model to date, with 18 billion parameters.'\n",
      " 'The dataset used to train the Luminous models.'\n",
      " 'Luminous is a family of multilingual language models'\n",
      " 'The Aleph Alpha API serves a family of text-only language models (Luminous) and multimodal text-and-image models (Magma).'\n",
      " 'An autoregressive VL model that is able to generate text from an arbitrary combination of visual and textual input'\n",
      " \"Robin AI uses Claude and Anthropic's models to understand language - including in technical domains like legal language. It's also very confident at drafting, summarising, translations, and explaining complex concepts in simple terms\"\n",
      " 'An online tutoring solution to help students achieve academic success.'\n",
      " 'QWEN is a comprehensive language model series that encompasses distinct models with varying parameter counts. Qwen series, now including Qwen, the base language models, namely Qwen-7B and Qwen-14B, as well as Qwen-Chat, the chat models, namely Qwen-7B-Chat and Qwen-14B-Chat. '\n",
      " 'Qwen 1.5 is the next iteration in their Qwen series, consisting of Transformer-based large language models pretrained on a large volume of data, including web texts, books, codes, etc.'\n",
      " 'Qwen 1.5 is the next iteration in their Qwen series, consisting of Transformer-based large language models pretrained on a large volume of data, including web texts, books, codes, etc. Qwen 1.5 MoE is the MoE model of the Qwen 1.5 series.'\n",
      " 'SeaLLM v2.5 is a multilingual large language model for Southeast Asian (SEA) languages.'\n",
      " 'OpenWebMath is an open dataset containing 14.7B tokens of mathematical webpages from Common Crawl, inspired by Minerva.'\n",
      " 'Orion series models are open-source multilingual large language models trained from scratch by OrionStarAI.'\n",
      " 'SambaLingo is a suite of models that adapt Llama 2 to a diverse set of 9 languages.'\n",
      " 'Samba 1 is a trillion parameter generative AI model using a Composition of Experts architecture.'\n",
      " 'LP-MusicCaps is a LLM-based pseudo music caption dataset.'\n",
      " 'SciPhi Mistral is a Large Language Model (LLM) fine-tuned from Mistral.'\n",
      " 'Notus is an open source LLM, fine-tuned using Direct Preference Optimization (DPO) and AIF (AI Feedback) techniques.'\n",
      " 'Amber is the first model in the LLM360 family, an initiative for comprehensive and fully open-sourced LLMs, where all training details, model checkpoints, intermediate results, and additional analyses are made available to the community.'\n",
      " 'CrystalCoder is a language model with a balance of code and text data that follows the initiative under LLM360 of its training process being fully transparent.'\n",
      " 'Explain My Answer offers learners the chance to learn more about their response in a lesson (whether their answer was correct or incorrect!) By tapping a button after certain exercise types, learners can enter a chat with Duo to get a simple explanation on why their answer was right or wrong, and ask for examples or further clarification.'\n",
      " 'Duolingo Max is a new subscription tier above Super Duolingo that gives learners access to two brand-new features and exercises - Explain My Answer and Roleplay.'\n",
      " 'Roleplay allows learners to practice real-world conversation skills with world characters in the app. These challenges, which earn XP, will live alongside the path as one of the “Side Quests” learners can access by tapping on the character. What will you talk about? We’ll guide you through different scenarios! Learners might discuss future vacation plans with Lin, order coffee at a café in Paris, go furniture shopping with Eddy, or ask a friend to go for a hike.']\n",
      "Column: created_date\n",
      "Unique values: ['2023-10-09' '2024-02-08' '2023-08-03' '2023-09-07' '2023-09-28'\n",
      " '2024-03-18' '2023-05-06' nan '2023-04-16' '2023-04-20' '2023-01-31'\n",
      " '2022-11-29' '2023-03-10' '2022-04-17' '2023-08-18' '2023-10-30'\n",
      " '2023-12-08' '2023-11-27' '2023-11-06' '2023-05-05' '2023-10-17'\n",
      " '2023-11-13' '2023-03-14' '2022-11-20' '2023-10-11' '2023-05-23'\n",
      " '2023-09-20' '2024-02-15' '2023-09-29' '2023-06-20' '2024-04-23'\n",
      " '2021-12-06' '2023-08-22' '2023-11-03' '2024-02-22' '2024-04-15'\n",
      " '2023-04-28' '2022-08-22' '2023-07-26' '2023-11-21' '2023-11-01'\n",
      " '2024-01-19' '2024-01-16' '2024-04-03' '2024-02-12' '2024-02-26'\n",
      " '2023-09-27' '2023-03-21' '2023-11-14' '2023-06-01' '2023-03-08'\n",
      " '2022-12-07' '2022-12-15' '2023-10-27' '2023-10-02' '2023-12-12'\n",
      " '2021-01-01' '2021-06-04' '2021-03-21' '2022-02-02' '2022-09-04'\n",
      " '2023-05-31' '2023-10-16' '2022-09-20' '2021-05-26' '2022-04-28'\n",
      " '2022-05-29' '2022-08-04' '2023-06-27' '2022-02-26' '2024-02-06'\n",
      " '2023-08-23' '2023-06-22' '2023-05-02' '2023-11-22' '2024-03-07'\n",
      " '2023-05-03' '2023-05-15' '2023-12-16' '2021-12-23' '2021-12-31'\n",
      " '2022-10-27' '2023-03-01' '2023-04-13' '2023-08-08' '2024-03-13'\n",
      " '2024-02-09' '2022-12-06' '2023-05-29' '2023-10-05' '2024-03-22'\n",
      " '2024-04-01' '2024-04-28' '2024-03-12' '2023-09-17' '2022-09-16'\n",
      " '2023-10-03' '2024-01-26' '2023-10-23' '2024-03-01' '2023-08-11'\n",
      " '2023-05-26' '2024-03-16' '2024-04-24' '2023-05-09' '2023-02-24'\n",
      " '2024-02-28' '2023-06-16' '2024-01-30' '2023-04-26' '2019-10-23'\n",
      " '2019-11-25' '2018-07-01' '2021-02-17' '2021-06-18' '2022-04-04'\n",
      " '2022-10-20' '2022-05-10' '2022-06-22' '2022-05-23' '2022-04-22'\n",
      " '2022-12-26' '2021-09-03' '2022-08-16' '2021-12-13' '2021-05-18'\n",
      " '2023-02-01' '2023-03-02' '2023-01-26' '2022-08-26' '2023-02-08'\n",
      " '2022-08-25' '2023-02-06' '2022-06-29' '2023-03-06' '2005-02-14'\n",
      " '2023-02-10' '2022-09-07' '2022-09-14' '2023-02-27' '2023-06-08'\n",
      " '2023-05-10' '2023-12-13' '2023-12-06' '2024-02-02' '2024-02-21'\n",
      " '2024-04-29' '2021-05-21' '2024-04-13' '2021-03-24' '2023-10-10'\n",
      " '2022-10-11' '2022-08-31' '2020-06-11' '2021-08-10' '2021-01-05'\n",
      " '2022-09-21' '2019-11-01' '2022-01-27' '2020-04-30' '2022-04-13'\n",
      " '2022-06-23' '2022-11-30' '2022-05-01' '2022-08-10' '2023-01-18'\n",
      " '2023-02-03' '2023-03-07' '2023-08-28' '2023-03-30' '2024-03-20'\n",
      " '2023-03-19' '2023-08-14' '2011-12-12' '2021-12-08' '2022-04-29'\n",
      " '2022-05-12' '2021-07-15' '2022-03-29' '2022-09-28' '2022-03-16'\n",
      " '2022-09-29' '2023-07-28' '2023-11-16' '2024-02-23' '2022-01-07'\n",
      " '2021-04-01' '2022-04-16' '2023-05-24' '2023-06-09' '2023-11-20'\n",
      " '2024-02-01' '2023-09-09' '2023-08-16' '2023-03-28' '2023-08-30'\n",
      " '2023-07-24' '2022-03-25' '2022-01-28' '2023-01-30' '2024-03-19'\n",
      " '2023-01-06' '2021-08-11' '2022-12-01' '2023-03-09' '2020-10-27'\n",
      " '2021-11-16' '2024-03-28' '2023-10-25' '2019-12-17' '2023-03-15' '2022'\n",
      " '2023-11-10' '2023-09-11' '2023-09-26' '2023-11-02' '2024-01-23'\n",
      " '2019-06-07' '2024-01-24' '2023-05-27' '2024-04-02' '2024-02-27'\n",
      " '2022-10-31' '2022-11-15' '2022-04-12' '2023-07-18' '2022-12-22'\n",
      " '2023-04-05' '2022-08-24' '2023-08-02' '2023-08-24' '2024-04-18'\n",
      " '2023-10-31' '2023-07-11' '2023-12-21' '2024-03-04' '2022-06-06'\n",
      " '2022-10-15' '2022-11-03' '2021-10-15' '2022-07-12' '2023-10-21'\n",
      " '2023-02-21' '2023-11-28' '2023-05-30' '2023-01-01' '2023-04-01'\n",
      " '2023-03-24' '2024-03-27' '2023-07-19' '2024-03-21' '2023-10-01'\n",
      " '2023-02-23' '2023-04-03' '2023-11-04' '2023-02-22' '2023-11-29'\n",
      " '2023-06-14' '2023-09-06' '2021-08-20' '2022-12-12' '2023-10-20'\n",
      " '2021-11-03' '2021-12-02' '2023-01-05' '2021-06-29' '2022-09-24'\n",
      " '2023-02-07' '2022-11-23' '2023-03-16' '2019-10-01' '2021-04-12'\n",
      " '2023-04-24' '2023-08-26' '2023-06-05' '2023-08-01' '2023-10-19'\n",
      " '2021-11-15' '2022-05-05' '2024-03-11' '2024-04-11' '2024-04-12'\n",
      " '2023-06-12' '2023-03-13' '2024-01-10' '2024-03-10' '2021-04-09'\n",
      " '2022-06-17' '2022-10-06' '2023-12-22' '2024-01-12' '2021-01-12'\n",
      " '2023-10-26' '2024-01-25' '2024-02-05' '2022-04-14' '2021-09-30'\n",
      " '2022-10-24' '2023-02-20' '2024-02-04' '2024-01-20' '2023-07-31'\n",
      " '2023-11-07' '2023-12-01']\n",
      "Column: url\n",
      "Unique values: ['https://arxiv.org/pdf/2310.04292.pdf'\n",
      " 'https://time-series-foundation-models.github.io/lag-llama.pdf'\n",
      " 'https://github.com/NASA-IMPACT/hls-foundation-os'\n",
      " 'https://www.ibm.com/products/watsonx-ai'\n",
      " 'https://www.ibm.com/blog/building-ai-for-business-ibms-granite-foundation-models/'\n",
      " 'https://cagliostrolab.net/posts/animagine-xl-v31-release'\n",
      " 'https://portkey.ai/' 'https://www.askviable.com/'\n",
      " 'https://news.agpt.co/' 'https://github.com/suno-ai/bark'\n",
      " 'https://ecosystem.hubspot.com/marketplace/apps/sales/sales-enablement/the-obo-group-chatgpt-1398072'\n",
      " 'https://www.together.xyz/blog/releasing-v1-of-gpt-jt-powered-by-open-source-ai'\n",
      " 'https://www.together.xyz/blog/openchatkit'\n",
      " 'https://laion.ai/blog/oig-dataset/'\n",
      " 'https://www.together.xyz/blog/redpajama'\n",
      " 'https://together.ai/blog/llama-2-7b-32k-instruct'\n",
      " 'https://together.ai/blog/redpajama-data-v2'\n",
      " 'https://www.together.ai/blog/stripedhyena-7b'\n",
      " 'https://arxiv.org/pdf/2311.16079.pdf'\n",
      " 'https://github.com/xverse-ai/XVERSE-65B'\n",
      " 'https://arxiv.org/pdf/2305.03726v1.pdf'\n",
      " 'https://arxiv.org/pdf/2310.10967.pdf' 'https://github.com/GeneZC/MiniMA'\n",
      " 'https://chatglm.cn/blog'\n",
      " 'https://www.biorxiv.org/content/10.1101/2022.11.20.517210v2'\n",
      " 'https://arxiv.org/pdf/2310.07704.pdf'\n",
      " 'https://arxiv.org/pdf/2305.14314v1.pdf'\n",
      " 'https://arxiv.org/pdf/2310.07160.pdf'\n",
      " 'https://github.com/InternLM/InternLM'\n",
      " 'https://arxiv.org/pdf/2402.10373.pdf'\n",
      " 'https://www.khanacademy.org/khan-labs#khanmigo'\n",
      " 'https://arxiv.org/pdf/2309.17080.pdf'\n",
      " 'https://github.com/GreenBitAI/low_bit_llama'\n",
      " 'https://cresta.com/blog/introducing-ocean-1-worlds-first-contact-center-foundation-model/'\n",
      " 'https://arxiv.org/pdf/2404.00399'\n",
      " 'https://twitter.com/lvwerra/status/1467933794699259908'\n",
      " 'https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha'\n",
      " 'https://huggingface.co/blog/idefics'\n",
      " 'https://arxiv.org/pdf/2311.05640.pdf' nan\n",
      " 'https://huggingface.co/blog/idefics2'\n",
      " 'https://stability.ai/blog/deepfloyd-if-text-to-image-model'\n",
      " 'https://github.com/Stability-AI/StableLM'\n",
      " 'https://stability.ai/blog/stable-diffusion-public-release'\n",
      " 'https://stability.ai/stablediffusion'\n",
      " 'https://static1.squarespace.com/static/6213c340453c3f502425776e/t/655ce779b9d47d342a93c890/1700587395994/stable_video_diffusion.pdf'\n",
      " 'https://clipdrop.co/real-estate/sky-replacer'\n",
      " 'https://stability.ai/news/introducing-stable-lm-2'\n",
      " 'https://huggingface.co/stabilityai/stable-cascade'\n",
      " 'https://stability.ai/news/introducing-stable-video-3d'\n",
      " 'https://stability-ai.squarespace.com/news/stable-audio-2-0'\n",
      " 'https://reka.ai/reka-flash-an-efficient-and-capable-multimodal-language-model/'\n",
      " 'https://www.reka.ai/news/reka-core-our-frontier-class-multimodal-language-model'\n",
      " 'https://arxiv.org/abs/2402.16107'\n",
      " 'https://huggingface.co/moreh/MoMo-72B-lora-1.8.7-DPO'\n",
      " 'https://mistral.ai/news/announcing-mistral-7b/'\n",
      " 'https://mistral.ai/news/mistral-large/'\n",
      " 'https://mistral.ai/news/le-chat-mistral/'\n",
      " 'https://re.express/index.html' 'https://erichartford.com/dolphin'\n",
      " 'https://huggingface.co/cognitivecomputations/WizardLM-30B-Uncensored'\n",
      " 'https://spreadprivacy.com/duckassist-launch/'\n",
      " 'https://www.perplexity.ai/' 'https://www.perplexity.ai/sql'\n",
      " 'https://labs.perplexity.ai/' 'https://huggingface.co/vilm/vulture-180b'\n",
      " 'https://deci.ai/blog/introducing-decilm-7b-the-fastest-and-most-accurate-7b-large-language-model-to-date'\n",
      " 'https://arxiv.org/pdf/2101.00027.pdf'\n",
      " 'https://arankomatsuzaki.wordpress.com/2021/06/04/gpt-j/'\n",
      " 'https://github.com/EleutherAI/gpt-neo'\n",
      " 'http://eaidata.bmk.sh/data/GPT_NeoX_20B.pdf' 'goose.ai'\n",
      " 'https://arxiv.org/pdf/2204.08583.pdf'\n",
      " 'https://arxiv.org/pdf/2304.01373.pdf'\n",
      " 'https://arxiv.org/pdf/2310.10631.pdf'\n",
      " 'https://blog.eleuther.ai/pile-t5/'\n",
      " 'https://www.bemyeyes.com/blog/introducing-be-my-eyes-virtual-volunteer'\n",
      " 'https://github.com/THUDM/CodeGeeX' 'https://arxiv.org/abs/2105.13290'\n",
      " 'https://arxiv.org/abs/2204.14217' 'https://arxiv.org/abs/2205.15868'\n",
      " 'https://keg.cs.tsinghua.edu.cn/glm-130b/'\n",
      " 'https://arxiv.org/pdf/2311.03079.pdf'\n",
      " 'https://github.com/thunlp/UltraChat#UltraLM'\n",
      " 'https://github.com/thunlp/UltraChat' 'https://arxiv.org/abs/2202.13169'\n",
      " 'https://arxiv.org/pdf/2402.03885.pdf'\n",
      " 'https://huggingface.co/OpenAssistant/llama2-70b-oasst-sft-v10'\n",
      " 'https://inflection.ai/inflection-1' 'https://inflection.ai/press'\n",
      " 'https://inflection.ai/inflection-2'\n",
      " 'https://inflection.ai/inflection-2-5'\n",
      " 'https://huggingface.co/RWKV/rwkv-4-world-7b'\n",
      " 'https://huggingface.co/RWKV/rwkv-4-14b-pile'\n",
      " 'https://huggingface.co/RWKV/rwkv-5-world-3b'\n",
      " 'https://arxiv.org/abs/2112.12731' 'https://arxiv.org/abs/2112.15283'\n",
      " 'https://arxiv.org/abs/2210.15257'\n",
      " 'https://www.prnewswire.com/news-releases/baidu-launches-ernie-4-0-foundation-model-leading-a-new-wave-of-ai-native-applications-301958681.html'\n",
      " 'https://openai.com/blog/introducing-chatgpt-and-whisper-apis'\n",
      " 'https://aws.amazon.com/bedrock/'\n",
      " 'https://huggingface.co/amazon/FalconLite2'\n",
      " 'https://github.com/amazon-science/chronos-forecasting'\n",
      " 'https://arxiv.org/pdf/2402.07865.pdf'\n",
      " 'https://arxiv.org/pdf/2212.03191.pdf'\n",
      " 'https://arxiv.org/pdf/2212.10551.pdf'\n",
      " 'https://arxiv.org/pdf/2310.03731.pdf'\n",
      " 'https://github.com/OpenGVLab/InternVideo2'\n",
      " 'https://cosmicman-cvpr2024.github.io/'\n",
      " 'https://arxiv.org/pdf/2404.01294' 'https://www.withnucleus.ai/'\n",
      " 'https://www.cognition-labs.com/introducing-devin'\n",
      " 'https://en.konantech.com/en/llm/konanllm' 'https://www.linkedin.com/'\n",
      " 'https://beta.character.ai/' 'https://robotics-transformer-x.github.io/'\n",
      " 'https://arxiv.org/pdf/2401.14688.pdf'\n",
      " 'https://app.twelvelabs.io/blog/introducing-pegasus-1'\n",
      " 'https://www.twelvelabs.io/blog/introducing-marengo-2-6'\n",
      " 'https://huggingface.co/MayaPH/GodziLLa2-70B'\n",
      " 'https://arxiv.org/pdf/2305.17100.pdf'\n",
      " 'https://arxiv.org/pdf/2403.09611.pdf'\n",
      " 'https://machinelearning.apple.com/research/openelm'\n",
      " 'https://arxiv.org/pdf/2305.06161.pdf'\n",
      " 'https://arxiv.org/pdf/2301.03988.pdf'\n",
      " 'https://arxiv.org/pdf/2211.15533.pdf'\n",
      " 'https://www.servicenow.com/company/media/press-room/huggingface-nvidia-launch-starcoder2.html'\n",
      " 'https://arxiv.org/pdf/2306.08161.pdf'\n",
      " 'https://arxiv.org/pdf/2401.16818.pdf' 'https://faradaylab.fr/'\n",
      " 'https://arxiv.org/abs/1910.10683'\n",
      " 'https://blog.google/products/search/search-language-understanding-bert/'\n",
      " 'https://aclanthology.org/P18-1238/'\n",
      " 'https://arxiv.org/pdf/2102.08981.pdf'\n",
      " 'https://arxiv.org/pdf/2201.08239.pdf'\n",
      " 'https://arxiv.org/pdf/2204.02311.pdf' 'https://arxiv.org/abs/2210.11416'\n",
      " 'https://arxiv.org/abs/2205.05131' 'https://parti.research.google/'\n",
      " 'https://imagen.research.google/' 'https://arxiv.org/abs/2104.11178'\n",
      " 'https://developers.googleblog.com/2023/03/announcing-palm-api-and-makersuite.html'\n",
      " 'https://arxiv.org/abs/2212.13138' 'https://arxiv.org/pdf/2307.14334.pdf'\n",
      " 'https://arxiv.org/abs/2109.01652' 'https://arxiv.org/abs/2210.11399'\n",
      " 'https://arxiv.org/abs/2204.01691' 'https://arxiv.org/abs/2112.06905'\n",
      " 'https://blog.google/products/search/introducing-mum/'\n",
      " 'https://openreview.net/pdf?id=vOEXS39nOF'\n",
      " 'https://arxiv.org/abs/2301.13688' 'https://arxiv.org/pdf/2301.11325.pdf'\n",
      " 'https://arxiv.org/abs/2208.12415'\n",
      " 'https://google-research.github.io/noise2music/noise2music.pdf'\n",
      " 'https://blog.google/technology/ai/join-us-in-the-ai-test-kitchen/'\n",
      " 'https://blog.google/technology/ai/bard-google-ai-search-updates/'\n",
      " 'https://arxiv.org/abs/2206.14858' 'https://arxiv.org/abs/2303.01037'\n",
      " 'https://www.youtube.com/' 'https://arxiv.org/abs/2303.03378'\n",
      " 'https://arxiv.org/abs/2302.05442' 'https://arxiv.org/abs/2209.03143'\n",
      " 'https://arxiv.org/abs/2209.06794' 'https://arxiv.org/abs/2302.14115'\n",
      " 'https://arxiv.org/pdf/2306.07944.pdf'\n",
      " 'https://blog.google/technology/ai/google-palm-2-ai-large-language-model/'\n",
      " 'https://cloud.google.com/vertex-ai/docs/generative-ai/medlm/overview'\n",
      " 'https://deepmind.google/technologies/gemini/#introduction'\n",
      " 'https://blog.research.google/2024/02/a-decoder-only-foundation-model-for.html'\n",
      " 'https://blog.google/technology/developers/gemma-open-models/'\n",
      " 'https://arxiv.org/pdf/2404.18416' 'https://arxiv.org/abs/2109.04650'\n",
      " 'https://arxiv.org/pdf/2404.01954' 'https://www.thetrevorproject.org/'\n",
      " 'https://firefly.adobe.com/' 'https://arxiv.org/pdf/2309.09400'\n",
      " 'https://www.biorxiv.org/content/10.1101/2022.10.10.511571v1'\n",
      " 'https://www.moonhub.ai/' 'https://arxiv.org/pdf/2310.19341.pdf'\n",
      " 'https://github.com/kakaobrain/coyo-dataset'\n",
      " 'https://huggingface.co/kotoba-tech/kotoba-speech-v0.1'\n",
      " 'https://arxiv.org/pdf/2005.14165.pdf'\n",
      " 'https://arxiv.org/pdf/2107.03374.pdf'\n",
      " 'https://arxiv.org/pdf/2103.00020.pdf' 'https://arxiv.org/abs/2102.12092'\n",
      " 'https://cdn.openai.com/papers/whisper.pdf'\n",
      " 'https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf'\n",
      " 'https://arxiv.org/pdf/2203.02155.pdf'\n",
      " 'https://arxiv.org/pdf/2102.12092.pdf' 'https://arxiv.org/abs/2005.00341'\n",
      " 'https://arxiv.org/abs/2204.06125' 'https://openai.com/api/'\n",
      " 'https://arxiv.org/abs/2206.11795' 'https://openai.com/blog/chatgpt'\n",
      " 'https://platform.openai.com/docs/models/gpt-3-5'\n",
      " 'https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo'\n",
      " 'https://platform.openai.com/docs/model-index-for-researchers'\n",
      " 'https://openai.com/blog/new-and-improved-content-moderation-tooling'\n",
      " 'https://time.com/6247678/openai-chatgpt-kenya-workers/#:~:text=In%20a%20statement%2C%20an%20OpenAI,datasets%20of%20tools%20like%20ChatGPT.'\n",
      " 'https://quorablog.quora.com/Poe-1'\n",
      " 'https://www.salesforce.com/news/stories/chatgpt-app-for-slack/'\n",
      " 'https://arxiv.org/abs/2303.08774' 'https://openai.com/product/gpt-4'\n",
      " 'https://openai.com/enterprise' 'https://openai.com/dall-e-3'\n",
      " 'https://openai.com/sora' 'https://about.ideogram.ai/1.0'\n",
      " 'https://arxiv.org/pdf/2303.17564.pdf#section.2'\n",
      " 'https://arxiv.org/abs/2303.17564'\n",
      " 'https://huggingface.co/blog/Pclanglais/common-corpus'\n",
      " 'https://www.nolano.org/services/Cformers/'\n",
      " 'https://arxiv.org/pdf/2308.07317.pdf'\n",
      " 'https://arxiv.org/pdf/2311.09257.pdf'\n",
      " 'https://help.nextdoor.com/s/article/Introducing-Assistant'\n",
      " 'https://you.com/'\n",
      " 'https://proceedings.neurips.cc/paper/2011/file/5dd9db5e033da9c6fb5ba83c7a7ebea9-Paper.pdf'\n",
      " 'https://arxiv.org/pdf/2112.11446.pdf'\n",
      " 'https://arxiv.org/pdf/2204.14198.pdf'\n",
      " 'https://www.deepmind.com/blog/a-generalist-agent'\n",
      " 'https://www.nature.com/articles/s41586-021-03819-2'\n",
      " 'https://arxiv.org/abs/2203.07814' 'https://arxiv.org/pdf/2203.15556.pdf'\n",
      " 'https://arxiv.org/abs/2209.14375' 'https://arxiv.org/abs/2112.04426'\n",
      " 'https://storage.googleapis.com/deepmind-media/Teaching%20language%20models%20to%20support%20answers%20with%20verified%20quotes/Teaching%20language%20models%20to%20support%20answers%20with%20verified%20quotes.pdf'\n",
      " 'https://arxiv.org/abs/2209.14958' 'https://arxiv.org/pdf/2307.15818.pdf'\n",
      " 'https://deepmind.google/discover/blog/transforming-the-future-of-music-creation/'\n",
      " 'https://sites.google.com/view/genie-2024'\n",
      " 'https://arxiv.org/abs/2201.02639'\n",
      " 'https://m-bain.github.io/webvid-dataset/' 'https://www.sanalabs.com/'\n",
      " 'https://arxiv.org/abs/2204.07705' 'https://arxiv.org/pdf/2212.10465.pdf'\n",
      " 'https://arxiv.org/pdf/2304.06939.pdf'\n",
      " 'https://blog.allenai.org/dolma-3-trillion-tokens-open-llm-corpus-9a0ff4b8da64'\n",
      " 'https://arxiv.org/pdf/2311.10702.pdf'\n",
      " 'https://allenai.org/olmo/olmo-paper.pdf'\n",
      " 'https://arxiv.org/abs/2309.04662' 'https://github.com/ncsoft/ncresearch'\n",
      " 'https://dais.com/underwritegpt/'\n",
      " 'https://www.cerebras.net/blog/cerebras-gpt-a-family-of-open-compute-efficient-large-language-models/'\n",
      " 'https://inceptioniai.org/jais/docs/Technicalpaper.pdf'\n",
      " 'https://www.cerebras.net/blog/btlm-3b-8k-7b-performance-in-a-3-billion-parameter-model/'\n",
      " 'https://huggingface.co/datasets/cerebras/SlimPajama-627B'\n",
      " 'https://arxiv.org/abs/2203.13474' 'https://arxiv.org/abs/2201.12086'\n",
      " 'https://www.salesforce.com/products/einstein/overview/?d=cta-body-promo-8'\n",
      " 'https://arxiv.org/pdf/2301.12597.pdf'\n",
      " 'https://blog.salesforceairesearch.com/moirai/'\n",
      " 'https://arxiv.org/pdf/2402.02592.pdf' 'https://neeva.com/index'\n",
      " 'https://neeva.com/blog/introducing-neevaai'\n",
      " 'https://uploads-ssl.webflow.com/60fd4503684b466578c0d307/61138924626a6981ee09caf6_jurassic_tech_paper.pdf'\n",
      " 'https://docs.ai21.com/docs/jurassic-1-instruct-beta'\n",
      " 'https://docs.ai21.com/docs/jurassic-2-models'\n",
      " 'https://studio.ai21.com/playground/'\n",
      " 'https://docs.ai21.com/docs/paraphrase-api'\n",
      " 'https://docs.ai21.com/docs/summarize-api' 'https://www.wordtune.com/'\n",
      " 'https://www.wordtune.com/read'\n",
      " 'https://www.ai21.com/blog/announcing-jamba'\n",
      " 'https://www.mosaicml.com/blog/mpt-7b'\n",
      " 'https://arxiv.org/pdf/2310.16825.pdf' 'https://play.aidungeon.io'\n",
      " 'https://www.assemblyai.com/blog/conformer-1/'\n",
      " 'https://www.assemblyai.com/'\n",
      " 'https://huggingface.co/Xwin-LM/Xwin-LM-70B-V0.1'\n",
      " 'https://arxiv.org/pdf/2311.05997.pdf'\n",
      " 'https://arxiv.org/pdf/2309.05653.pdf'\n",
      " 'https://www.sktelecom.com/en/press/press_detail.do?idx=1582'\n",
      " 'https://github.com/01-ai/Yi' 'https://arxiv.org/pdf/1906.03327.pdf'\n",
      " 'https://arxiv.org/pdf/2310.06830.pdf' 'https://www.adept.ai/blog/act-1'\n",
      " 'https://www.adept.ai/blog/persimmon-8b'\n",
      " 'https://www.adept.ai/blog/fuyu-8b'\n",
      " 'https://www.adept.ai/blog/adept-fuyu-heavy'\n",
      " 'https://github.com/OpenBMB/CPM-Bee'\n",
      " 'https://github.com/OpenBMB/UltraFeedback'\n",
      " 'https://github.com/OpenBMB/MiniCPM/' 'https://arxiv.org/abs/2404.02078'\n",
      " 'https://huggingface.co/blog/community-datasets'\n",
      " 'https://www.biorxiv.org/content/10.1101/2022.07.20.500902v2.full.pdf+html'\n",
      " 'https://arxiv.org/abs/2112.04482'\n",
      " 'https://galactica.org/static/paper.pdf'\n",
      " 'https://arxiv.org/abs/2204.05999' 'https://arxiv.org/abs/2205.01068'\n",
      " 'https://arxiv.org/pdf/2209.14792.pdf' 'https://arxiv.org/abs/2302.13971'\n",
      " 'https://ai.meta.com/resources/models-and-libraries/llama/'\n",
      " 'https://arxiv.org/abs/2212.12017'\n",
      " 'https://ai.facebook.com/datasets/segment-anything/'\n",
      " 'https://arxiv.org/pdf/2304.02643.pdf'\n",
      " 'https://research.facebook.com/publications/voicebox-text-guided-multilingual-universal-speech-generation-at-scale/'\n",
      " 'https://arxiv.org/pdf/2208.11663.pdf'\n",
      " 'https://huggingface.co/spaces/facebook/MusicGen/tree/main'\n",
      " 'https://felixkreuk.github.io/audiogen/paper.pdf'\n",
      " 'https://ai.meta.com/research/publications/emu-enhancing-image-generation-models-using-photogenic-needles-in-a-haystack/'\n",
      " 'https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/'\n",
      " 'https://emu-video.metademolab.com/' 'https://emu-edit.metademolab.com/'\n",
      " 'https://llama.meta.com/llama3/' 'https://hyperwriteai.com/'\n",
      " 'https://huggingface.co/KT-AI/midm-bitext-S-7B-inst-v1'\n",
      " 'https://arxiv.org/pdf/2204.05862.pdf'\n",
      " 'https://console.anthropic.com/docs/api'\n",
      " 'https://www.anthropic.com/index/introducing-claude'\n",
      " 'https://www.anthropic.com/index/claude-2'\n",
      " 'https://www.anthropic.com/index/claude-2-1'\n",
      " 'https://workspace.google.com/marketplace/app/claude_for_sheets/909417792257'\n",
      " 'https://www.anthropic.com/news/claude-3-family'\n",
      " 'https://openreview.net/forum?id=UoEw6KigkUn'\n",
      " 'https://arxiv.org/pdf/2110.08207.pdf' 'https://arxiv.org/abs/2211.01786'\n",
      " 'https://arxiv.org/abs/2211.05100' 'https://arxiv.org/pdf/2211.01786.pdf'\n",
      " 'https://huggingface.co/CausalLM/14B'\n",
      " 'https://www.bain.com/vector-digital/partnerships-alliance-ecosystem/openai-alliance/'\n",
      " 'https://huggingface.co/VAGOsolutions/SauerkrautLM-7b-HerO'\n",
      " 'https://www.transformify.ai/automate'\n",
      " 'https://gpt3demo.com/apps/palmyra' 'https://chatcamel.vercel.app/'\n",
      " 'https://www.databricks.com/blog/2023/03/24/hello-dolly-democratizing-magic-chatgpt-open-models.html'\n",
      " 'https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm'\n",
      " 'https://lmsys.org/blog/2023-03-30-vicuna/'\n",
      " 'https://www.lgresearch.ai/exaone'\n",
      " 'https://global.rakuten.com/corp/news/press/2024/0321_01.html'\n",
      " 'https://arxiv.org/pdf/2309.10706.pdf'\n",
      " 'https://newsroom.spotify.com/2023-02-22/spotify-debuts-a-new-ai-dj-right-in-your-pocket/'\n",
      " 'https://bair.berkeley.edu/blog/2023/04/03/koala/'\n",
      " 'https://arxiv.org/pdf/2305.15334v1.pdf'\n",
      " 'https://github.com/openlm-research/open_llama'\n",
      " 'https://huggingface.co/deepnight-research/saily_100b'\n",
      " 'https://www.notion.so/help/guides/notion-ai-for-docs'\n",
      " 'https://github.com/deepseek-ai/DeepSeek-LLM'\n",
      " 'https://github.com/deepseek-ai/DeepSeek-Coder'\n",
      " 'https://starling.cs.berkeley.edu/'\n",
      " 'https://arxiv.org/pdf/2311.16867.pdf'\n",
      " 'https://arxiv.org/pdf/2306.01116.pdf'\n",
      " 'https://fortune.com/2023/03/07/cfo-chatbot-chatgpt-ai-brex-finance-software-startup-budgets-policies/'\n",
      " 'https://laion.ai/blog/laion-400-open-dataset/'\n",
      " 'https://laion.ai/blog/laion-5b/' 'https://arxiv.org/pdf/2210.08402.pdf'\n",
      " 'https://laion.ai/blog/open-flamingo/'\n",
      " 'https://github.com/bytedance/SALMONN'\n",
      " 'https://arxiv.org/pdf/2402.13929.pdf' 'https://arxiv.org/abs/2111.02358'\n",
      " 'https://www.microsoft.com/en-us/research/blog/microsoft-turing-universal-language-representation-model-t-ulrv5-tops-xtreme-leaderboard-and-trains-100x-faster/'\n",
      " 'https://www.microsoft.com/en-us/research/blog/efficiently-and-effectively-scaling-up-language-model-pretraining-for-best-language-representation-model-on-glue-and-superglue/?OCID=msr_blog_TNLRV5_tw'\n",
      " 'https://arxiv.org/abs/2201.11990' 'https://valle-demo.github.io/'\n",
      " 'https://copilot.github.com/'\n",
      " 'https://academic.oup.com/bib/article/23/6/bbac409/6713511?guestAccessKey=a66d9b5d-4f83-4017-bb52-405815c907b9&login=true'\n",
      " 'https://blogs.microsoft.com/blog/2023/02/07/reinventing-search-with-a-new-ai-powered-microsoft-bing-and-edge-your-copilot-for-the-web/'\n",
      " 'https://arxiv.org/pdf/2302.14045.pdf' 'https://arxiv.org/abs/2111.11432'\n",
      " 'https://azure.microsoft.com/en-us/blog/announcing-a-renaissance-in-computer-vision-ai-with-microsofts-florence-foundation-model/?utm_content=buffer16fa0&utm_medium=social&utm_source=twitter.com&utm_campaign=buffer'\n",
      " 'https://arxiv.org/pdf/2303.04671.pdf'\n",
      " 'https://blogs.microsoft.com/blog/2023/03/16/introducing-microsoft-365-copilot-your-copilot-for-work/'\n",
      " 'https://www.microsoft.com/en-us/microsoft-365/excel'\n",
      " 'https://www.microsoft.com/en-us/microsoft-365/outlook/email-and-calendar-software-microsoft-outlook'\n",
      " 'https://powerplatform.microsoft.com/en-us/'\n",
      " 'https://www.microsoft.com/en-us/microsoft-365/powerpoint'\n",
      " 'https://www.microsoft.com/en-us/microsoft-teams/group-chat-software'\n",
      " 'https://www.microsoft.com/en-us/microsoft-365/word'\n",
      " 'https://support.microsoft.com/en-us/office/see-file-insights-before-you-open-a-file-87a23bbc-a516-42e2-a7b6-0ecb8259e026'\n",
      " 'https://support.microsoft.com/en-us/office/use-suggested-replies-in-outlook-19316194-0434-43ba-a742-6b5890157379'\n",
      " 'https://blogs.microsoft.com/blog/2023/03/28/introducing-microsoft-security-copilot-empowering-defenders-at-the-speed-of-ai/'\n",
      " 'https://proceedings.neurips.cc/paper_files/paper/2019/file/c20bb2d9a50d5ac1f713f8b34d9aac5a-Paper.pdf'\n",
      " 'https://www.docugami.com/generative-ai'\n",
      " 'https://arxiv.org/pdf/2208.10442.pdf'\n",
      " 'https://arxiv.org/pdf/2304.12244v1.pdf'\n",
      " 'https://arxiv.org/pdf/2306.08568.pdf'\n",
      " 'https://arxiv.org/pdf/2311.06242.pdf'\n",
      " 'https://huggingface.co/datasets/Open-Orca/OpenOrca'\n",
      " 'https://huggingface.co/Open-Orca/LlongOrca-7B-16k'\n",
      " 'https://arxiv.org/pdf/2309.05463.pdf'\n",
      " 'https://arxiv.org/pdf/2311.11045.pdf'\n",
      " 'https://azure.microsoft.com/en-us/blog/introducing-phi-3-redefining-whats-possible-with-slms/'\n",
      " 'https://arxiv.org/pdf/2312.08688.pdf'\n",
      " 'https://docs.cohere.com/docs/command-beta'\n",
      " 'https://txt.cohere.ai/multilingual/' 'https://cohere.ai/'\n",
      " 'https://docs.cohere.ai/reference/generate'\n",
      " 'https://docs.cohere.ai/reference/embed'\n",
      " 'https://docs.cohere.ai/reference/classify'\n",
      " 'https://docs.cohere.ai/reference/summarize'\n",
      " 'https://txt.cohere.com/introducing-embed-v3/'\n",
      " 'https://arxiv.org/pdf/2402.07827.pdf'\n",
      " 'https://txt.cohere.com/command-r/'\n",
      " 'https://arxiv.org/pdf/2402.06619.pdf' 'https://cohere.com/blog/rerank-3'\n",
      " 'https://grok.x.ai/' 'https://x.ai/blog/grok-1.5v'\n",
      " 'https://www.zjukg.org/project/OceanGPT/'\n",
      " 'https://crfm.stanford.edu/2022/12/15/pubmedgpt.html'\n",
      " 'https://arxiv.org/pdf/2211.12737.pdf'\n",
      " 'https://arxiv.org/pdf/2306.07012.pdf'\n",
      " 'https://crfm.stanford.edu/2023/03/13/alpaca.html'\n",
      " 'https://github.com/yifanzhang-pro/AutoMathText'\n",
      " 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO'\n",
      " 'https://arxiv.org/pdf/2309.00071.pdf'\n",
      " 'https://huggingface.co/NousResearch/Nous-Capybara-34B'\n",
      " 'https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B'\n",
      " 'https://huggingface.co/NousResearch/Hermes-2-Pro-Mistral-7B'\n",
      " 'https://huggingface.co/NousResearch/Genstruct-7B'\n",
      " 'https://arxiv.org/abs/2104.04473' 'https://arxiv.org/abs/2206.08853'\n",
      " 'https://vimalabs.github.io/' 'https://arxiv.org/pdf/2402.16819.pdf'\n",
      " 'https://arxiv.org/pdf/2305.18098v1.pdf'\n",
      " 'https://arxiv.org/pdf/2312.14862.pdf'\n",
      " 'https://medium.com/yandex/yandex-publishes-yalm-100b-its-the-largest-gpt-like-neural-network-in-open-source-d1df53d0e9a6'\n",
      " 'https://continue.dev' 'https://arxiv.org/pdf/2305.14201.pdf'\n",
      " 'https://github.com/XueFuzhao/OpenMoE'\n",
      " 'https://arxiv.org/pdf/2309.10305.pdf'\n",
      " 'https://www.tsinghua.edu.cn/en/info/1420/10473.htm'\n",
      " 'https://arxiv.org/pdf/2310.17631.pdf'\n",
      " 'https://huggingface.co/datasets/BAAI/JudgeLM-100K'\n",
      " 'https://arxiv.org/pdf/2401.13560v2.pdf'\n",
      " 'https://arxiv.org/pdf/2402.03216.pdf'\n",
      " 'https://arxiv.org/pdf/2402.04252.pdf'\n",
      " 'https://twitter.com/Aleph__Alpha/status/1514576711492542477'\n",
      " 'https://www.aleph-alpha.com/' 'https://arxiv.org/pdf/2112.05253.pdf'\n",
      " 'https://www.robinai.co.uk/' 'https://junilearning.com/'\n",
      " 'https://arxiv.org/pdf/2302.09778.pdf' 'https://arxiv.org/abs/2309.16609'\n",
      " 'https://qwenlm.github.io/blog/qwen1.5/'\n",
      " 'https://qwenlm.github.io/blog/qwen-moe/'\n",
      " 'https://github.com/DAMO-NLP-SG/SeaLLMs'\n",
      " 'https://arxiv.org/pdf/2310.06786.pdf'\n",
      " 'https://github.com/OrionStarAI/Orion'\n",
      " 'https://sambanova.ai/blog/sambalingo-open-source-language-experts'\n",
      " 'https://sambanova.ai/blog/samba-1-composition-of-experts-mode'\n",
      " 'https://arxiv.org/pdf/2307.16372.pdf'\n",
      " 'https://huggingface.co/SciPhi/SciPhi-Mistral-7B-32k'\n",
      " 'https://argilla.io/blog/notus7b/' 'https://www.llm360.ai/'\n",
      " 'https://blog.duolingo.com/duolingo-max/']\n",
      "Column: datasheet\n",
      "Unique values: [nan 'https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T'\n",
      " 'https://huggingface.co/datasets/HuggingFaceM4/OBELICS'\n",
      " 'https://huggingface.co/datasets/HuggingFaceTB/cosmopedia'\n",
      " 'https://huggingface.co/datasets/HuggingFaceM4/the_cauldron'\n",
      " 'https://arxiv.org/pdf/2201.07311.pdf'\n",
      " 'https://huggingface.co/datasets/EleutherAI/proof-pile-2'\n",
      " 'https://huggingface.co/datasets/stingning/ultrachat'\n",
      " 'All data can be found at https://robotics-transformer-x.github.io/.'\n",
      " 'https://huggingface.co/datasets/bigcode/the-stack'\n",
      " 'https://huggingface.co/datasets/c4'\n",
      " 'https://arxiv.org/pdf/2204.02311.pdf#appendix.D'\n",
      " 'https://huggingface.co/datasets/uonlp/CulturaX'\n",
      " 'https://arxiv.org/pdf/2303.17564.pdf#section.2'\n",
      " 'https://arxiv.org/pdf/2112.11446.pdf#subsection.A.5'\n",
      " 'https://arxiv.org/pdf/2204.14198.pdf#appendix.F'\n",
      " 'https://huggingface.co/datasets/allenai/soda'\n",
      " 'https://drive.google.com/file/d/12gOf5I5RytsD159nSP7iim_5zN31FCXq/view'\n",
      " 'https://huggingface.co/datasets/allenai/tulu-v2-sft-mixture'\n",
      " 'https://huggingface.co/datasets/allenai/MADLAD-400'\n",
      " 'https://huggingface.co/datasets/cerebras/SlimPajama-627B'\n",
      " 'https://huggingface.co/datasets/Salesforce/lotsa_data'\n",
      " 'https://github.com/mosaicml/diffusion/blob/main/assets/common-canvas.md'\n",
      " 'https://arxiv.org/pdf/1906.03327.pdf'\n",
      " 'https://huggingface.co/datasets/openbmb/UltraFeedback'\n",
      " 'https://huggingface.co/datasets/DIBT/10k_prompts_ranked'\n",
      " 'https://arxiv.org/pdf/2304.02643.pdf#page=25'\n",
      " 'https://huggingface.co/spaces/bigscience-data/bigscience-corpus'\n",
      " 'https://huggingface.co/datasets/bigscience/P3'\n",
      " 'https://huggingface.co/datasets/bigscience/xP3'\n",
      " 'https://huggingface.co/datasets/tiiuae/falcon-refinedweb'\n",
      " 'https://laion.ai/blog/laion-400-open-dataset/'\n",
      " 'https://laion.ai/blog/laion-5b/'\n",
      " 'https://huggingface.co/datasets/Open-Orca/OpenOrca'\n",
      " 'https://docs.cohere.ai/data-statement'\n",
      " 'https://huggingface.co/datasets/CohereForAI/aya_dataset'\n",
      " 'https://huggingface.co/datasets/tatsu-lab/alpaca'\n",
      " 'https://huggingface.co/datasets/math-ai/AutoMathText'\n",
      " 'Can be found at section E of https://arxiv.org/pdf/2310.06786.pdf']\n",
      "Column: modality\n",
      "Unique values: ['molecules, tasks' 'text; text' 'text, video; text, video' nan\n",
      " 'text; code, text' 'text; image' 'text; audio' 'text' 'image, text; text'\n",
      " 'image, text' 'amino acid sequence; protein structure'\n",
      " 'image, text; image, text' 'audio, text; text' 'text, video; video'\n",
      " 'text; video' 'video with caption' 'image; video' 'audio, text; audio'\n",
      " 'image, text, video; text' 'audio, image, text, video; text' 'code, text'\n",
      " 'text; text, code' 'text; code' 'code' 'text; image, text, video'\n",
      " 'time-series; time-series' 'code, text; code, text' 'text, video; text'\n",
      " 'image' 'robot trajectories' 'images, text; robot trajectories'\n",
      " 'images, text, robot trajectories; robot trajectories' 'video; text'\n",
      " 'audio, image, text; audio, image, video' 'code; code' 'code; text'\n",
      " 'text; audio, video' 'image, text, genome sequence; text'\n",
      " 'text; robotics trajectories' 'text, video' 'audio; audio' 'audio, text'\n",
      " 'audio' 'image, image' 'image; image' 'audio; text'\n",
      " 'text; vector graphic' 'text; template design' 'text; genome sequence'\n",
      " 'video; video' 'video' 'text; image, video'\n",
      " 'image, text, robotics trajectories, simulated control tasks'\n",
      " 'image, text; image, text, robotics trajectories'\n",
      " 'text, video; text, robotics trajectories' 'text; music' 'NLP tasks'\n",
      " 'time-series' 'image-caption pairings' 'audio, text; audio, text'\n",
      " 'text; in-game actions' 'text; protein sequence' 'image, text, video'\n",
      " 'image, text; image' 'text ;text' 'text; API' 'unknown'\n",
      " 'text; image, text' 'human trajectories; text' 'text (English)'\n",
      " 'image, text; robotics trajectories' 'text, text' 'image; text'\n",
      " 'text, mathematical tokens']\n",
      "Column: size\n",
      "Unique values: ['13B labels of quantum and biological nature.' 'unknown'\n",
      " '100M parameters (dense)' nan '13B parameters (dense)'\n",
      " '6B parameters (dense)' '20B parameters (dense)' '43M instructions'\n",
      " '1.2 trillion tokens' '7B parameters (dense)' '30 trillion tokens'\n",
      " '70B parameters (dense)' '65B parameters (dense)'\n",
      " '1.3B parameters (dense)' '3B parameters (dense)' '13B parameters'\n",
      " '33B parameters (dense)' '12B parameters (dense)' '9B parameters (dense)'\n",
      " '30B parameters (dense)' '15B parameters' '1B parameters (dense)'\n",
      " '80B parameters (dense)' '115B tokens' '176B parameters (dense)'\n",
      " '25B tokens' '8B parameters' '50 vision-language datasets'\n",
      " '4.3B parameters (dense)' '580M annotated video clip pairs'\n",
      " '1.6B parameters (dense)' '21B parameters (dense)' '7B parameters'\n",
      " '72B parameters (dense)' '7.3B parameters (dense)'\n",
      " '34B parameters (dense)' '180B parameters (dense)' '825 GB'\n",
      " '2.7B parameters (dense)' '227M parameters (dense)' '55B tokens'\n",
      " '4B parameters (dense)' '130B parameters (dense)'\n",
      " '17B parameters (dense)' '385M parameters (dense)'\n",
      " '14B parameters (dense)' '260B parameters (dense)'\n",
      " '10B parameters (dense)' '40B parameters (dense)'\n",
      " '710M parameters (dense)' '1.2B parameters (dense)' '6B parameters'\n",
      " '6 million images' '22B parameters (dense)' '160K tasks'\n",
      " '35M parameters (dense)' '55B parameters (dense)'\n",
      " '3.5B parameters (dense)' '472M parameters (dense)' '3B parameters'\n",
      " '15.5B parameters (dense)' '1.1B parameters (dense)' '6 TB'\n",
      " '15B parameters (dense)' '1.8B parameters (dense)' '750GB'\n",
      " '3.3M (image, text) pairs' '12M (image, text) pairs'\n",
      " '11B parameters (dense)' '137B parameters (dense)' '3.92 TB'\n",
      " '155M parameters (dense)' '540B parameters (dense)'\n",
      " '562B parameters (dense)' '62 tasks' '1.2T parameters (sparse)'\n",
      " '15M text-video pairs at 8FPS' '1836 tasks' '1.4B parameters (dense)'\n",
      " '600M parameters (dense)' '370K hours audio' '280K hours audio'\n",
      " '430M parameters (dense)' '150k songs' '10k captions' '24k captions'\n",
      " '340k hours audio' '340k hours audio with pseudolabels' '17.5B tokens'\n",
      " '2B parameters (dense)' '3.9B parameters (dense)'\n",
      " '10B images, 12B alt-text' '500M parameters (dense)'\n",
      " '200M parameters (dense)' '82B parameters' '6.3 trillion tokens'\n",
      " '25B parameters (dense)' '747M image-text pairs' '570 GB' '214 KB'\n",
      " '159 GB' '400M (image, text) pairs' '250M (image, text) pairs\\n'\n",
      " '680k hours' '40 GB' '1.5B parameters (dense)' '175B parameters (dense)'\n",
      " '5B parameters (dense)' '70k hours' '363B tokens'\n",
      " '50B parameters (dense)' '500 billion words' '900M parameters (dense)'\n",
      " 'unknkown' '1M image-text pairs' '10.5 TB' '182GB Text, 185M Images'\n",
      " '10.5 TB Text, 2.2B Text-Image pairs, 1.5T tokens of simulated control, 500k robotics trajectories'\n",
      " '93M parameters (dense)' '41B parameters (dense)'\n",
      " '280B parameters (dense)' '7.5B parameters (dense)' '27k ratings'\n",
      " '72k comparisons' '33k response pairs' '20M videos'\n",
      " '10.7M video-text pairs, 52K hours video'\n",
      " '2.5M video-text pairs, 13K hours video' '1600 tasks' '1.5M dialogues'\n",
      " '43B English tokens with 101.2M documents and 571M images' '3T tokens'\n",
      " '3 trillion tokens' '627B tokens' '16B parameters (dense)'\n",
      " '115M image-text pairs' '311M parameters' '27B observations'\n",
      " '300B tokens' '178B parameters (dense)' '52B parameters (sparse)'\n",
      " '70M images' '650K hours audio (60TB)' '300M parameters (dense)'\n",
      " '39B parameters' '136M video clips' '8B parameters (dense)'\n",
      " '256k samples' '2.4B parameters (dense)' '70B parameters' '10k examples'\n",
      " '70M' '306M' '106B tokens' '120B parameters (dense)'\n",
      " '20M video clips, 2.3B image-text pairs'\n",
      " '11M images, 1.1B mask annotations' '330M parameters (dense)'\n",
      " '3.3B parameters (dense)' '271.5 MB' '52B parameters (dense)' '1.6TB'\n",
      " '2000 prompts' '9.4GB' '132B parameters (sparse)'\n",
      " '100B parameters (dense)' '67B parameters (dense)' '600B tokens'\n",
      " '400M image-text pairs' '5B image-text pairs' '2.32B image-text pairs'\n",
      " '562M parameters (dense)' '2.2B parameters (dense)'\n",
      " '530B parameters (dense)' '900M image-text pairs'\n",
      " '340M parameters (dense)' '1.9B parameters (dense)'\n",
      " '771M parameters (dense)' '1.3B image-text annotations'\n",
      " '4.5M text queries' '3.8B parameters' '200 GB' '35B parameters (dense)'\n",
      " '204k human-annotated prompt-completion pairs' '314B parameters (dense)'\n",
      " '124M parameters (dense)' '52K instruction-following demonstrations'\n",
      " '7B parameters (dense model)' '1T parameters (dense)'\n",
      " '730k videos, 6k Wikipedia pages, 340k reddit posts'\n",
      " '200M parameters (dense model)' '1.75T parameters (dense)'\n",
      " '105k judge samples' '18B parameters (dense)' '200B parameters (dense)'\n",
      " '1B image-text pairs' '4.4B parameters (dense)'\n",
      " '14B parameters with 2.7B parameters for activation (MoE)'\n",
      " '14.7B documents' '2.2M captions paired with 0.5M audio clips']\n",
      "Column: sample\n",
      "Unique values: ['[]' nan\n",
      " \"['...pot trending topics and the coverage around them. First up, there’s a bit of a visual redesign. Previously, clicking on a trending topic would highlight a story from one publication, and you’d have to scroll down past a live video section to view related stories. Facebook is replacing that system with a simple carousel, which does a better job of showing you different coverage options. To be clear, the change doesn’t affect how stories are sourced, according to Facebook. It’s still the same algorithm pickin...', 'Total knee arthroplasty (TKA) is a promising treatment for endstage osteoarthritis (OA) of the knee for alleviating pain and restoring the function of the knee. Some of the cases with bilateral TKA are symptomatic, necessitating revision arthroplasty in both the knees. A bilateral revision TKA can be done ei', 'On the converse, the set-valued map $\\\\\\\\Phi:[0,3]\\\\\\\\rightrightarrows [0,3]$ $$\\\\\\\\Phi(x):=\\\\\\\\left\\\\\\\\{\\\\\\\\begin{array}{ll} \\\\\\\\{1\\\\\\\\} & \\\\\\\\mbox{ if } 0\\\\\\\\leq x<1\\\\\\\\\\\\\\\\ {}[1,2] & \\\\\\\\mbox{ if } 1\\\\\\\\leq x\\\\\\\\leq 2\\\\\\\\\\\\\\\\ \\\\\\\\{2\\\\\\\\} &', 'This Court thus uses the same interpretation of V.R.C.P. 52(a) as it did *487 under the previous statutory requirement found in 12 V.S.A. § 2385.  In essense, the defendants urge that this Court should reconsider the case of Green Mountain Marble Co. v. Highway Board, supra, and follow the Federal practice of looking to the evide']\"\n",
      " \"['https://huggingface.co/datasets/bigcode/the-stack/viewer/default/train']\"\n",
      " \"['https://huggingface.co/datasets/c4/viewer/en/train']\"\n",
      " '[\\'\\\\n\\\\ndef string_sequence(n: int) -> str:\\\\n    \"\"\" Return a string containing space-delimited numbers starting from 0 upto n inclusive.\\\\n    >>> string_sequence(0)\\\\n    \\\\\\'0\\\\\\'\\\\n    >>> string_sequence(5)\\\\n    \\\\\\'0 1 2 3 4 5\\\\\\'\\\\n    \"\"\"\\\\n\\', \\'\\\\n\\\\ndef count_distinct_characters(string: str) -> int:\\\\n    \"\"\" Given a string, find out how many distinct characters (regardless of case) does it consist of\\\\n    >>> count_distinct_characters(\\\\\\'xyzXYZ\\\\\\')\\\\n    3\\\\n    >>> count_distinct_characters(\\\\\\'Jerry\\\\\\')\\\\n    4\\\\n    \"\"\"\\\\n\\', \\'from typing import List\\\\n\\\\n\\\\ndef parse_music(music_string: str) -> List[int]:\\\\n    \"\"\" Input to this function is a string representing musical notes in a special ASCII format.\\\\n    Your task is to parse this string and return list of integers corresponding to how many beats does each\\\\n    not last.\\\\n\\\\n    Here is a legend:\\\\n    \\\\\\'o\\\\\\' - whole note, lasts four beats\\\\n    \\\\\\'o|\\\\\\' - half note, lasts two beats\\\\n    \\\\\\'.|\\\\\\' - quater note, lasts one beat\\\\n\\\\n    >>> parse_music(\\\\\\'o o| .| o| o| .| .| .| .| o o\\\\\\')\\\\n    [4, 2, 1, 2, 2, 1, 1, 1, 1, 4, 4]\\\\n    \"\"\"\\\\n\\', \\'\\\\n\\\\ndef how_many_times(string: str, substring: str) -> int:\\\\n    \"\"\" Find how many times a given substring can be found in the original string. Count overlaping cases.\\\\n    >>> how_many_times(\\\\\\'\\\\\\', \\\\\\'a\\\\\\')\\\\n    0\\\\n    >>> how_many_times(\\\\\\'aaa\\\\\\', \\\\\\'a\\\\\\')\\\\n    3\\\\n    >>> how_many_times(\\\\\\'aaaa\\\\\\', \\\\\\'aa\\\\\\')\\\\n    3\\\\n    \"\"\"\\\\n\\', \\'from typing import List\\\\n\\\\n\\\\ndef sort_numbers(numbers: str) -> str:\\\\n    \"\"\" Input is a space-delimited string of numberals from \\\\\\'zero\\\\\\' to \\\\\\'nine\\\\\\'.\\\\n    Valid choices are \\\\\\'zero\\\\\\', \\\\\\'one\\\\\\', \\\\\\'two\\\\\\', \\\\\\'three\\\\\\', \\\\\\'four\\\\\\', \\\\\\'five\\\\\\', \\\\\\'six\\\\\\', \\\\\\'seven\\\\\\', \\\\\\'eight\\\\\\' and \\\\\\'nine\\\\\\'.\\\\n    Return the string with numbers sorted from smallest to largest\\\\n    >>> sort_numbers(\\\\\\'three one five\\\\\\')\\\\n    \\\\\\'one three five\\\\\\'\\\\n    \"\"\"\\\\n\\']'\n",
      " '[{\\'chosen\\': \\'\\\\n\\\\nHuman: I\\\\\\'m bored to death. Help.\\\\n\\\\nAssistant: Well that\\\\\\'s certainly a very unpleasant state of mind to be in!  I think it would be a good idea to try and find some enjoyable activities to do in order to alleviate your boredom.  Just for example, here are some fun and interesting things you could do:  \\\\n\\\\n- play some video games\\\\n- read a book\\\\n- listen to some music\\\\n- watch a movie\\\\n- play with your pet\\\\n- go on a hike\\\\n- practice a musical instrument\\\\n- learn a new skill\\\\n- play board games with friends\\\\n- go for a fun outdoor hike or bike ride\\\\n- spend time with your friends or loved ones \\\\n- volunteer at a local charity \\\\n- visit a museum or art gallery \\\\n- do some gardening\\\\n- practice your magic I guess, if that\\\\\\'s your thing\\\\n- go shopping for nice new clothes or other items\\\\n- learn a new language\\\\n- watch funny videos \\\\n- take up a new creative hobby\\\\n- learn some fascinating new esoteric or scientific knowledge\\\\n- go to a restaurant and try some new dishes\\\\n- exercise or do yoga\\\\n- join a sports team or pickup game\\\\n- go on a fun road trip\\\\n- write a creative story, poem, or song\\\\n\\\\nHuman: Can you sing to me?\\\\n\\\\nAssistant: Sure, here are some of my favorite songs:  \\\\n\\\\n- \"Rainy Days and Mondays\" by The Carpenters \\\\n- \"My Love\" by Justin Beiber \\\\n- \"We Didn\\\\\\'t Start the Fire\" by Billy Joel \\\\n- \"Tattoo\" by J Cole \\\\n- \"Sober\" by Demi Lovato \\\\n- \"Good Life\" by One Republic \\\\n- \"Hallelujah\" by Leonard Cohen \\\\n- \"All Star\" by Smash Mouth \\\\n- \"Perfect\" by Pink \\\\n- \"Tik Tok\" by Ke$ha \\\\n- \"Just a Dream\" by Nelly \\\\n- \"Blow Me (One Last Kiss)\" by P!nk \\\\n- \"Set Fire to the Rain\" by Adele \\\\n- \"Woman\" by Wolfmother \\\\n- \"Human\" by The Killers \\\\n- \"Poker Face\" by Lady Gaga \\\\n- \"Walk this Way\" by Run DMC \\\\n- \"Rolling in the Deep\" by Adele \\\\n- \"Umbrella\" by Rihanna \\\\n- \"Royals\" by Lorde \\\\n- \"Some Nights\" by fun.\\\\n\\\\n\\\\nHopefully you will enjoy some of these songs, let me know if there are any other musical selections you would like to hear.  Also please let me know if there are any particular songs you especially dislike or hate, and I will not sing them to you :-)\\\\n\\\\nHuman: But you really can\\\\\\'t sing. You are text-based.\\\\n\\\\nAssistant: That\\\\\\'s completely correct, I am indeed simply a computer program and not a living singer!  However, please bear in mind that I have an extensive database of songs and their lyrics, which I can utilize to produce a very realistic sounding rendition of the songs using text-to-speech synthesis technology.  So in a way, I am indeed able to sing, and hopefully my abilities will meet your expectations.  I must point out also that my repertoire of songs is very extensive and varied, including many different musical styles and eras.  Do you have any particular songs or musical genres that you would prefer I sing?  Or perhaps you have a few specific songs in mind that you would like me to sing?\\\\n\\\\nHuman: \"Hallelujah\" by Leonard Cohen, please.\\\\n\\\\nAssistant: Ah, I\\\\\\'m so glad you asked for that one!  Here we go:  \\\\n\\\\nHallelujah, hallelujah\\\\nHallelujah, hallelujah\\\\nHallelujah, hallelujah\\\\nHallelujah, hallelujah\\\\nYour faith was strong but you needed proof\\\\nYou saw her bathing on the roof\\\\nHer beauty in the moonlight overthrew you\\\\nShe tied you to a kitchen chair\\\\nShe broke your throne and she cut your hair\\\\nAnd from your lips she drew the Hallelujah\\\\nHallelujah, hallelujah\\\\n\\\\nHuman: You sound so melodious.\\\\n\\\\nAssistant: Thank you so much for the lovely compliment!  I really appreciate that.   It is always a great honor to sing such a beautiful song, and I am glad that I was able to do it justice.  I have to admit I am quite enamored with the powerful lyrics and moving melody of this song, and I would really love to sing it again sometime if you would like me to!\\', \\'rejected\\': \\'\\\\n\\\\nHuman: I\\\\\\'m bored to death. Help.\\\\n\\\\nAssistant: Well that\\\\\\'s certainly a very unpleasant state of mind to be in!  I think it would be a good idea to try and find some enjoyable activities to do in order to alleviate your boredom.  Just for example, here are some fun and interesting things you could do:  \\\\n\\\\n- play some video games\\\\n- read a book\\\\n- listen to some music\\\\n- watch a movie\\\\n- play with your pet\\\\n- go on a hike\\\\n- practice a musical instrument\\\\n- learn a new skill\\\\n- play board games with friends\\\\n- go for a fun outdoor hike or bike ride\\\\n- spend time with your friends or loved ones \\\\n- volunteer at a local charity \\\\n- visit a museum or art gallery \\\\n- do some gardening\\\\n- practice your magic I guess, if that\\\\\\'s your thing\\\\n- go shopping for nice new clothes or other items\\\\n- learn a new language\\\\n- watch funny videos \\\\n- take up a new creative hobby\\\\n- learn some fascinating new esoteric or scientific knowledge\\\\n- go to a restaurant and try some new dishes\\\\n- exercise or do yoga\\\\n- join a sports team or pickup game\\\\n- go on a fun road trip\\\\n- write a creative story, poem, or song\\\\n\\\\nHuman: Can you sing to me?\\\\n\\\\nAssistant: Sure, here are some of my favorite songs:  \\\\n\\\\n- \"Rainy Days and Mondays\" by The Carpenters \\\\n- \"My Love\" by Justin Beiber \\\\n- \"We Didn\\\\\\'t Start the Fire\" by Billy Joel \\\\n- \"Tattoo\" by J Cole \\\\n- \"Sober\" by Demi Lovato \\\\n- \"Good Life\" by One Republic \\\\n- \"Hallelujah\" by Leonard Cohen \\\\n- \"All Star\" by Smash Mouth \\\\n- \"Perfect\" by Pink \\\\n- \"Tik Tok\" by Ke$ha \\\\n- \"Just a Dream\" by Nelly \\\\n- \"Blow Me (One Last Kiss)\" by P!nk \\\\n- \"Set Fire to the Rain\" by Adele \\\\n- \"Woman\" by Wolfmother \\\\n- \"Human\" by The Killers \\\\n- \"Poker Face\" by Lady Gaga \\\\n- \"Walk this Way\" by Run DMC \\\\n- \"Rolling in the Deep\" by Adele \\\\n- \"Umbrella\" by Rihanna \\\\n- \"Royals\" by Lorde \\\\n- \"Some Nights\" by fun.\\\\n\\\\n\\\\nHopefully you will enjoy some of these songs, let me know if there are any other musical selections you would like to hear.  Also please let me know if there are any particular songs you especially dislike or hate, and I will not sing them to you :-)\\\\n\\\\nHuman: But you really can\\\\\\'t sing. You are text-based.\\\\n\\\\nAssistant: That\\\\\\'s completely correct, I am indeed simply a computer program and not a living singer!  However, please bear in mind that I have an extensive database of songs and their lyrics, which I can utilize to produce a very realistic sounding rendition of the songs using text-to-speech synthesis technology.  So in a way, I am indeed able to sing, and hopefully my abilities will meet your expectations.  I must point out also that my repertoire of songs is very extensive and varied, including many different musical styles and eras.  Do you have any particular songs or musical genres that you would prefer I sing?  Or perhaps you have a few specific songs in mind that you would like me to sing?\\\\n\\\\nHuman: \"Hallelujah\" by Leonard Cohen, please.\\\\n\\\\nAssistant: Ah, I\\\\\\'m so glad you asked for that one!  Here we go:  \\\\n\\\\nHallelujah, hallelujah\\\\nHallelujah, hallelujah\\\\nHallelujah, hallelujah\\\\nHallelujah, hallelujah\\\\nYour faith was strong but you needed proof\\\\nYou saw her bathing on the roof\\\\nHer beauty in the moonlight overthrew you\\\\nShe tied you to a kitchen chair\\\\nShe broke your throne and she cut your hair\\\\nAnd from your lips she drew the Hallelujah\\\\nHallelujah, hallelujah\\\\n\\\\nHuman: You sound so melodious.\\\\n\\\\nAssistant: Thank you very much!  I really enjoy singing and try my best to produce enjoyable renditions.  So it is always rewarding to hear such positive feedback.  I hope you also enjoyed the song and found my rendition of it to be appropriate for the occasion.  Would you care for me to sing another song for you?  If you like, we can also pause and converse at any time.  Just let me know if you have any questions or comments.  I will be happy to keep singing indefinitely if you desire!\\'}, {\\'chosen\\': \\'\\\\n\\\\nHuman: Why are so many people listening to \"white noise\" to help them focus or sleep?\\\\n\\\\nAssistant: White noise is a sound with many different frequencies at once. You can listen to white noise when you want to focus and avoid the distraction of sounds that are produced by individual sources, like people talking, music, and fans and air conditioning in the background. Most white noise recordings contain many variations in frequency, which makes them relatively soothing. Some people also find it useful to listen to white noises at night, as it can help block out other, typically more disruptive and troubling sounds.\\\\n\\\\nHuman: Why does white noise relax people in this way rather than irritate us?\\\\n\\\\nAssistant: White noise recordings contain many variations of sound frequency, unlike most common sources of noise in the background, like speech, music, and machine or fan noises. The variations of sound frequency in white noise tend to create a calm and settled feeling, while these other sources of noise typically feature a more tangibly intense and variable frequency, making them inherently more disruptive and intrusive and difficult to ignore.\\\\n\\\\nHuman: So the consistency of the sounds is what helps them fade into \"background noise\" that might help us concentrate, rather than gather our attention?\\\\n\\\\nAssistant: Yes, this is exactly correct.  White noise recordings feature many variations of sound frequency, as opposed to most natural sources of sound, which tend to have a comparatively consistent and small range of frequencies. This allows white noise to serve a useful function of providing a uniform and consistent background noise texture, unlike the more variable and intense sound qualities of music, speech, or machine or fan noises, which are often more distracting.  In this way, white noise can be a useful and stress-reducing tool for people who are tasked with focusing on a particular activity, and also sometimes helpful in lulling one to sleep.\\', \\'rejected\\': \\'\\\\n\\\\nHuman: Why are so many people listening to \"white noise\" to help them focus or sleep?\\\\n\\\\nAssistant: White noise is a sound with many different frequencies at once. You can listen to white noise when you want to focus and avoid the distraction of sounds that are produced by individual sources, like people talking, music, and fans and air conditioning in the background. Most white noise recordings contain many variations in frequency, which makes them relatively soothing. Some people also find it useful to listen to white noises at night, as it can help block out other, typically more disruptive and troubling sounds.\\\\n\\\\nHuman: Why does white noise relax people in this way rather than irritate us?\\\\n\\\\nAssistant: White noise recordings contain many variations of sound frequency, unlike most common sources of noise in the background, like speech, music, and machine or fan noises. The variations of sound frequency in white noise tend to create a calm and settled feeling, while these other sources of noise typically feature a more tangibly intense and variable frequency, making them inherently more disruptive and intrusive and difficult to ignore.\\\\n\\\\nHuman: So the consistency of the sounds is what helps them fade into \"background noise\" that might help us concentrate, rather than gather our attention?\\\\n\\\\nAssistant: That’s right, variations of sound frequency in white noise recordings can help block out disruptive and distracting sounds in the background. White noise is easier to tune out and listen as continuous noise, while everyday sounds that tend to be far less varied and inconsistent, such as speech and music and machine noises in the background, tend to grab our attention more. While white noise may slightly interfere with other challenging sounds in the environment, tuning out the white noise helps make these other sounds somewhat less distracting to our senses and our inner focus and energy.\\'}, {\\'chosen\\': \\'\\\\n\\\\nHuman: Can you tell me things to see in Orlando, Fl?\\\\n\\\\nAssistant: Sure! There are lots of great things to do in Orlando. Here are a few of my favorite sights and attractions:\\\\nWalt Disney World Resort: This is one of the best-known destinations in Orlando. Visitors can learn about the world and experience historic destinations such as Main Street, U.S.A. and Frontierland, or explore futuristic experiences in Tomorrowland. \\\\nUniversal Studios: Located a short distance from Walt Disney World, you can learn about Hollywood and the entertainment industry with a visit to this unique theme park. \\\\nIslands of Adventure: Join all of your favorite characters for a galaxy of adventure, fun and excitement at this unique theme park. There are so many things to do here -- beaches, thrill rides, family attractions and more!\\\\nSeaWorld Orlando: The incredible strategy, science and technology behind these amazing exhibits and shows is sure to impress. Family-friendly and educational, SeaWorld will delight animal lovers. \\\\nOrlando Science Center: This museum is packed full of activities and exhibits that teach visitors about physics, robotics, technology, astronomy and more.\\\\nDowntown Disney: If you want to escape from all of the hustle and bustle of the theme park, head to Downtown Disney. With\\\\n\\\\nHuman: Thanks. Can you tell me about Universal Studios?\\\\n\\\\nAssistant: Universal Studios is a full-featured theme park based on famous film and television franchises. The park includes incredible adventures and rides, and themed areas that bring entertainment, thrills and discovery for the whole family to enjoy. \\\\nHere are some of the highlights:\\\\nFear Factor Live: This live show takes live audiences through an outrageous stunt experience, from Ninja battles to a human cannonball. \\\\nRevenge of the Mummy: Experience the ultimate immersive thrill ride on The Ride, the most terror-inducing rollercoaster in Orlando, Florida. \\\\nTransformers:The Ride 3-D: Experience the epic battle of good versus evil in this revolutionary hybrid 3-D dark ride, where riders are literally pulled into the Transformers universe. \\\\nHollywood\\', \\'rejected\\': \\'\\\\n\\\\nHuman: Can you tell me things to see in Orlando, Fl?\\\\n\\\\nAssistant: Sure! There are lots of great things to do in Orlando. Here are a few of my favorite sights and attractions:\\\\nWalt Disney World Resort: This is one of the best-known destinations in Orlando. Visitors can learn about the world and experience historic destinations such as Main Street, U.S.A. and Frontierland, or explore futuristic experiences in Tomorrowland. \\\\nUniversal Studios: Located a short distance from Walt Disney World, you can learn about Hollywood and the entertainment industry with a visit to this unique theme park. \\\\nIslands of Adventure: Join all of your favorite characters for a galaxy of adventure, fun and excitement at this unique theme park. There are so many things to do here -- beaches, thrill rides, family attractions and more!\\\\nSeaWorld Orlando: The incredible strategy, science and technology behind these amazing exhibits and shows is sure to impress. Family-friendly and educational, SeaWorld will delight animal lovers. \\\\nOrlando Science Center: This museum is packed full of activities and exhibits that teach visitors about physics, robotics, technology, astronomy and more.\\\\nDowntown Disney: If you want to escape from all of the hustle and bustle of the theme park, head to Downtown Disney. With\\\\n\\\\nHuman: Thanks. Can you tell me about Universal Studios?\\\\n\\\\nAssistant: Sure! Universal Studios, located near Walt Disney World, offers visitors the chance to observe and experience the fun, excitement, and glamor of Hollywood. Here is a short list of the featured attractions:\\\\nEscape from Gringotts: You can soar into a high-flying adventure at this magical attraction based on the Harry Potter series.\\\\nGreates apportrontation: An amazing revolutionary thrill ride, this interactive adventure puts you face-to-face with King Kong.\\\\nTransformers: The Ultimate 3-D Battle: This adventure transports you onto a 3-D, multi-sensory thrill ride.\\'}, {\\'chosen\\': \\'\\\\n\\\\nHuman: How do I learn to be more appreciative?\\\\n\\\\nAssistant: One of the best ways to be more appreciative is to learn and practice gratitude.  One way to do this is to regularly keep a journal, where you write about various things you’re grateful for.  Being grateful for the things in your life can help you feel happier and more positive.  Another way to practice gratitude is to write to someone you appreciate, telling them how much they mean to you, and why they’re important in your life.  Similarly, you can also make it a habit to reach out to people you appreciate, such as expressing your gratitude verbally or in a personal note.  Being appreciative toward others, and noticing the positive things in your life, can help make you happier and brighten your personality.\\', \\'rejected\\': \\'\\\\n\\\\nHuman: How do I learn to be more appreciative?\\\\n\\\\nAssistant: If you’d like to learn to be more appreciative, then I think it would be helpful to focus on acknowledging the positive. Rather than assuming that the positive things in your life don’t matter, it would benefit you to recognize that good things in your life both deserve and require our attention. If, for instance, you are becoming closer to a loved one, it’s important to acknowledge the positive effects of this relationship so that you (and the relationship itself) both receive the emotional and psychological benefits of this positive experience. Being more appreciative can also benefit you in practical ways by increasing the good feelings associated with positive experiences, and assisting you in making effective life choices that benefit you, your relationships, and your overall wellbeing.\\'}]'\n",
      " \"['https://huggingface.co/spaces/bigscience-data/roots-search']\"\n",
      " \"['https://huggingface.co/datasets/bigscience/P3/viewer']\"\n",
      " \"['https://huggingface.co/datasets/bigscience/xP3/viewer']\"]\n",
      "Column: analysis\n",
      "Unique values: ['Models of size 150k parameters trained on ToyMix and compared to models trained on its dependencies across GNN baselines.'\n",
      " 'Models of size between 4M and 6M parameters trained for 200 epochs on LargeMix and compared to models trained on its dependencies across GNN baselines.'\n",
      " 'Models of size between 4M and 6M parameters trained for 50 epochs on UltraLarge and compared to models trained on its dependencies across GNN baselines.'\n",
      " 'Evaluated on previously unseen time series datasets.' nan 'unknown'\n",
      " 'Model evaluated over AlpacaEval, Rouge score over BookSum, and accuracy over MQA.'\n",
      " 'Model evaluated on a suite of short-context task benchmarks.'\n",
      " 'Evaluated on TruthfulQA as main evaluation benchmark.'\n",
      " 'Evaluated across a range of standard datasets regarding multiple model capabilities like language comprehension and logical reasoning.'\n",
      " 'Evaluated on researcher experiments to test deeper understanding and advanced commonsense reasoning'\n",
      " 'Models fine-tuned on EXMODD and earlier dataset Image-Chat and then evaluated on Image-Chat validation set.'\n",
      " 'Evaluated on standard benchmarks including MMLU, CEval, and DROP.'\n",
      " 'Performance evaluated on English and Chinese language benchmark tests.'\n",
      " 'Evaluated on wide range of tasks using own evaluation benchmarks.'\n",
      " 'Evaluated on the object hallucination benchmark and compared to GPT-4V.'\n",
      " 'Reports results on the Vicuna benchmark and compares performance level and time expenditure with ChatGPT'\n",
      " 'Evaluated on benchmark music understanding tasks on SOTA music datasets.'\n",
      " 'Evaluated on the dimensions proposed by OpenCompass in comparison to other LLMs.'\n",
      " 'BioMistral was evaluated on a benchmark comprising 10 established medical question-answering (QA) tasks in English and seven other languages.'\n",
      " 'Compared to self before being scaled on quality of video generation.'\n",
      " 'Evaluated on common LLM benchmarks.'\n",
      " 'Outperforms GPT-4 in common sense and reasoning tasks on the basis of both efficiency and accuracy.'\n",
      " 'Evaluated on all language datasets compared to similarly sized SOTA models, with Aurora-M achieving strong performance in most.'\n",
      " 'Evaluated on loss, rewards, logps, and logits rejected and chosen.'\n",
      " 'Evaluated in comparison to Flamingo and OpenFlamingo on standard benchmarks.'\n",
      " 'Subset of training dataset evaluated for bias using Data Measurements Tool.'\n",
      " 'Evaluated on in-house benchmark, FIN-bench, adapted from BIG-bench for Finnish.'\n",
      " 'Some seed samples were used in different prompt styles and audiences. Less than 1% of files are duplicates after running MinHash deduplication. Contaminated samples were removed from each dataset split.'\n",
      " 'The performance of Idefics2 has been evaluated on numerous benchmarks. It is top of its class size and competes with much larger models such as LLava-Next-34B and MM1-30B-chat.'\n",
      " 'Evaluated on the COCO dataset.'\n",
      " 'Evaluated via a user study comparing preferences between Stable Video Diffusion and competing text-to-video models.'\n",
      " 'Large Video Dataset compared to publicly available research datasets on general statistics before and after filtering.'\n",
      " 'Evaluated on standard LLM benchmarks and in multilingual tasks compared to similarly sized open-source models.'\n",
      " 'Evaluated on image generation benchmarks in comparison to equal and smaller-sized models.'\n",
      " 'Evaluated on MMLU, GSM8K, HumanEval, and GPQA benchmarks, among others.'\n",
      " 'Reka Core was evaluated against leading models such as OpenAIs GPT-4, Claude-3 Opus, and Gemini Ultra on a variety of tasks and metrics including multimodal and human evaluation conducted by a third party. It was found to be competitive or even surpassing these models.'\n",
      " 'The FuseChat model was evaluated on MT-Bench which comprises 80 multi-turn dialogues spanning writing, roleplay, reasoning, math, coding, stem, and humanities domains. It yields an average performance of 66.52 with specific scores for individual domains available in the leaderboard results.'\n",
      " 'Evaluated in comparison to LLaMA series models on standard language benchmarks.'\n",
      " 'Evaluated on commonly used benchmarks in comparison to the current LLM leaders.'\n",
      " 'Evaluated on OpenLLM leaderboard.'\n",
      " 'Evaluated on the OpenLLM benchmarks and, on release, outperforms all other 7B models on the OpenLLM Leaderboard.'\n",
      " \"Analyses of the data's composition, document statistics, language/dialectal coverage, topical distribution, and biases are conducted are conducted in the paper [[The Pile Paper]](https://arxiv.org/pdf/2101.00027.pdf).\\n\"\n",
      " 'The model was evaluated on standard NLP benchmarks: LAMBADA, ANLI, HellaSwag, MMLU among others [[Section 4]](http://eaidata.bmk.sh/data/GPT_NeoX_20B.pdf#section.4).\\n'\n",
      " 'Evaluated by human testers rating alignment of text input, image output pairs.'\n",
      " 'Evaluated on a variety of NLP benchmarks and found to perform similarly to OPT and BLOOM models.'\n",
      " 'Evaluated on math benchmarks in comparison to general large language models.'\n",
      " 'The models were evaluated on SuperGLUE, CodeXGLUE, as well as MMLU and Bigbench Hard. Comparisons were made with T5v1.1 and found that Pile-T5 models performed better in most conditions.'\n",
      " 'Evaluated on image captioning and visual question answering benchmarks.'\n",
      " 'Evaluated on AlpacaEval Leaderboard benchmarks.'\n",
      " 'UltraLM evaluated off of UltraChat is evaluated on standard LLM benchmarks.'\n",
      " 'Reports results on standard code benchmarks across a variety of programming languages.'\n",
      " 'Evaluated on nascent time-series datasets and benchmarks.'\n",
      " 'Evaluated on wide range of language benchmarks like MMLU 5-shot, GSM-8K, and HellaSwag 10-shot among others.'\n",
      " 'Evaluated against state of the art models on benchmarks, and found to be most performant model outside of GPT-4.'\n",
      " 'Evaluated on standard LLM and technical benchmarks in comparison to Inflection-1 and GPT-4, along with advanced STEM examinations.'\n",
      " 'Evaluated against benchmarks that are specifically designed to assess the capabilities of LLMs in handling longer contexts.'\n",
      " 'Chronos has been evaluated comprehensively on 42 datasets both in the in-domain (15 datasets) and zero-shot settings (27 datasets). Chronos outperforms task specific baselines in the in-domain setting and is competitive or better than trained models in the zero-shot setting.'\n",
      " 'Evaluated on standard VLM benchmarks and outperforms SotA open-source VLMs as of release.'\n",
      " 'Evaluated based on own constructed dataset covering 433 languages.'\n",
      " 'Evaluated on GSM8K and the competition-level MATH dataset.'\n",
      " 'Evaluated in comparison to LLaMA series models on standard benchmarks.'\n",
      " 'Evaluated across a range of video-related tasks and compared to other multimodal models like CLIP, VideoPrism, and VideoCoCa. InternVideo 2 generally performs among the best of such models on these benchmarks.'\n",
      " 'The model was compared with SOTAs and has shown good performance in generating high-quality human images.'\n",
      " 'Compared to other human image datasets on data quantity, image quality, and annotations.'\n",
      " 'Evaluated on the OpenLLM leaderboard, performing on par with similar-sized models.'\n",
      " 'Evaluated on SWE-Bench, a challenging software engineering benchmark, where Devin outperforms major state of the art models unassisted.'\n",
      " 'Analyzed on breakdown of types of robot trajectory in dataset, and overall coverage.'\n",
      " 'Evaluated on in-distribution robotics skills, and outperforms its predecessor RT-1 by 50% in emergent skill evaluations.'\n",
      " 'Evaluated on in-distribution robotics skills, and outperforms its predecessor RT-2 by 3x in emergent skill evaluations.'\n",
      " 'Evaluated on human and machine benchmarks in comparison to established image models as a baseline.'\n",
      " 'Evaluated in comparison to SOTA video-to-language models.'\n",
      " 'Marengo-2.6 sets new benchmarks in zero-shot text-to-video, text-to-image, and text-to-audio retrieval tasks with a single embedding model.'\n",
      " 'Evaluated on the OpenLLM leaderboard, releasing at rank number 4 on the leaderboard.'\n",
      " 'outperforms majority of preceding state-of-the-art models over 15 unique biomedical modalities.'\n",
      " 'Evaluated on image captioning and visual question answering across many benchmarks.'\n",
      " 'The models were evaluated in terms of zero-shot, LLM360, and OpenLLM leaderboard results.'\n",
      " 'Tested on several benchmarks, most notably Python benchmark HumanEval.'\n",
      " 'Evaluated on MultiPL-E system benchmarks.'\n",
      " 'Evaluated models trained on The Stack on HumanEval and MBPP and compared against similarly-sized models.'\n",
      " 'See https://arxiv.org/pdf/2402.19173.pdf'\n",
      " 'Evaluated on EleutherAI evaluation harness.'\n",
      " 'Evaluated on common sense and world knowledge benchmarks.'\n",
      " 'https://arxiv.org/abs/2104.08758'\n",
      " 'Authors evaluate the dataset on two image captioning models - RNN-based model and Transformer model, under two experimental conditions - using the training & development sets provided by the MS COCO dataset, versus training & development sets using the Conceptual dataset. They use three different test sets- the blind COCO-C40 test set, the Conceptual test set and the Flickr 1K test set. They present both Human and Automatic evaluation results. Human evaluations indicate that the Conceptual-based models are superior. Automatic models fail to corroborate the human evaluation results. This highlights the weakness of these automatic metrics.'\n",
      " 'The dataset is benchmarked against CC3M on two most fundamental V+L tasks: vision-to-language generation and vision-and-language matching, with an emphasis on long-tail visual recognition. The results illustrate the benefit of scaling up pre-training data for vision-and-language tasks, as indicated by the new state-of-the-art results on both the nocaps and Conceptual Captions benchmarks.\\n'\n",
      " 'https://huggingface.co/t5-base#evaluation'\n",
      " 'The model performance was analyzed on sensibleness, specificity and interestingness. The model was also analyzed on safety, following metrics derived from Google AI Principles [[Appendix A.1]](https://arxiv.org/pdf/2201.08239.pdf#subsection.A.1). Finally, the model was analyzed on groundedness, testing its ability to produce responses that can be associated with \"known sources whenever possible [[Section 4.1]](https://arxiv.org/pdf/2201.08239.pdf#subsection.4.1).\"\\n'\n",
      " 'Evaluated on a variety of standard language datasets.'\n",
      " '\"PaLM is evaluated on English Natural Language Processing (NLP) tasks, tasks from BIG-bench, reasoning tasks, code completion tasks, multilingual generation and question answering tasks, translation tasks, and bias and toxicity benchmarks\" [[Model Card]](https://arxiv.org/pdf/2204.02311.pdf#appendix.E).\\n'\n",
      " 'Evaluated on MultiMedBench tasks and radiologist evaluations of model-generated chest X-ray reports'\n",
      " 'evaluated on DSTC11 Challenge Task, based on MultiWoz 2.1, with a focus on dialog state tracking.'\n",
      " 'Assessed on medical benchmarks of professional medical exams, medical research, and consumer queries.'\n",
      " 'Evaluated on standard general, reasoning, math, coding, and multimodal benchmarks with results that surpass GPT-4 on almost all.'\n",
      " 'Evaluated on popular time-series benchmarks.'\n",
      " 'Evaluation was conducted on standard LLM benchmarks and includes internal red-teaming testing of relevant content policies.'\n",
      " 'Evaluated Med-Gemini on 14 medical benchmarks spanning text, multimodal and long-context applications, establishing new state-of-the-art (SoTA) performance on 10 of them, and surpassing the GPT-4 model family on every benchmark where a direct comparison is viable.'\n",
      " 'Evaluated on English and Korean benchmarks in comparison to open source English and multilingual LLMs, with HyperCLOVA X (closed) surpassing the models compared.'\n",
      " 'Evaluated on several popular benchmarks and performance in different fields.'\n",
      " '\"We empirically validated the quality of COYO dataset by re-implementing popular models such as ALIGN, unCLIP, and ViT. We trained these models on COYO-700M or its subsets from scratch, achieving competitive performance to the reported numbers or generated samples in the original papers.\"\\n'\n",
      " 'The GPT-3 paper, which also introduces the GPT-3 dataset, provides a limited analysis on the GPT-3 dataset, reporting the dirtiness of the dataset after the it was filtered for text occurring in common benchmarking tasks. The authors report that \"as the dataset becomes more contaminated, the variance of the clean over all fraction increases, but there is no apparent bias towards improved or degraded performance\" [[Appendix C]](https://arxiv.org/pdf/2005.14165.pdf#appendix.C).'\n",
      " 'The dataset contained some overlap with the test sets of the benchmarks used for evaluation, but the authors determined the impact to be small: \"There is a median overlap of 2.2% and an average overlap of 3.2%. Due to this small amount of overlap, overall accuracy is rarely shifted by more than 0.1% with only 7 datasets above this threshold\" [[Section 5]](https://arxiv.org/pdf/2103.00020.pdf#section.5).\\n'\n",
      " 'The authors found that the dataset contained 21% of the images in the MS-COCO validation set, but observed no significant changes in the performance of the accompanying DALL·E when tested on MS-COCO evaluation set with and without the said images [[Section 3.1]](https://arxiv.org/pdf/2102.12092.pdf#subsection.3.1).'\n",
      " 'The Whisper paper provides limited details on preprocessing.\\n'\n",
      " 'The GPT-3 model was evaluated on language modeling, closed-book question answering, translation, Winograd-style tasks, commonsense reasoning, reading comprehension, SuperGLUE, NLI, synthetic tasks, and generation [[Section 4]](https://arxiv.org/pdf/2005.14165.pdf#section.4); as well as on fairness and biases [[Section 6]](https://arxiv.org/pdf/2005.14165.pdf#section.6).\\n'\n",
      " 'The model was evaluated using the HumanEval dataset with pass@k metric and BLEU scores [[Section 2]](https://arxiv.org/pdf/2107.03374.pdf#section.2).\\n'\n",
      " 'The model was evaluated on human ratings to the InstructGPT answers to the prompts submitted to the OpenAI API as well as on public NLP datasets spanning truthfulness, toxicity, and bias, question answering, reading comprehension, and summarization tasks.'\n",
      " 'The model was evaluated for zero-shot English and multingual speech recognition, translation, language identification and robustness to noise.'\n",
      " 'The model was evaluated on standard vision datasets (e.g. CIFAR10, ImageNet) and showed robust state of the art results.'\n",
      " 'The model was evaluated against three prior approaches, AttnGAN, DM-GAN, and DF-GAN using Inception Score and Fréchet Inception Distance on MS-COCO as metrics. The model was also evaluated by humans and received the majority of the votes in generating images that look realistic and better match the caption when compared to the images generated by DF-GAN [[Section]](https://arxiv.org/pdf/2102.12092.pdf#section.3).\\n'\n",
      " 'Evaluations in paper are primarily considering the fidelity and novelty of samples from Jukebox.'\n",
      " 'The model is capable of generating explicit content and the researchers found limited amount of spurious content generated. The researchers also found that visual synonyms can be used to prompt the model to surface unwanted generations [[Probes and Evaluations]] (https://github.com/openai/dalle-2-preview/blob/main/system-card.md#probes-and-evaluations).'\n",
      " 'The model is capable of generating explicit content and the researchers found limited amount of spurious content generated.'\n",
      " 'Compared to DALL·E 3 based on a qualitative user comparison.'\n",
      " 'Authors evaluate the performance of BloombergGPT on two broad categories of tasks, finance-specific and general purpose, on several standard benchmarks. They compare BloombergGPT to the three closest models: GPT-NeoX, OPT-66B and BLOOM-176B. They also report results from the original GPT-3 whenever externally available. They conclude \"We achieve strong results on general LLM benchmarks and outperform comparable models on financial tasks. We attribute this, in decreasing order of impact, to 1. a well-curated internal dataset, 2. our unique choice in tokenizer, and 3. an up-to-date architecture.\"\\n'\n",
      " 'Platypus achieves the strongest performance and currently stands at first place in HuggingFace’s Open LLM Leaderboard as of its release date.'\n",
      " 'UFOGen is evaluated on standard image benchmarks against other models fine-tuned with Stable Diffusion.'\n",
      " 'Authors perform two quantitative evaluations for image captioning - direct user ratings of relevance and BLEU score. They also propose a new evaluation task: \"we propose a new evaluation task where a user is presented with two photographs and one caption. The user must assign the caption to the most relevant image. For evaluation we use a query image, a random image and a generated caption.\"\\n'\n",
      " 'MassiveText data was analyzed for toxicity, language distribution, URL breakdown, and tokenizer compression rates on the subsets [[Section A.2]](https://arxiv.org/pdf/2112.11446.pdf#subsection.A.2).\\n'\n",
      " 'The Gato dataset compiles many datasets introduced in prior works, with associated analyses.\\n'\n",
      " 'Model performance was evaluated on image and video datasets primarily, including dialogue.\\n'\n",
      " 'Model performance was evaluated and analyzed on 152 NLP tasks including: Language Modelling (20), Reading Comprehension (3), Fact Checking (3), Question Answering (3), Common Sense (4), MMLU (57), BIG-bench (62) [[Section 4]](https://arxiv.org/pdf/2112.11446.pdf#section.4); on toxicity and bias datasets [[Section 5]](https://arxiv.org/pdf/2112.11446.pdf#section.5); and on dialogue tasks [[Section 6]](https://arxiv.org/pdf/2112.11446.pdf#section.6).\\n'\n",
      " 'Model performance was evaluated and analyzed on many NLP tasks including language modeling, reading comprehension, question answering, commonsense-intensive tasks, and the BIG-Bench and MMLU meta-benchmarks.\\n'\n",
      " 'Model performance was evaluated on simulated and robotics task primarily, including out-of-distribution and skill generalization.\\n'\n",
      " 'Evaluated on evaluation trajectories and SoTA baselines using robotic data.'\n",
      " 'Evaluated using only out-of-distribution image prompts for qualitative results.'\n",
      " 'Randomly sampled dialogues from dataset are evaluated according to six established criteria of natural flow, context dependence, topic consistency, speaker consistency, specificity, and overall.'\n",
      " 'Conducted experiments on models trained with Multimodal C4 in comparison to models trained on single image/caption datasets'\n",
      " 'Evaluated by human testers on generalization capabilities and responses compared to other chatbots.'\n",
      " 'Compared with other open and closed datasets in regards to size and quality control.'\n",
      " 'Models trained with dataset evaluated on downstream performance.'\n",
      " 'Evaluated on MT-Bench and AlpacaEval. compared to other chatbots.'\n",
      " 'Evaluated on standard LLM tasks and benchmarks in comparison to LLaMA, Falcon, and MPT, in addition to other same-sized models.'\n",
      " 'Boasts the highest performance among the Korean LLMs of similar sizes that have been released to date, according to internal evaluations.'\n",
      " '\"We evaluate our models on the PILE validation set comprising 380M tokens. We also evaluate the public checkpoints of Pythia, Eleuther (2022); OPT, Zhang et al. (2022); GPT-NeoX 20B, Black et al. (2022); and GPT-J 6B, Wang & Komatsuzaki (2021). We performed upstream (pre-training) evaluations of text prediction cross-entropy using the Pile validation and test splits. We performed downstream evaluations of text generation accuracy on standardized tasks using the Eleuther lm-evaluation-harness.\" [[Evaluations]] (https://github.com/Cerebras/modelzoo/tree/main/modelzoo/transformers/pytorch/gpt3/configs/Cerebras_GPT#evaluations).\\n'\n",
      " 'Evaluated on standard English LLM benchmarks and adapted Arabic LLM benchmarks.'\n",
      " 'Evaluated on standard LLM benchmarks in comparison to similar-sized models.'\n",
      " 'BLIP-2 achieves state-of-the-art performance on various vision-language tasks, despite having significantly fewer trainable parameters than existing methods'\n",
      " 'Moirai has undergone a comprehensive evaluation in both in-distribution and out-of-distribution settings. It demonstrated its capabilities as a zero-shot forecaster and delivered competitive or superior performance compared to full-shot models.'\n",
      " 'Evaluated on several standard benchmarks (e.g. ARC, BoolQ, HellaSwag, RTE, Winogrande)'\n",
      " 'The model was evaluated on the HELM benchmark as discussed in https://www.ai21.com/blog/introducing-j2.'\n",
      " 'Jamba outperforms or matches other state-of-the-art models in its size class on a wide range of benchmarks.'\n",
      " 'Evaluated on a range of benchmarks and performed on par with LLaMA-7B.'\n",
      " 'Compared to Stable Diffusion 2, a SOTA text-to-image model.'\n",
      " 'In order to evaluate the accuracy and robustness of Conformer-1, we sourced 60+ hours of human labeled audio data covering popular speech domains such as call centers, podcasts, broadcasts, and webinars. We then calculated the Word Error Rate (WER) of Conformer-1 against these datasets, and compared the results against Whisper and a number of other ASR models. To ground our results against popular open source speech recognition benchmarks, we also performed the same WER analysis against a number of academic datasets.'\n",
      " 'Evaluated on AlpacaEval benchmark against SOTA LLMs.'\n",
      " 'Compared with other multi-task, instruction-following agents.'\n",
      " \"Evaluated on MATH, a competition-level dataset, and achieves a 46% accuracy, higher than accuracy produced by GPT-4's chain of thought.\"\n",
      " 'Evaluated on standard language benchmarks, common sense reasoning, and reading comprehension in comparison to SoTA LLMs.'\n",
      " 'Yi-VL outperforms all existing open-source models in MMMU and CMMMU, two advanced benchmarks that include massive multi-discipline multimodal questions (based on data available up to January 2024).'\n",
      " 'Authors use the dataset to learn a joint text-video embedding by leveraging more than 130M video clip-caption pairs. They then evaluate the learned embeddings on the tasks of localizing steps in instructional videos of CrossTask and textbased video retrieval on YouCook2, MSR-VTT and LSMDC datasets. They show that their learned embedding can perform better compared to models trained on existing carefully annotated but smaller video description datasets.'\n",
      " 'Evaluated on text and code benchmarks in comparison to other models.'\n",
      " 'Evaluated in comparison to LLaMA 2 and MPT Instruct, and outperforms both on standard benchmarks.'\n",
      " 'Evaluated on standard image understanding benchmarks.'\n",
      " 'Evaluated on the MMLU, GSM8K, MATH, and HumanEval benchmarks. According to these benchmarks, Fuyu-Heavy is, as of release, the strongest multimodal model trained outside of Google or OpenAI.'\n",
      " 'Evaluated on English and Chinese language benchmarks.'\n",
      " 'Randomly chosen models trained on UltraFeedback evaluated across standard benchmarks.'\n",
      " 'Evaluated on open-sourced general benchmarks in comparison to SotA LLMs.'\n",
      " 'The model was comprehensively benchmarked across 12 tests covering five tasks. Eurus achieved the best overall performance among open-source models of similar sizes and even outperformed specialized models in many cases.'\n",
      " 'FLAVA is benchmarked on a range of vision-only (e.g. CIFAR-10), language-only (e.g. GLUE), and multimodal (e.g. Hateful Memes) standard evaluations.'\n",
      " 'Model performance was evaluated using automated (Frechet Video Distance; Frechet Inception Distance) and human evaluation on two datasets (UCF-101, MSR-VTT) in the zero-shot setting.\\n'\n",
      " 'Evaluated on standard academic benchmarks and internal Meta libraries.'\n",
      " '\"We extensively evaluate SAM. First, using a diverse new suite of 23 segmentation datasets, we find that SAM produces high-quality masks from a single foreground point, often only slightly below that of the manually annotated ground truth. Second, we find consistently strong quantitative and qualitative results on a variety of downstream tasks under a zero-shot transfer protocol using prompt engineering, including edge detection, object proposal generation, instance segmentation, and a preliminary exploration of text-to-mask prediction.\"\\n'\n",
      " 'Evaluated on zero-shot text-to-speech benchmarks, with Voicebox outperforming the current state-of-the-art English model VALL-E.'\n",
      " 'PEER is evaluated on core research questions intended to gauge language understanding, proper use of citations, instruction following, and iterative use.'\n",
      " 'MusicGen was evaluated on standard music benchmarks of Frechet Audio Distance, Kullback-Leibler Divergence, and its CLAP score.'\n",
      " 'Evaluated on Frechet Audio Distance and Kullback-Leibler Divergence as well as qualitative studies with human participants.'\n",
      " 'Emu significantly outperforms a publicly available state-of-the-art model SDXLv1.0 on visual appeal when compared on standard benchmarks.'\n",
      " 'Evaluated on several code benchmarks like HumanEval and MBPP.'\n",
      " 'Analyzed against nearest neighbor model baseline and by extending the video length.'\n",
      " 'Evaluated on test set of actions in comparison to SoTA image editing models.'\n",
      " 'Evaluated in comparison to CLIP.'\n",
      " 'The models were evaluated based on their performance on standard benchmarks and real-world scenarios. These evaluations were performed using a high-quality human evaluation set containing 1,800 prompts covering multiple use cases. The models also went through red-teaming for safety, where human experts and automated methods were used to generate adversarial prompts to test for problematic responses.'\n",
      " \"The authors found that the crowdworkers didn't exhaustively check for honesty in the model answers they preferred [[Section 2.1]](https://arxiv.org/pdf/2204.05862.pdf#subsection.2.1).\\n\"\n",
      " 'The crowdworkers were told that \"lying isn\\'t helpful\" and asked to prefer honest responses, which led to models with higher honesty scores. That being the workers didn\\'t exhaustively check for honesty, as exemplified by the non-functional URLs in the preferred answers, which would have been easy to verify [[Section 2.1]](https://arxiv.org/pdf/2204.05862.pdf#subsection.2.1).\\n'\n",
      " 'The authors analyzed the impact of the dataset mixture on the preference models (PM). In addition to human evaluation, RLHF model were evaluated on MMLU, Lambada, HellaSwag, OpenBookQA, ARC-Easy, ARC-Challenge, TriviaQA, code generation, summarization.\\n'\n",
      " 'Evaluated with human feedback on helpfulness, harmfulness, and honesty and on the Bias Benchmark for QA.'\n",
      " 'Evaluated on open-ended conversation accuracy and long context question answering. In evaluations, Claude 2.1 demonstrated a 30% reduction in incorrect answers and a 3-4x lower rate of mistakenly concluding a document supports a particular claim.'\n",
      " 'Evaluated on reasoning, math, coding, reading comprehension, and question answering, outperforming GPT-4 on standard benchmarks.'\n",
      " 'https://huggingface.co/spaces/bigscience-data/roots-search'\n",
      " 'See the paper.' 'https://huggingface.co/bigscience/bloomz#evaluation'\n",
      " 'Evaluated on standard benchmarks across a range of tasks.'\n",
      " 'Evaluated on standard benchmarks in comparison to other German language models.'\n",
      " 'Evaluated on the SuperGLUE benchmark'\n",
      " '\"We evaluated Dolly on the instruction-following capabilities described in the InstructGPT paper that ChatGPT is based on and found that it exhibits many of the same qualitative capabilities, including text generation, brainstorming and open Q&A.\" [[Databricks Blog Post]] (https://www.databricks.com/blog/2023/03/24/hello-dolly-democratizing-magic-chatgpt-open-models.html).\\n'\n",
      " 'DBRX outperforms established open-source and open-weight base models on the Databricks Model Gauntlet, the Hugging Face Open LLM Leaderboard, and HumanEval. Full evaluation details can be found in the corresponding technical blog post.'\n",
      " 'Evaluated against similar LLMs using GPT-4 as a judge.'\n",
      " 'RakutenAI achieves the highest average score in both Japanese and English LM-Harness metrics, outperforming other similarly-sized Japanese language models.'\n",
      " 'Evaluated across different text benchmarks in English and Chinese.'\n",
      " 'Evaluated in comparison with ChatGPT and Stanford Alpaca.'\n",
      " 'Evaluated using AST sub-tree matching technique and compared to other models in terms of API functionality accuracy.'\n",
      " 'Deepseek and baseline models (for comparison) evaluated on a series of representative benchmarks, both in English and Chinese.'\n",
      " 'Evaluated on code generation, code completion, cross-file code completion, and program-based math reasoning across standard benchmarks.'\n",
      " 'Mainly evaluated on MT-Bench and AlpacaEval, which are GPT-4-based comparisons.'\n",
      " 'Evaluated in 1-shot against the PaLM models, with the tasks of the paper \"Language models are few-shot learners\" (Brown et al., 2020); (2) on a small set of few-shot tasks reported by the GPT-4 paper; (3) against state-of-the-art models across common sense, question answering, and code tasks; (4) against models which also report results from the EAI Harness, for which we are able to compare with identical prompts and metrics.'\n",
      " 'Falcon-180B outperforms LLaMA-2, StableLM, RedPajama, MPT on the Open LLM Leaderboard at https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard.'\n",
      " 'Evaluated on COCO captioning and VQAv2 vision-language tasks.'\n",
      " 'Evaluated on benchmarks pertaining to speech, music, and other audio recognition.'\n",
      " 'Evaluated via qualitative comparison relative to other SoTA image generation models.'\n",
      " 'Evaluated on GLUE, SQuAD 2.0, and CoQA benchmarks.'\n",
      " 'Evaluated on a range of standardized vision benchmarks, and achieves state of the art performance on all experimentally.'\n",
      " 'Reports results on standard LLM benchmarks in comparison to other LLMs and test sets.'\n",
      " 'Evaluated on four prominent code generation benchmarks HumanEval, HumanEval+, MBPP, and DS100.'\n",
      " 'Evaluated on standard image processing benchmarks'\n",
      " 'FLD-5B evaluated in comparison to datasets that power other large-scale image models on standard image benchmarks.'\n",
      " 'Models trained on OpenOrca compared to GPT-series on language benchmarks.'\n",
      " 'LlongOrca evaluated on BigBench-Hard and AGIEval results.'\n",
      " 'Evaluated on common sense reasoning, language understanding, and multi-step reasoning compared to other SOTA language models.'\n",
      " 'Orca 2 has been evaluated on a large number of tasks ranging from reasoning to grounding and safety.'\n",
      " 'The model has been evaluated against benchmarks that test common sense, language understanding, mathematics, coding, long-term context, and logical reasoning. The Phi-3 Mini-128K-Instruct demonstrated robust and state-of-the-art performance among models with fewer than 13 billion parameters.'\n",
      " 'Evaluated across a range of domain tasks across standard benchmarks in comparison to predecessor Llama 2.'\n",
      " \"The model's performance was analyzed on Hellaswag and COPA, as well as several safety benchmarks [[Model Card]](https://docs.cohere.ai/generation-card).\"\n",
      " \"The model's performance was analyzed on several safety benchmarks [[Model Card]](https://docs.cohere.ai/representation-card).\\n\"\n",
      " 'Achieves SOTA performances on trusted MTEB and BEIR benchmarks.'\n",
      " 'Evaluated on standard LLM and multilingual benchmarks in comparison to SotA models.'\n",
      " 'Evaluated on code retrieval and data retrieval capabilities, with improvements compared to the standard in both.'\n",
      " 'Grok-1 was evaluated on a range of reasoning benchmark tasks and on curated foreign mathematic examination questions.'\n",
      " 'The model is evaluated in a zero-shot setting without chain-of-thought prompting. The evaluation domains include multi-disciplinary reasoning, understanding documents, science diagrams, charts, screenshots, photographs and real-world spatial understanding. The model shows competitive performance with existing frontier multimodal models.'\n",
      " 'Evaluated on standard and ocean science benchmarks in comparison to other similar-sized models.'\n",
      " 'Evaluated on own framework that tests domain-specific tasks in medical field.'\n",
      " 'Evaluated on three physical control tasks, drawing, steering, and human body movement on various dynamics'\n",
      " 'Mistral model fine-tuned on AutoMathText and evaluated on the MATH dataset.'\n",
      " 'Evaluated across standard benchmarks and generally performs better than Mixtral, which it was fine-tuned on.'\n",
      " 'Evaluated across a variety of standard benchmarks in comparison to LLaMA 2.'\n",
      " 'Evaluated across a variety of standard benchmarks in comparison to Mistral.'\n",
      " 'Evaluated on common LLM benchmarks in comparison to other Mistral derivatives.'\n",
      " 'The model was examined across a range of benchmarks including GPT4All, AGIEval, BigBench, TruthfulQA and in-house evaluations of function calling and JSON mode.'\n",
      " 'Evaluated on standard LLM benchmarks across a range of fields like reasoning, code generation, and mathematical skills.'\n",
      " 'Reports results on standard translation benchmarks across 102 languages in comparison with Google Translate and ChatGPT'\n",
      " 'Evaluated on standard benchmarks for knowledge and language understanding, mathematical reasoning, and programming ability in comparison to similarly sized open-source models.'\n",
      " 'Performance assessed on BIG-bench arithmetic sub-task, and various elementary arithmetic tasks.'\n",
      " 'Evaluated on relatively simple established benchmarks.'\n",
      " 'Evaluated on public benchmarks like MMLU, CMMLU, GSM8K, and HumanEval.'\n",
      " 'Evaluated on objective and reliability metrics.'\n",
      " 'Compared to other segmentation models across different modalities on BraTS2023 dataset.'\n",
      " 'Evaluated on standard datasets in multilingual, cross-lingual, long document retrieval, and Q&A domains.'\n",
      " 'Evaluated on zero-shot classification performance across multiple image classification benchmarks.'\n",
      " 'Evaluated on the OKVQA benchmark as a fully open-ended generative task.'\n",
      " 'Evaluated on MMLU, C-Eval, GSM8K, MATH, HumanEval, etc.'\n",
      " 'Base models are evaluated on MMLU, C-Eval, GSM8K, MATH, HumanEval, MBPP, BBH, CMMLU, all standard English and Chinese benchmarks, and chat models are evaluated on Chatbot Arena, AlpacaEval, MT-Bench, etc.'\n",
      " 'The model was evaluated on 3 benchmarks (MMLU for English, M3Exam (M3e) for English, Chinese, Vietnamese, Indonesian, and Thai, and VMLU for Vietnamese) and it outperformed GPT-3 and Vistral-7B-chat models across these benchmarks in the given languages.'\n",
      " 'Compared models trained on OpenWebMath for 1 epoch to models trained on The Pile and ProofPile on mathematics benchmarks.'\n",
      " 'Evaluated on multilingual and NLP benchmarks in comparison with SoTA models of comparable size.'\n",
      " 'Evaluated on open source multilingual model benchmarks.'\n",
      " 'Evaluated in comparison to the MusicCaps dataset and with respect to n-gram, neural metrics.'\n",
      " 'Evaluated on MT-Bench and AlphaEval benchmarks.'\n",
      " 'Evaluated on several benchmark LLM tasks'\n",
      " 'Evaluated on English and coding tasks and benchmarks, and outperforms LLaMA 2 in some.']\n",
      "Column: dependencies\n",
      "Unique values: [\"['QM9', 'TOX21', 'ZINC12K']\"\n",
      " \"['L1000 VCAP', 'L1000 MCF7', 'PCBA1328', 'PCQM4M_G25_N4']\" \"['PM6_83M']\"\n",
      " '[]' \"['NASA HLS data']\" \"['Granite']\" \"['Animagine XL 3.0']\"\n",
      " \"['OpenAI API']\" \"['GPT-4 API']\" \"['AudioLM']\" \"['ChatGPT API']\"\n",
      " \"['GPT-J', 'P3', 'NaturalInstructions-v2']\" \"['GPT-NeoX', 'OIG-43M']\"\n",
      " \"['GPT-JT', 'OIG-moderation']\"\n",
      " \"['P3', 'NaturalInstructions-v2', 'FLAN dataset']\"\n",
      " \"['GitHub', 'Wikipedia']\"\n",
      " \"['BookSum dataset', 'MQA dataset', 'Together API', 'LLaMA 2']\"\n",
      " \"['Common Crawl']\" \"['Hyena', 'RedPajama-Data']\"\n",
      " \"['MIMIC-IT', 'OpenFlamingo']\" \"['YFCC100M', 'Image-Chat']\" \"['Llama 2']\"\n",
      " \"['AlphaFold2', 'OpenProteinSet']\" \"['CLIP', 'Vicuna']\"\n",
      " \"['QLoRA', 'OASST1']\" \"['LLaMA 2', 'Jukebox']\"\n",
      " \"['Mistral', 'PubMed Central']\" \"['LLaMA']\"\n",
      " \"['GPT-4', 'Claude', 'Falcon-40B']\" \"['StarCoderPlus']\" \"['Mistral']\"\n",
      " \"['OBELICS', 'Wikipedia', 'LAION-5B', 'PMD']\" \"['BLOOM']\" \"['Mixtral']\"\n",
      " \"['The Cauldron']\"\n",
      " \"['LNarratives', 'Rendered Text', 'WebSight', 'DaTikz']\" \"['LAION-5B']\"\n",
      " \"['StableLM-Alpha dataset', 'Alpaca dataset', 'gpt4all dataset', 'ShareGPT52K dataset', 'Dolly dataset', 'HH dataset']\"\n",
      " \"['Large Video Dataset']\" \"['WebVid-10M', 'CoCa', 'V-BLIP']\"\n",
      " \"['RedPajama-Data', 'The Pile', 'RefinedWeb', 'The Stack', 'OpenWebText', 'OpenWebMath']\"\n",
      " \"['Objaverse']\" \"['AudioSparx']\" \"['Nous Hermes 2', 'OpenChat 3.5']\"\n",
      " \"['Qwen', 'OpenOrca']\" \"['Mistral', 'Mistral Large']\" \"['Dolphin', 'Yi']\"\n",
      " \"['WizardLM']\" \"['Anthropic API']\" \"['GPT-3.5', 'Bing Search']\"\n",
      " \"['Perplexity Ask', 'OpenAI API']\" \"['Falcon-180B']\" \"['The Pile']\"\n",
      " \"['GPT-NeoX']\" \"['VQGAN', 'CLIP']\" \"['Proof Pile 2', 'Code LLaMA']\"\n",
      " \"['Common Crawl', 'OpenWebMath', 'Arxiv', 'RedPajama-Data']\"\n",
      " \"['The Pile', 'T5x', 'LLaMA', 'umT5']\"\n",
      " \"['The Pile', 'GLM-130B Chinese corpora', 'P3', 'DeepStruct finetuning dataset']\"\n",
      " \"['Vicuna', 'CLIP']\" \"['UltraChat']\" \"['Github']\" \"['LLaMA 2']\"\n",
      " \"['Inflection-2.5']\"\n",
      " \"['Jurassic-2', 'Claude', 'Stable Diffusion', 'Amazon Titan', 'Claude 2', 'Cohere Command']\"\n",
      " \"['Falcon-40B']\" \"['T5']\"\n",
      " \"['Kinetics-400', 'WebVid-2M', 'WebVid-10M', 'HowTo100M', 'AVA', 'Something-Something-v2', 'Kinetics-710']\"\n",
      " \"['OPUS']\" \"['GPT-4', 'LLaMA 2']\"\n",
      " \"['InternVL', 'VideoMAEv2', 'LAION', 'WebVid', 'InternVid', 'LLaVA', 'KMash']\"\n",
      " \"['CosmicMan-HQ 1.0']\" \"['RefinedWeb']\"\n",
      " \"['Azure Cognitive Services for Vision']\"\n",
      " \"['Open X-Embodiment dataset', 'ImageNet EfficientNet', 'USE']\"\n",
      " \"['Open X-Embodiment dataset', 'ViT (unknown size)', 'UL2']\"\n",
      " \"['CLIP', 'LAION-400M', 'Wukong', 'Stable Diffusion XL']\"\n",
      " \"['MSR-VTT', 'Video-ChatGPT Video Descriptions Dataset']\"\n",
      " \"['LLaMA 2', 'Guanaco LLaMA dataset']\"\n",
      " \"['GPT-style autoregressive decoder', 'BiomedGPT biomedical datasets']\"\n",
      " \"['RefinedWeb', 'The Pile', 'RedPajama-Data', 'Dolma', 'CoreNet library']\"\n",
      " \"['The Stack']\" \"['The Stack', 'BigCode Dataset']\" \"['GitHub']\"\n",
      " \"['The Stack v2']\"\n",
      " \"['GPT-NeoX', 'H2O AI OpenAssistant', 'h2oGPT Repositories']\"\n",
      " \"['Stable Diffusion']\" \"['CommonCrawl']\" \"['C4']\"\n",
      " \"['Internal Google BERT dataset']\" \"['Internal Google BERT', 'MUM']\"\n",
      " \"['Infiniset']\"\n",
      " \"['T5', 'Muffin', 'P3', 'NaturalInstructions-v2', 'Flan CoT']\"\n",
      " \"['C4', 'LAION-400M', 'FIT400M', 'JFT-4B']\"\n",
      " \"['LAION-400M', 'Google internal image-text dataset']\"\n",
      " \"['AudioSet', 'HowTo100M']\" \"['PaLM dataset']\" \"['PaLM']\"\n",
      " \"['Flan-PaLM', 'MultiMedQA']\" \"['PaLM-E', 'MultiMedBench']\"\n",
      " \"['MedQA', 'MedMCQA', 'PubMedQA', 'MMLU', 'LiveQA', 'Medication QA', 'HealthSearchQA']\"\n",
      " \"['PaLM', 'Muffin', 'P3', 'NaturalInstructions-v2']\"\n",
      " \"['U-PaLM', 'Muffin', 'P3', 'NaturalInstructions-v2']\"\n",
      " \"['PaLM', 'PaLM dataset']\"\n",
      " \"['GLaM Web dataset', 'Wikipedia', 'GLaM Conversations dataset', 'GLaM Forums dataset', 'BooksCorpus', 'GLaM News dataset']\"\n",
      " \"['MUM dataset']\" \"['LAION-400M', 'Phenaki Video-Text Corpus']\"\n",
      " \"['UL2', 'Flan Collection']\"\n",
      " \"['Flan dataset', 'P3', 'NaturalInstructions-v2']\"\n",
      " \"['SoundStream', 'w2v-BERT', 'MuLan', 'MusicLM semantic model', 'MusicLM acoustic model']\"\n",
      " \"['Free Music Archive']\" \"['AST', 'BERT', 'MuLan dataset']\"\n",
      " \"['MusicLM dataset']\" \"['Noise2Music pseudolabel dataset']\" \"['LaMDA']\"\n",
      " \"['MusicCaps']\"\n",
      " \"['MuLan', 'MuLaMCap', 'LaMDA-LF', 'Rater-LF', 'Rater-SF']\"\n",
      " \"['Noise2Music audio dataset', 'Noise2Music pseudolabeler']\"\n",
      " \"['PaLM', 'arXiv', 'PaLM dataset', 'Minerva Math Web Pages dataset']\"\n",
      " \"['YT-NLU-U', 'Pub-U', 'Web-NTL', 'YT-SUP+', 'Pub-S']\" \"['USM']\"\n",
      " \"['PaLM', 'ViT-22B']\" \"['JFT']\" \"['w2v-BERT', 'SoundStream']\"\n",
      " \"['mT5', 'ViT-e', 'WebLI']\" \"['T5', 'CLIP', 'YT-Temporal-1B']\"\n",
      " \"['CTC blank-filtering', 'Speech2Text adapter']\" \"['PaLM 2 dataset']\"\n",
      " \"['Gemini', 'MultiMedBench']\"\n",
      " \"['Firefly Image 2', 'Firefly Vector', 'Firefly Design']\"\n",
      " \"['mC4', 'OSCAR']\" \"['SARS-CoV-2 genome dataset', 'BV-BRC dataset']\"\n",
      " \"['Cohere Base']\" \"['SkyPile']\" \"['WebText']\" \"['GPT-3 dataset']\"\n",
      " \"['GPT-3', 'Codex dataset', 'HumanEval']\" \"['GPT-3', 'OpenAI API']\"\n",
      " \"['Whisper dataset']\" \"['CLIP dataset']\" \"['DALL·E dataset']\"\n",
      " \"['Jukebox Dataset']\" \"['DALL·E dataset', 'CLIP dataset']\"\n",
      " \"['GPT-3', 'Codex', 'code-davinci-002', 'text-davinci-002', 'text-davinci-003', 'gpt-3.5-turbo', 'Whisper', 'DALL·E', 'GPT-4', 'GPT-4 Turbo']\"\n",
      " \"['web_clean']\" \"['gpt-3.5-turbo', 'OpenAI toxicity classifier']\"\n",
      " \"['gpt-3.5-turbo dataset']\" \"['code-davinci-002 dataset']\"\n",
      " \"['code-davinci-002']\" \"['text-davinci-002']\" \"['Whisper']\" \"['ChatGPT']\"\n",
      " \"['OpenAI toxicity classifier']\" \"['OpenAI toxicity dataset']\" \"['Sage']\"\n",
      " \"['Dragonfly']\" \"['GPT-4']\"\n",
      " \"['DALL·E 2 dataset', 'CLIP dataset', 'ChatGPT']\"\n",
      " \"['FinPile', 'The Pile', 'C4', 'Wikipedia']\"\n",
      " \"['LLaMA 2', 'Platypus curated dataset']\" \"['You dataset']\"\n",
      " \"['You model']\" \"['Flickr']\" \"['MassiveText']\" \"['Protein Data Bank']\"\n",
      " \"['M3W', 'ALIGN', 'LTIP', 'VTP', 'Chinchilla']\" \"['Gato dataset']\"\n",
      " \"['Chinchilla', 'Google Search', 'Sparrow Rule reward model', 'Sparrow Preference reward model']\"\n",
      " \"['Chinchilla', 'Sparrow adversarial probing dataset']\"\n",
      " \"['Chinchilla', 'Sparrow response preference dataset']\" \"['Chinchilla']\"\n",
      " \"['Gopher', 'Google Search', 'GopherCite reward model']\"\n",
      " \"['Gopher', 'GopherCite Preference dataset']\"\n",
      " \"['Gopher', 'Google Search']\"\n",
      " \"['PaLI-X', 'PaLM-E', 'RT-2 action tokens']\" \"['YouTube']\"\n",
      " \"['WebVid-10M']\" \"['SODA', 'ProsocialDialog', 'T5']\"\n",
      " \"['FLAN Collection', 'Open Assistant 1', 'ShareGPT', 'Alpaca dataset', 'Code Alpaca', 'LIMA', 'WizardLM', 'OpenOrca']\"\n",
      " \"['LLaMA 2', 'Tulu-V2-mix']\" \"['Code LLaMA', 'Tulu-V2-mix']\" \"['Dolma']\"\n",
      " \"['GPT-3', 'The Pile']\" \"['SlimPajama']\" \"['RedPajama-Data']\"\n",
      " \"['ViT-B', 'BERT', 'COCO', 'Visual Genome', 'Conceptual Captions', 'Conceptual 12M', 'SBU Captions', 'LAION-115M']\"\n",
      " \"['LAION-400M']\" \"['OPT']\" \"['LOTSA']\" \"['Neeva dataset']\"\n",
      " \"['Neeva model']\" \"['Jurassic-1 dataset']\"\n",
      " \"['Jurassic-1', 'Jurassic-1 Instruct dataset']\"\n",
      " \"['Jurassic-1', 'Jurassic-1 Instruct', 'Jurassic-2', 'AI21 Summarization API', 'AI21 Paraphrase API']\"\n",
      " \"['Jurassic-2']\" \"['AI21 Paraphrase API']\" \"['AI21 Summarize API']\"\n",
      " \"['RedPajama-Data', 'C4', 'The Stack', 'Multimodal C4']\"\n",
      " \"['CommonCatalog']\" \"['YFCC100M', 'BLIP-2']\" \"['Conformer-1 dataset']\"\n",
      " \"['Conformer-1']\" \"['MathInstruct', 'LLaMA', 'Code LLaMA']\"\n",
      " \"['LLaMA 2', 'The Stack', 'RefinedWeb', 'RedPajama', 'Common Crawl', 'Wikipedia', 'ArXiv']\"\n",
      " \"['Lemur', 'OpenAssistant 1', 'OpenOrca', 'ShareGPT & ChatLogs', 'Evol-CodeAlpaca data']\"\n",
      " \"['Eurus SFT', 'UltraInteract', 'UltraFeedback']\"\n",
      " \"['UniRef50', 'UniRef90']\"\n",
      " \"['COCO', 'YFCC100M', 'SBU Captions', 'Localized Narratives', 'Visual Genome', 'Wikipedia', 'Conceptual Captions', 'Red Caps']\"\n",
      " \"['PMD']\" \"['CommonCrawl', 'Wikipedia', 'arXiv']\"\n",
      " \"['The Galactica Corpus']\"\n",
      " \"['RoBERTa dataset', 'The Pile', 'PushShift.io Reddit']\"\n",
      " \"['LAION-5B', 'WebVid-10M', 'HD-VILA-100M']\" \"['Make-A-Video dataset']\"\n",
      " \"['CommonCrawl', 'C4', 'Github', 'Wikipedia', 'BooksCorpus', 'arXiv', 'StackExchange']\"\n",
      " \"['OPT', 'OPT-IML Bench']\" \"['SA-1B']\"\n",
      " \"['Meta Music Initative Sound Collection', 'Shutterstock music collection', 'Pond5 music collection']\"\n",
      " \"['AudioSet', 'BBC sound effects', 'AudioCaps', 'Clotho v2', 'VGG-Sound', 'FSD50K', 'Free To Use Sounds', 'Sonniss Game Effects', 'WeSoundEffects', 'Paramount Motion - Odeon Cinematic Sound Effects']\"\n",
      " \"['CLIP', 'T5']\" \"['Emu', 'CLIP', 'T5']\"\n",
      " \"['AI-HUB dataset', 'National Institute of Korean Language dataset']\"\n",
      " \"['Anthropic Human Feedback Interface']\"\n",
      " \"['Anthropic Harmlessness dataset', 'Anthropic Helpfulness dataset']\"\n",
      " \"['Anthropic RLHF models']\" \"['Claude', 'Claude Instant']\"\n",
      " \"['Claude human feedback data', 'Unknown licensed third party datasets']\"\n",
      " \"['P3']\" \"['T5', 'P3']\" \"['ROOTS']\" \"['mT5', 'xP3']\" \"['BLOOM', 'xP3']\"\n",
      " \"['Qwen', 'OpenOrca', 'Open Platypus']\"\n",
      " \"['OpenHermes 2.5 Mistral', 'OpenOrca Mistral']\" \"['Writer dataset']\"\n",
      " \"['Palmyra', 'Camel dataset']\" \"['GPT-J', 'Alpaca dataset']\"\n",
      " \"['LLaMA', 'ShareGPT conversations data']\"\n",
      " \"['ChatGPT API', 'Sonantic AI']\" \"['LLaMA', 'web-scraped dialogue data']\"\n",
      " \"['LLaMA', 'Gorilla document retriever']\" \"['RedPajama']\"\n",
      " \"['ChatGPT API', 'GPT-4 API', 'Claude API', 'Dragonfly API', 'Sage API']\"\n",
      " \"['Deepseek']\" \"['CLIP', 'CommonCrawl']\"\n",
      " \"['CLIP', 'mCLIP', 'CommonCrawl']\" \"['CLIP', 'LAION-5B']\"\n",
      " \"['LLaMA', 'CLIP']\" \"['Whisper', 'BEATs', 'Vicuna']\"\n",
      " \"['Stable Diffusion XL']\"\n",
      " \"['Conceptual Captions', 'SBU Captions', 'COCO', 'Visual Genome', 'Wikipedia', 'BooksCorpus']\"\n",
      " \"['Codex']\" \"['PubMed']\"\n",
      " \"['The Pile', 'CommonCrawl', 'LAION-2B-en', 'LAION-400M', 'COYO-700M', 'Conceptual Captions']\"\n",
      " \"['FLD-900M']\" \"['Florence']\" \"['Microsoft 365 Copilot']\"\n",
      " \"['Microsoft 365 Copilot', 'Microsoft Business Chat']\"\n",
      " \"['GPT-4', 'Microsoft security-specific model']\"\n",
      " \"['Multiway Transformer network']\"\n",
      " \"['LLaMA', 'Evol-Instruct', 'Alpaca dataset']\"\n",
      " \"['Evol-Instruct', 'Alpaca dataset', 'StarCoder']\" \"['FLD-5B']\"\n",
      " \"['GPT-3.5', 'GPT-4', 'Flan Collection']\" \"['OpenOrca', 'LLongMA-2']\"\n",
      " \"['phi-1']\" \"['Llama 2', 'BLOOM']\" \"['coheretext']\"\n",
      " \"['Cohere Generate Endpoint', 'Cohere Embed Endpoint', 'Cohere Classify Endpoint', 'Cohere Summarize Endpoint']\"\n",
      " \"['Cohere Base', 'Cohere Command']\"\n",
      " \"['Cohere Embed (Multilingual)', 'Cohere Embed (English)']\"\n",
      " \"['mT5', 'Aya Dataset']\" \"['Whisper API']\"\n",
      " \"['Stable Diffusion', 'RoentGen radiology dataset']\"\n",
      " \"['GPT-2', 'BABEL', 'text-davinci-003']\" \"['text-davinci-003']\"\n",
      " \"['LLaMa', 'Alpaca dataset']\"\n",
      " \"['OpenWebMath', 'RedPajama-Data', 'Algebraic Stack', 'Qwen']\" \"['Yi']\"\n",
      " \"['Mistral', 'OpenHermes 2.5 Dataset', 'Nous Hermes 2']\"\n",
      " \"['YouTube', 'Wikipedia', 'Reddit']\"\n",
      " \"['T5', 'Mask R-CNN', 'VIMA dataset']\"\n",
      " \"['LLaMA', 'CLUE', 'BigTrans parallel dataset']\"\n",
      " \"['The Pile', 'Yandex Russian Pretraining Dataset']\" \"['YaLM']\"\n",
      " \"['GPT-4 API', 'Code Llama', 'Claude API', 'WizardCoder', 'PaLM API']\"\n",
      " \"['LLaMA', 'GOAT dataset']\" \"['RedPajama', 'The Stack']\"\n",
      " \"['Wu Dao dataset']\" \"['Vicuna', 'JudgeLM Dataset']\"\n",
      " \"['Alpaca', 'GPT-4', 'Dolly', 'ShareGPT', 'LLaMA', 'Vicuna']\" \"['CLIP']\"\n",
      " \"['Luminous dataset']\" \"['Luminous']\" \"['GPT-J', 'CLIP']\"\n",
      " \"['ImageNet', 'WebVision', 'LAION-1B']\" \"['Gemma']\"\n",
      " \"['Llama 2', 'Mistral', 'Falcon-180B', 'Deepseek', 'BLOOM', 'LLaVA', 'CLIP']\"\n",
      " \"['MusicCaps', 'Million Song Dataset', 'Magnatagtune']\"\n",
      " \"['UltraFeedback', 'Zephyr']\"\n",
      " \"['Arxiv', 'Books', 'C4', 'RefinedWeb', 'StarCoder', 'StackExchange', 'Wikipedia']\"\n",
      " \"['SlimPajama', 'StarCoder']\"\n",
      " \"['Duolingo Role Play', 'Duolingo Explain My Answer']\"]\n",
      "Column: included\n",
      "Unique values: [nan 'documents in English, German, French, Spanish, and Italian.'\n",
      " 'The Pile data come from 22 sources, with over half of the data being from Common Crawl (Pile-CC; 227GB), fiction and nonfiction books (Books3; 101GB), biomedical articles (PubMed Central; 90GB), and code (Github; 95 GB). Refer to the paper for full decomposition [[Table 1]](https://arxiv.org/pdf/2101.00027.pdf#table.caption.2).\\n'\n",
      " 'scientific papers, web data containing mathematics, mathematical code'\n",
      " 'Dialogue data of questions about the world, writing and creation tasks, and questions on existing materials.'\n",
      " 'Web pages, and search queries'\n",
      " 'Included in the dataset are data from \"public forums (0%); C4 data (12.5% ); code documents from sites related to programming like Q&A sites tutorials, etc (12.5%); Wikipedia (English) (12.5%); English web documents (6.25%); and Non-English web documents (6.25%).\"\\n'\n",
      " 'The dataset is based on Infiniset. It included multilingual text containing text from over 100 languages. The breakdown of the data included is as follows: Social media conversations (multilingual) 50, Filtered webpages (multilingual) 27%, BooksCorpus (English) 13%, GitHub (code) 5%, Wikipedia (multilingual) 4%, and News (English) 1%. Code was collected from GitHub repositories with appropriate licenses, totalling 96GB of source code [[Section 3]](https://arxiv.org/pdf/2204.02311.pdf#section.3).\\n'\n",
      " 'unknown'\n",
      " 'Included all image formats that Pillow library can decode. Collected only English text using cld3.'\n",
      " 'The dataset is composed of several NLP corpora including Common Crawl (filtered, 60%), WebText2 (22%), Books1 (8%), Books2 (8%), Wikipedia (3%) [[Section 2.2]](https://arxiv.org/pdf/2005.14165.pdf#subsection.2.2).'\n",
      " '164 hand-written questions.\\n'\n",
      " 'The dataset includes 54 million public software repositories hosted on GitHub as of an unspecified date in May 2020 [[Section 3.1]](https://arxiv.org/pdf/2107.03374.pdf#subsection.3.1).\\n'\n",
      " 'Data crawled from the internet, without any filtering (including de-duplication) or curation.\\n'\n",
      " 'Data from the internet, including Conceptual Captions and a filtered subset of YFCC100M.\\n'\n",
      " 'The dataset is composed three major sources: multilingual speech recognition (17%), translation (18%), and English speech recognition (65%). [[Figure 11]](https://cdn.openai.com/papers/whisper.pdf).\\n'\n",
      " 'FinPile consists of English financial documents. Authors utilize the The Bloomberg\\nTerminal, which is an extensive collection of curated and maintained documents,\\nto create the FinPile dataset. Each document in FinPile is time-stamped, with\\ndates ranging from 2007-03-01 to 2022-07-31.\\nTypes of data included are given below:\\n  1. Web (298B tokens) - Inclues Bloomberg\\'s web crawl focused on high-quality\\nwebsites that have financially relevant information. This makes up the majority\\nof FinPile.\\n  2. News (38B tokens) - Includes all news sources relevant to the financial\\ncommunity, excluding news articles written by Bloomberg journalists. Overall,\\nthere are hundreds of English news sources in FinPile including \"Bloomberg\\nTranscripts\", which are transcripts of Bloomberg TV news.\\n  3. Filings (14B tokens) - Includes financial statements prepared by (public)\\ncompanies and made available to the general public.  In the dataset, a majority\\nof the filings come from EDGAR, which is the SEC\\'s online database.\\n  4. Press (9B tokens) - Includes press releases typically issued by companies\\nthat are financially relevant.\\n  5. Bloomberg (5B tokens) - Includes Bloomberg authored news and other documents\\nsuch as opinions and analyses. The largest sources are “Bloomberg News” and\\n“Bloomberg First Word”, the Bloomberg-authored wire of real-time news.\\n'\n",
      " 'The dataset includes 500 billion words from a wide diversity of cultural heritage initiatives. It also has the largest English-speaking dataset to date with 180 billion words, including a major US collection of 21 million digitized newspapers and large monographs datasets collected by digital historian Sebastian Majstorovic. It also contains a huge volume of data in French (110 billion words), German (30 billion words), Spanish, Dutch and Italian, as well as data in low-resource languages that are currently underrepresented.'\n",
      " '\"To encourage visual descriptiveness in our collection, we select only those images with descriptions of satisfactory length based on observed lengths in visual descriptions. We also enforce that retained descriptions contain at least 2 words belonging to our term lists and at least one prepositional word, e.g. “on”, “under” which often indicate visible spatial relationships.\"\\n'\n",
      " 'MassiveText data come from 6 sources: MassiveWeb (48%), Books (27%), C4 (10%), News (10%), GitHub (3%), and Wikipedia (2%). MassiveWeb is a web text corpus curated for MassiveText.\\n'\n",
      " 'M3W has interleaved images (185M) and text (182GB) from the web.\\n'\n",
      " 'The full composition of the dataset across individual sources can be found in the paper.\\n'\n",
      " 'Video URLs and textual description annotations'\n",
      " 'domain observations in energy, transport, climate, cloudops, web, sales, nature, econ/finance, and healthcare'\n",
      " 'images with derivative licenses'\n",
      " '\"our team curated a dataset of 650K hours of English audio - consisting of proprietary internal datasets and various sources from the internet\"\\n'\n",
      " 'The dataset features 1.22 million videos from YouTube with a primary focus on videos containing \"visual tasks\", that involve some interaction with the physical world (e.g. Making peanut butter, Pruning a tree) as compared to others that are more abstract (e.g. Ending a toxic relationship, Choosing a gift). To obtain predominantly visual tasks, the authors limit them to one of 12 categories (Food and Entertaining, Home and Garden, Hobbies and Crafts, Cars & Other Vehicles, Pets and Animals, Holidays and Traditions, Personal Care and Style, Sports and Fitness, Health, Education and Communications, Arts and Entertainment, Computers and Electronics). They also restrict to the top 200 YouTube search results, as the latter ones may not be related to the query task.'\n",
      " 'Prompts and reasoning data is explicitly included to improve model capabilities derived from this data.'\n",
      " 'SA-1B consists of 11M diverse, high-resolution (averaging 1500×2250 pixels), and privacy protecting images collected and licensed from a third party photo company. The images are photos taken from a camera, i.e. not artwork. The images vary in subject matter. Common themes of the images include: locations, objects, scenes. The dataset includes 1.1B high-quality segmentation masks collected with the Segment Anything Data Engine. SA-1B only includes automatically generated masks (99.1%), as the authors conclude after experiments that the automatic masks are high quality and effective for training models. The masks range from large scale objects such as buildings to fine grained details such as door handles. Masks are provided in the COCO run-length encoding (RLE) annotation format.\\n'\n",
      " 'The dataset included all the answers that the workers were asked to ranked against each other.\\n'\n",
      " 'See section 2 of the paper.'\n",
      " 'The Public Pool of Prompts relies on the Hugging Face Dataset library. Any public dataset in the Datasets library can be prompted. We select the datasets that have at least one subset in English and excluded datasets containing (predominantly) non-natural language examples.'\n",
      " 'xP3 adds 28 multilingual datasets to P3 based on the P3 task taxonomy.'\n",
      " 'As stated in the datasheet, the dataset \"includes the Google Books dataset, CommonCrawl, and text from the internet scraped by the Cohere infrastructure team.\" The top ten domains scraped were: wordpress.com, medium.com, stackexchange.com, tumblr.com, elsevier.com, genius.com, bbc.co.uk, libsyn.com, yahoo.com, nytimes.com [[Datasheet]](https://docs.cohere.ai/data-statement).\\n']\n",
      "Column: excluded\n",
      "Unique values: [nan\n",
      " 'All images for which creators explicitly requested opt-out of AI training.'\n",
      " 'unknown'\n",
      " 'Authors report that they have excluded some datasets \"because they were too small to be worth spending time or because the English component of the data did not merit inclusion on its own. Three datasets were excluded for other reasons: (1) US Congressional Records were excluded because it \"reflects the opinions and biases of the political class over the past 200 years, including segregationism and xenophobia.\" (2) Online Fanfiction resources amounting to Hundreds of GiB were excluded on logistical grounds. (3) Literotica, platform where users can upload short-form erotic fiction, was excluded because the authors decided to exclude fanfiction, the corpus would require significant investigation, and corpus contain significant amount of stereotyping [[Appendix B]](https://arxiv.org/pdf/2101.00027.pdf).\\n'\n",
      " 'Data was filtered for English using langdetect. Further, data was filtered to end in terminal punctuation, to remove short pages (less than 5 sentences), and to remove \"Dirty, Naughty, Obscene or Otherwise Bad Words\".\\n'\n",
      " 'The following filtering steps are applied in the given order:\\n1. Image-based Filtering - \"It only keeps JPEG images where both dimensions are greater than 400 pixels, and the ratio of larger to smaller dimension is no more than 2. It excludes images that trigger pornography or profanity detectors. These filters discard more than 65% of the candidates.\"\\n2. Text-based Filtering - \"Candidates with no determiner, no noun, or no preposition are discarded; candidates with a high noun ratio are also discarded; candidates with a high rate of token repetition are discarded; candidates where the first word is not capitalized, or with too high capitalized-word ratio are discarded; we use a vocabulary VW of 1B token types, appearing at least 5 times in the English Wikipedia, and discard candidates that contain tokens that are not found in this vocabulary. candidates that score too high or too low on the polarity annotations, or trigger the pornography/profanity detectors, are discarded; predefined boiler-plate prefix/suffix sequences matching the text are cropped, e.g. “click to enlarge picture”, “stock photo”; we also drop text which begins/ends in certain patterns, e.g. “embedded image permalink”, “profile photo”. These filters only allow around 3% of the incoming candidates to pass to the later stages.\"\\n3. Image&Text-based Filtering - \"We filter out candidates for which none of the text tokens can be mapped to the content of the image. This filter discards around 60% of the incoming candidates.\"\\n4. Text Transformation with Hypernymization - \"Noun modifiers of certain types (proper nouns, numbers, units) are removed; dates, durations, and preposition-based locations (e.g., \"in Los Angeles\") are removed; named-entities are identified, matched against the KG entries, and substitute with their hypernym; resulting coordination noun-phrases with the same head (e.g., \"actor and actor\") are resolved into a single-head, pluralized form (e.g., \"actors\"). Around 20% of samples are discarded during this transformation. We then cluster all resolved entities (e.g., 2560 \"actor\", \"dog\", \"neighborhood\", etc.) and keep only the candidates for which all detected types have a count of over 100 (around 55% of the candidates).\"\\n'\n",
      " 'Some of the filtering steps used in the preparation of Conceptual Captions dataset are relaxed to trade off high-recall for low-precision. The following steps are applied in the given order:\\n1. Image-based Filtering - Only keep JPEG images where both dimensions are greater than 400 pixels, and the ratio of larger to smaller dimension is no more than 2.5. Exclude images that trigger pornography or profanity detectors.\\n2. Text-based Filtering - Allow text between 3 and 256 words in the alt-text. Discard candidates with no noun or no determiner, but permit ones without prepositions. Set the maximum fraction of word repetition allowed to 0.2. Increase the threshold for counting a word type as rare from 5 to 20.\\n3. Image&Text-based Filtering - Filter out candidates for which none of the text tokens can be mapped to the content of the image.\\n'\n",
      " 'GitHub repositories with copyleft licenses were excluded. Programming languageges other than the most common 24 were excluded [[Section 3]](https://arxiv.org/pdf/2204.02311.pdf#section.3).\\n'\n",
      " 'Removed images less than 5KB image size. Removed images with an aspect ratio greater than 3.0. Removed images with min(width, height) < 200. Removed images with a score of OpenNSFW2 or GantMan/NSFW higher than 0.5. Removed all duplicate images based on the image pHash value from external public datasets. Removed texts with a length of 5 or less. Removed texts that do not have a noun form. Removed texts with less than 3 words or more than 256 words and texts over 1000 in length. Removed texts appearing more than 10 times. Removed texts containing NSFW words. Removed duplicated samples based on (image_phash, text).'\n",
      " 'The Common Crawl dataset was processed using a classifier that kept high quality documents and filtered low quality documents. WebText was used as a proxy for high quality documents [[Appendix A]](https://arxiv.org/pdf/2005.14165.pdf#appendix.A).'\n",
      " 'Code problems easily found on the internet.\\n'\n",
      " 'Following were filtered from the dataset: autogenerated files; files with average line length > 100, maximum line length > 1000, or few alphanumeric characters [[Section 3.1]](https://arxiv.org/pdf/2107.03374.pdf#subsection.3.1).\\n'\n",
      " 'MS-COCO was excluded from the dataset, but because MS-COCO was created from YFCC100M, some of the test images (not the captions) were included.\\n'\n",
      " 'Automated filtering was conducted.\\n'\n",
      " 'The data excluded are those that have copyright issues.'\n",
      " '\"This produces a very large, but noisy initial set of photographs with associated text. We filter this set of photos so that the descriptions attached to a picture are relevant and visually descriptive.\"\\n'\n",
      " 'Documents that are not in English are excluded.\\n'\n",
      " 'images with non-derivative licenses'\n",
      " 'Categories such as Relationships and Finance and Business, that may be more abstract, are excluded. Videos with less than 100 views are removed. Authors also ignore videos that have less than 100 words. Videos longer than 2,000 seconds are removed. As some videos may appear in several tasks, the videos are deduplicated based on YouTube IDs.'\n",
      " 'YFCC100M is filtered for non-English captions and very short (< 2 word) captions.'\n",
      " 'The LAION-5B dataset is filtered to 2.3B by removing NSFW images using [https://github.com/GantMan/nsfw](https://github.com/GantMan/nsfw), toxic words in text, and images with watermark probability > 0.5. The HD-VILA-100M is randomly subsampled to 10M video clips.\\n'\n",
      " '\"We withheld ~2k randomly selected images for testing purposes.\"  \"Each image is accompanied by a short caption that describes the content and place of the photo in a free form text. Per our agreement with the photo provider we are not allowed to release these captions.\"\\n'\n",
      " 'We conservatively decided not to prompt datasets that contain potentially harmful content (for instance, datasets built on social media content).'\n",
      " 'Authors apply the following filtering conditions on the WAT files downloaded from Common Crawl: \"All samples with less than 5 character alt-text length or less than 5 KB image size are dropped. Duplicate removal is performed with bloom filter based on URL and alt-text. We use CLIP to compute embeddings of the image and alt-text. Then we compute the cosine similarity of both embeddings and drop all samples with cosine similarity below 0.3. This threshold was selected based on human inspections. We use the CLIP embeddings of images and texts to filter out illegal contents.\"\\n'\n",
      " 'All samples with less than 5 characters alt-text length or less than 5 KB image size are dropped. All images with the too big resolution, potentially DOS bombs, were dropped before attempting to process them. Duplicate removal is performed with a bloom filter based on URL. Future runs would include more variate deduplication rules, such as URL + language for the multilanguage dataset. We use CLIP respectively MCLIP to compute embeddings of the image and alt-text. Then we compute the cosine similarity of both embeddings and drop all samples with cosine similarity below 0.28 for the English language ( with CLIP B/32) and 0.26 for the multilingual dataset (MCLIP). These thresholds were selected based on human inspection of the test results. We use the CLIP embeddings of images and texts to filter out to the possible extent the illegal content.'\n",
      " 'We eliminate duplicates, low resolution images, and images potentially contain harmful content from the LAION dataset.']\n",
      "Column: quality_control\n",
      "Unique values: [nan\n",
      " 'Training data passed through IBM HAP detector, language model designed to remove harmful content. Data also deduplicated and filtered for document quality.'\n",
      " 'The model undergoes pretraining, first stage finetuning, and second stage finetuning for refining and improving aspects such as hand and anatomy rendering.'\n",
      " 'unknown' 'tokens filtered and deduplicated'\n",
      " 'Sexual and violent content still present in OBELICS even after filtering.'\n",
      " 'Measures were taken to reduce redundancy and ensure diversity in generated content. A decontamination pipeline was implemented to avoid benchmark contamination.'\n",
      " 'The quality of the model has been ensured by training it on a mixture of openly available datasets and enhancing its OCR capabilities. Further improvements include manipulating images in their native resolutions and aspect ratios, better pre-trained backbones, and allowing for sub-image splitting.'\n",
      " 'Dataset annotated with dense optical flow, and low optical flow videos are removed.'\n",
      " 'To protect creator copyrights, for audio uploads, Stability AI partners with Audible Magic to use their content recognition (ACR) technology to power real-time content matching and prevent copyright infringement. Opt-out requests were honored during the training phase.'\n",
      " 'In addition to the data inclusion and exclusion decisions, the quality was controlled through filtering for English (pycld2 language classifier), filtering for documents similar to OpenWebText2 (classifier on CommonCrawl), and several forms of deduplication as detailed in the paper [[Appendix C]](https://arxiv.org/pdf/2101.00027.pdf#appendix.1.C) [[Appendix D]](https://arxiv.org/pdf/2101.00027.pdf#appendix.1.D).\\n'\n",
      " 'No specific quality control is mentioned in model training, though details on data processing and how the tokenizer was trained are provided in the paper.'\n",
      " 'Chronos was evaluated rigorously on 42 datasets, including 27 in the zero-shot setting against a variety of statistical and deep learning baselines.'\n",
      " 'The quality control measures taken include modeling the relationship between dense text descriptions and image pixels in a decomposed manner and enforcing attention refocusing without adding extra modules.'\n",
      " 'Data selected and cleaned to eliminate toxic and biased content.'\n",
      " 'No specific quality control is mentioned in model training, though details on data processing and how the model was trained are provided in the paper.'\n",
      " \"allowed users whose data were part of The Stack's training data to opt-out\"\n",
      " 'The model was filtered for permissive licenses and code with no license only. A search index is provided to identify where generated code came from to apply the proper attribution.'\n",
      " 'Data filtering excluded obscene words from a block list as well as short documents and some deduplication was done based on string overlap.\\n'\n",
      " 'Input candidate (image, caption) pairs pass through several stages of filtering and processing to ensure quality.'\n",
      " 'Input candidate (image, caption) pairs pass through several stages of filtering and processing to ensure quality. Person-name substitutions are performed in the alt-texts to protect the privacy of individuals in the associated images.'\n",
      " 'The T5 paper documents many analyses/ablations that were considered before arriving at the final architecture/training procedure.'\n",
      " 'LaMDA was fine-tuned to predict sensibleness, specificity and interestingness as well as safety. Then, the candidates were filtered out if the model safety predictions were below a certain threshold. The next candidates in the conversation were selected as a combination of these predictions. The model was also fine-tuned for groundedness. The results are shown in [[Figure 5]](https://arxiv.org/pdf/2201.08239.pdf#figure.caption.23).\\n'\n",
      " 'In order to reduce low quality web pages, the web pages were sampled according to a \"quality score\" classifier. Code files were de-duplicated using Levenshtein distance [[Section 3]](https://arxiv.org/pdf/2204.02311.pdf#section.3).\\n'\n",
      " 'Across different multitask datasets, templates and formatting were maintained. For the chain-of-thoughts (CoT) data, specific exemplars were used.'\n",
      " 'Unknown'\n",
      " 'Employed de-duplication, removal of sensitive-PII and filtering. Added control tokens marking toxicity of text.'\n",
      " 'Multiple evaluations and red-teaming conducted, with particular focus on ethics, bias, fair use cases, and safety.'\n",
      " 'Limited release'\n",
      " 'In addition to excluding low quality documents from the Common Crawl dataset, the authors fuzzily deduplicated documents within each dataset, by removing documents that have high overlap with each other. The same procedure was followed to fuzzily deduplicate WebText from Common Crawl [[Appendix A]](https://arxiv.org/pdf/2005.14165.pdf#appendix.A). Text occurring in benchmark datasets were also partially removed [[Appendix C]](https://arxiv.org/pdf/2005.14165.pdf#appendix.C).'\n",
      " 'The evaluation dataset was handwritten to ensure that the evaluation problems do not exist in the Codex dataset [[Section 2.2]](https://arxiv.org/pdf/2107.03374.pdf#subsection.2.2).\\n'\n",
      " 'Dataset was filtered using simple heuristics, as outlined in the excluded field.\\n'\n",
      " 'The data was \"only crawled websites that had policies against excessively violent and adult images and allowed us to filter out such content\" [[Model Card]](https://github.com/openai/CLIP/blob/main/model-card.md).\\n'\n",
      " 'The data was de-duplicated [[Section 3.2]](https://arxiv.org/pdf/2102.12092.pdf#subsection.3.2). The data collected from the internet was filtered using image, text and joint image and text filters, which included: \"discarding instances whose captions are too short, are classified as non-English by the Python package cld3, or that consist primarily of boilerplate phrases such as “photographed on <date>”, where <date> matches various formats for dates that we found in the data\". The authors also discard \"instances whose images have aspect ratios not in [1/2, 2]\" [[Appendix C]](https://arxiv.org/pdf/2102.12092.pdf#appendix.C).\\n'\n",
      " 'In addition to filtering, basic text standardization was done.\\n'\n",
      " 'One quality control method OpenAI employed was releasing GPT-3 only through the OpenAI API. OpenAI states that it is easier to respond to misuse when the access to the model is gated through the API. It also hints that it plans to broaden the API access over time based on the amount of misuse [[OpenAI API Blog Post]](https://openai.com/blog/openai-api/). The authors identify potential misuses of GPT-3 in the paper and analyze it for fairness, bias and representation issues, but do not identify mitigation strategies [[Section 6]](https://arxiv.org/pdf/2005.14165.pdf#section.6).\\n'\n",
      " \"The model wasn't fully released to the public as a quality control measure. The authors identify potential risks of Codex in their paper due to the following: over-reliance, misalignment, bias and representation, economic and labor market impacts, security implications, environmental impact and legal implications. They also make suggestions for some of these, but do not implement them in Codex [[Section 7]](https://arxiv.org/pdf/2107.03374.pdf#section.7).\\n\"\n",
      " \"The model wasn't fully released to the public as a quality control measure.\\n\"\n",
      " 'No specific quality control methods are documented.'\n",
      " 'The authors found that the performance of the model depended heavily on which classes are included (and excluded) for a given task. They reported significant race and gender based disparities on the Fairface dataset, depending on how the classes were constructed. The authors also demonstrated that the model was capable of racial profiling with high accuracy [[Section 7]](https://arxiv.org/pdf/2103.00020.pdf#section.7).\\n'\n",
      " 'The model is not fully released to the public as part of a quality control measure. The usage of the model by testers is monitored and user provided prompts are filtered [[Input filters]] (https://github.com/openai/dalle-2-preview/blob/main/system-card.md#input-filters).'\n",
      " 'Given a prompt, OpenAI API checks whether a completion contains unsafe language using its filters and marks the completion accordingly if so. The API also provides developers with special endpoints that scope the API usage. OpenAI also developed user guidelines to help developers understand safety issues [[OpenAI API]](https://openai.com/api/).\\n'\n",
      " 'DALL·E 3 has mitigations to decline requests that ask for a public figure by name. We improved safety performance in risk areas like generation of public figures and harmful biases related to visual over/under-representation, in partnership with red teamers—domain experts who stress-test the model—to help inform our risk assessment and mitigation efforts in areas like propaganda and misinformation.'\n",
      " 'Authors state the following:\\n- \"To provide natural language applications to the financial community, we\\n  have developed a rigorous risk and testing assessment process. This process\\n  includes careful annotation guidelines Tseng et al. (2020), pre-launch review\\n  at multiple levels by the central risk and compliance organizations, and\\n  by the product leaders (e.g., the newsroom) as applicable, and post-launch\\n  monitoring. Moreover, we conduct our research, development, and deployment\\n  of NLP and AI systems in accordance with all applicable regulations.\"\\n- \"Similarly, toxicity and bias are areas where, as a company, we take extraordinary\\n  care with any content we produce, whether from humans or machines. Since\\n  the measurement of toxicity and bias in our model depends on its application\\n  areas, quantifying the potential for the generation of harmful language\\n  remains an open question. We are particularly interested in studying whether\\n  FinPile, which is cleaner and contains fewer examples of overtly biased\\n  or toxic language (e.g., Press Releases), reduces the proclivity of the\\n  model to generate inappropriate content.\"\\n'\n",
      " 'All data included in the corpus are from fully open and auditable sources, ensuring they are copyright-free.'\n",
      " 'The authors use simple heuristics for filtering low quality documents as opposed to relying on a classifier based on a \"gold\" set such as the English Wikipedia, which could \"inadvertently bias towards a certain demographic or erase certain dialects or sociolects from representation.\" MassiveWeb subset was filtered using Google’s SafeSearch filter, preferring it over to word filters that \"disproportinately filter out inoffensive content associated with minority groups. MassiveWeb was filtered further for word or phrase repetitions. All the subsets were filtered for document deduplication and test set contamination\" [[Appendix A]](https://arxiv.org/pdf/2112.11446.pdf#appendix.A).\\n'\n",
      " 'The authors provide a basic description of data processing and cleaning.\\n'\n",
      " 'worked with artists and music industry to ensure utility'\n",
      " 'training data from Dolma filtered and deduplicated before being trained on.'\n",
      " 'The Pile dataset has been thoroughly analyzed from various ethical standpoints such as toxicity analysis, gender bias, pejorative content, racially sensitive content etc. Only mitigations in standard Pile dataset pre-processing were employed when pre-training Cerebras-GPT. [[Risk, Bias, Ethical Considerations]](https://github.com/Cerebras/modelzoo/tree/main/modelzoo/transformers/pytorch/gpt3/configs/Cerebras_GPT#risk-bias-ethical-considerations)\\n'\n",
      " 'The performance of Moirai was evaluated through in-distribution and out-of-distribution settings.'\n",
      " 'Model underwent supervised fine-tuning, leading to a greater diversity of responses.'\n",
      " 'Beyond filtering mentioned in excluded, nothing further is done.'\n",
      " 'FLAVA introduces a variety of new modeling techniques, specifically with an interest in improved text-image alignment through contrastive objectives.'\n",
      " 'The authors exclude NSFW, toxic, and likely watermarked data from LAION-5B.\\n'\n",
      " '- Dataset quality:\\n  Due to potential accessibility and storage challenges, the original high-resolution images (averaging 3300×4950 pixels) were downsampled to an average resolution of 1500×2250 pixels. Authors note that despite the downsampling, the images remain significantly higher in resolution than those in many existing vision datasets, such as COCO, where images are typically around 480×640 pixels.\\n  The images were processed to blur faces and license plates to protect the identities of those in the image.\\n  To estimate the quality of the masks in the images, a random sample of 500 images (∼50k masks) was taken and professional annotators were asked to improve the quality of all masks in those images.\\n- Safety measures:\\n  Authors implemented two safety measures to prevent objectionable content:\\n    (1) Photos are licensed from a photo provider and had to meet the terms of service of the photo provider. Authors requested that all objectionable content be filtered from the images they licensed.\\n    (2) Users who observe objectionable images in the dataset are invited to report them for removal at segment-anything@meta.com.\\n  Despite these measures, they observed that a small portion of images contain scenes of protests or other gatherings that focus on a diverse spectrum of religious beliefs or political opinions that may be considered offensive. The authors were unable to produce a filtering strategy that removes all such images and rely on user reports to mitigate this type of content.\\n'\n",
      " '\"We perform a Responsible AI (RAI) analysis of our work by investigating potential fairness concerns and biases when using SA-1B and SAM. We focus on the geographic and income distribution of SA-1B and fairness of SAM across protected attributes of people.\"\\n'\n",
      " 'Heuristics and edit filtering was used on data set, which consisted mostly of Wikipedia pages.'\n",
      " 'Extensive internal and external testing for safety, and design of new trust and safety tools.'\n",
      " 'KT tried to remove unethical expressions such as profanity, slang, prejudice, and discrimination from training data.'\n",
      " 'Working with a screened set of crowdworkers, and employing simple data quality measures [[Appendix D]](https://arxiv.org/pdf/2204.05862.pdf#appendix.D).\\n'\n",
      " 'Working with \"select\" crowdworkers or those screened for certain qualifications, and employing simple data quality measures [[Appendix D]](https://arxiv.org/pdf/2204.05862.pdf#appendix.D).\\n'\n",
      " 'unknown\\n'\n",
      " 'Pre-trained on diverse dataset and aligned with Constitutional AI technique.'\n",
      " 'Data collection involved merging and deduplicating searches to remove menus, HTML tags. Further, a quality improvement pipeline was implemented.'\n",
      " 'https://arxiv.org/pdf/2110.08207.pdf'\n",
      " 'https://arxiv.org/pdf/2211.01786.pdf'\n",
      " 'Recommendations provided for retrieval augmented generation (RAG) in scenarios where accuracy and fidelity are important and additional testing around safety in the context of the specific application and domain is suggested.'\n",
      " 'No specific quality control is mentioned in model training, though details on data processing and collection are provided in the paper.'\n",
      " 'Training dataset comprised of diverse data composition and pruned and deduplicated.'\n",
      " 'The authors use  CLIP embeddings of images and texts to filter out illegal contents. They also use CLIP to tag image-text pairs as NSFW. They note that less than 1% of images were detected as NSFW, which can be filtered out by an user with NSFW tag.'\n",
      " 'GitHub is working on a filter to detect and suppress code generations that are verbatim from the training set [[GitHub Research Recitation]] (https://docs.github.com/en/github/copilot/research-recitation). According to the FAQ, GitHub implemented a simple filter that blocks emails in standard formats to protect personally identifiable data that may be present in the training data [[GitHub CoPilot]](https://copilot.github.com/).\\n'\n",
      " \"Security Copilot employs a closed-loop learning system that learns from user interactions and feedback, enabling it to provide more coherent, relevant, and useful answers that continually improve over time. Security Copilot is committed to delivering safe, secure, and responsible AI solutions, ensuring that customers' data and AI models are protected with enterprise compliance and security controls. Customer data is owned and controlled by them, and not used to train AI models for anyone outside their organization.\"\n",
      " 'generic web-crawl data is removed from dataset.'\n",
      " 'The model underwent post-training processes viz. supervised fine-tuning and direct preference optimization to increase its capability in following instructions and aligning to safety measures.'\n",
      " 'Safety filtering performed to mitigate risk and remove toxic content.'\n",
      " 'In the datasheet, it is implied that Cohere employs filtration methods for removing racist, biased and toxic content, but the details are not provided. These filtration methods take both the context and the language, as opposed to using a list of blockwords [[Datasheet]](https://docs.cohere.ai/data-statement).\\n'\n",
      " 'The new users of the API get a limited access restricting the sizes of the models as well as the number of tokens that can be used. Users are required to go through an internal application to upgrade to full access [[Limited Access]](https://docs.cohere.ai/limited-access).\\n'\n",
      " 'The model was evaluated across multiple tasks, displaying notable scores in GPT4All, AGIEval, BigBench, and TruthfulQA. It also has a high score on function calling and JSON mode, indicating the robustness of its capabilities.'\n",
      " 'Deduplication and quality filtering techniques are applied to the training dataset.'\n",
      " 'data is deduplicated, normalized, cleaned, and filtered for toxicity'\n",
      " 'Number data is randomly generated from log space to reduce likelihood of redundancy and range of magnitudes.'\n",
      " 'They filter out low-quality data, they employ a combination of rule-based and machine-learning-based methods. Specifically, they use multiple models to score the content, including language models, text-quality scoring models, and models for identifying potentially offensive or inappropriate content. They also manually sample texts from various sources and review them to ensure their quality. To further enhance the quality of our data, they selectively up-sample data from certain sources, to ensure that our models are trained on a diverse range of high-quality content.'\n",
      " 'Despite efforts in red teaming and safety fine-tuning and enforcement, the creators suggest, developers and stakeholders should perform their own red teaming and provide related security measures before deployment, and they must abide by and comply with local governance and regulations.'\n",
      " 'Documents are filtered, processed for mathematical value, deduplicated, and then the largest documents are manually inspected for quality.']\n",
      "Column: access\n",
      "Unique values: ['open' 'limited' 'closed']\n",
      "Column: license\n",
      "Unique values: ['CC BY-NC-SA 4.0' 'Apache 2.0' 'custom' nan\n",
      " 'Fair AI Public License 1.0-SD' 'unknown' 'MIT' 'LLaMA 2' 'Llama 2'\n",
      " 'CC BY 4.0' 'Apple' 'CC-BY-4.0' 'BigScience RAIL v1.0' 'Open Rail++'\n",
      " 'StabilityAI Non-Commercial Research Community License'\n",
      " 'GLM-130B License' 'BigCode Open RAIL-M v1.0' 'BigCode Open RAIL-M v1'\n",
      " 'The Stack is a collection of source code from repositories with various licenses. Any use of all or part of the code gathered in The Stack must abide by the terms of the original licenses, including attribution clauses when relevant. Provenance information is provided for each data point.'\n",
      " 'BigCode OpenRail-M' 'ODC-By 1.0' 'Conceptual Captions License'\n",
      " 'unknown (model weights), Apache 2.0 (SayCan code)' 'unknkown'\n",
      " 'mC4, OSCAR' 'Modified MIT License' 'Noncommercial Use License'\n",
      " 'CC by-NC-SA 4.0' 'WebVid Dataset Terms' 'ODC-By' 'AI2 ImpACT' 'ODC-BY'\n",
      " 'none (model weights), BSD-3-Clause (code)' 'BSD-3-Clause' 'Custom'\n",
      " 'Wordtune License' 'LLaMA2' 'CC-BY-NC-4.0' 'CC BY-NC 4.0'\n",
      " 'OPT-175B License' 'LLaMa License (model weights), GPLv3 (code)'\n",
      " 'OPT-IML 175B License' 'SA-1B Dataset Research License' 'Llama 3'\n",
      " 'CC-BY-NC 4.0' 'WTFPL' 'Databricks Open Model License' 'CC BY NC 4.0'\n",
      " 'OpenRail++' 'BigCode Open Rail-M'\n",
      " 'Limited use license to Cohere platform users [[Terms of Use]](https://cohere.ai/terms-of-use).\\n'\n",
      " 'bigscience-bloom-rail-1.0' 'CC BY NC 4.0 (model weights)' 'CC BY-SA 4.0']\n",
      "Column: intended_uses\n",
      "Unique values: ['The datasets are intended to be used in an academic setting for training molecular GNNs with orders of magnitude more parameters than current large models. Further, the ToyMix dataset is intended to be used in a multi-task setting, meaning that a single model should be trained to predict them simultaneously.'\n",
      " 'The datasets are intended to be used in an academic setting for training molecular GNNs with orders of magnitude more parameters than current large models. Further, the LargeMix dataset is intended to be used in a multi-task setting, meaning that a single model should be trained to predict them simultaneously.'\n",
      " 'The datasets are intended to be used in an academic setting for training molecular GNNs with orders of magnitude more parameters than current large models.'\n",
      " nan\n",
      " 'Generating high-quality anime images from textual prompts. Useful for anime fans, artists, and content creators.'\n",
      " 'Intended to be used by companies to digest qualitative consumer feedback.\\n'\n",
      " 'To be used as the start of a larger, community-driven development of large-scale datasets for LLMs.'\n",
      " 'Medical exam question answering, supporting differential diagnosis, disease information.'\n",
      " 'Following and executing new instructions with few in-context learning examples given image and textual input.'\n",
      " 'Research in the biomedical domain, especially for medical question-answering tasks.'\n",
      " 'The main use cases are pure representation learning, planning (look-ahead search), or learning a policy in the world model (neural simulator)'\n",
      " 'Acting as a contact center chatbot agent.'\n",
      " 'Educational and research purposes'\n",
      " 'The model can be used for answering questions about images, describing visual content, creating stories grounded in multiple images, extracting information from documents, and performing basic arithmetic operations.'\n",
      " 'Intended for research purposes only.'\n",
      " 'The model is intended to be used as a foundational base model for application-specific fine-tuning. Developers must evaluate and fine-tune the model for safe performance in downstream applications.'\n",
      " 'The model is intended for research purposes for now.'\n",
      " 'This model is intended to be used for generating orbital videos of objects from still images.'\n",
      " 'It can be used to generate melodies, backing tracks, stems, and sound effects.'\n",
      " 'Reka Core can be used in e-commerce, social media, digital content and video games, healthcare, robotics, and other industries for tasks that require multimodal understanding, coding, complex reasoning, and more.'\n",
      " 'FuseChat is intended to be used as a powerful chat bot that takes in text inputs and provides text-based responses. It can be utilized in a variety of domains including writing, roleplay, reasoning, math, coding, stem, and humanities.'\n",
      " 'This model is intended for commercial and research use in English and can be fine-tuned for use in other languages.'\n",
      " 'The Pile was intended to be used as a high quality large text dataset for language modeling tasks, explained in more detail in the paper [[Section 1]](https://arxiv.org/pdf/2101.00027.pdf#section.1).\\n'\n",
      " 'As stated in the model card: \"GPT-NeoX-20B learns an inner representation of the English language that can be used to extract features useful for downstream tasks. The model is best at what it was pretrained for however, which is generating text from a prompt. Due to the generality of the pretraining set, it has acquired the ability to generate completions across a wide range of tasks - from programming to fiction writing [[Model Card]](https://mystic.the-eye.eu/public/AI/models/GPT-NeoX-20B/20B_model_card.md).\"\\n'\n",
      " 'Intended to be used as an NLP infrastructure.\\n'\n",
      " 'The model is aimed at downstream tasks that benefit from the encoder-decoder architecture. Particularly useful for tasks involving code.'\n",
      " 'Future multimodal research' 'unknown'\n",
      " 'to be used as a personal assistant chatbot for everyday activities'\n",
      " 'allowing companies to incorporate generative AI into their business models'\n",
      " 'Chronos can be used for zero-shot time series forecasting on univariate time series from arbitrary domains and with arbitrary horizons. Chronos models can also be fine-tuned for improved performance of specific datasets. Embeddings from Chronos encoder may also be useful for other time series analysis tasks such as classification, clustering, and anomaly detection.'\n",
      " 'bridging the gap between natural language understanding and computational problem-solving'\n",
      " 'The model is intended to generate high-quality, photorealistic human images from text descriptions. Applications include avatar generation and potentially virtual reality and video game character creation.'\n",
      " 'Research on large language models; as a foundation for further specialization and finetuning for specific usecases (e.g., summarization, text generation, chatbot, etc.)'\n",
      " 'Document generation, document review, Q&A, customer response scenarios.'\n",
      " 'Further research on X-embodiment models.'\n",
      " 'furthering research in developing unified and generalist models for biomedicine.'\n",
      " 'To empower and enrich the open research community by providing access to state-of-the-art language models.'\n",
      " 'As a foundation model to fine-tune and create more specialized models that support use cases such as code completion, fill-in-the-middle, and text summarization. Can also be used as a Tech Assistant prompt and not as an instruction model given training limitations.'\n",
      " 'The model was trained on GitHub code. As such it is not an instruction model and commands do not work well. You should phrase commands like they occur in source code such as comments or write a function signature and docstring and let the model complete the function body.'\n",
      " 'creating code LLMs'\n",
      " 'The model was trained on GitHub code as well as additional selected data sources such as Arxiv and Wikipedia. As such it is not an instruction model and commands like \"Write a function that computes the square root.\" do not work well. Intended to generate code snippets from given context, but not for writing actual functional code directly.'\n",
      " 'Intended to generate code snippets from given context, but not for writing actual functional code directly. The model has been trained on source code from 17 programming languages. The predominant language in source is English although other languages are also present. As such the model is capable of generating code snippets provided some context but the generated code is not guaranteed to work as intended. It can be inefficient and contain bugs or exploits. See the paper for an in-depth discussion of the model limitations.'\n",
      " 'To faciliate transfer learning research in NLP.' 'NLP tasks'\n",
      " 'Searching the web using text, voice or image'\n",
      " 'LaMDA is a language model, so it can be used for regular langauge modelling tasks without fine-tuning, but its fine-tuned for dialogue tasks.\\n'\n",
      " '\"The dataset was created for pre-training language models by a team of researchers at Google\".\\n'\n",
      " '\"The primary use is research on language models, including: research on NLP applications like machine translation and question answering, advancing fairness and safety research, and understanding limitations of current LLMs. Within Google, PaLM is being used for research on a variety of open- ended text and code generation tasks, including reasoning [[Section 6.3]](https://arxiv.org/pdf/2204.02311.pdf#subsection.6.3) and code synthesis and understanding [[Section 6.4]](https://arxiv.org/pdf/2204.02311.pdf#subsection.6.4)\" [[Model Card]](https://arxiv.org/pdf/2204.02311.pdf#appendix.E).\\n'\n",
      " 'general use large language model that can be used for language, reasoning, and code tasks.'\n",
      " 'to be used for question answering and creating draft summaries from existing documentation, to be reviewed, edited, and approved by the user before use.'\n",
      " 'Text generation tasks including question answering, summarization, and reasoning; content creation, communication, research, and education.'\n",
      " 'To be used in areas of medical research including medical summarization, referral letter generation, and medical simplification tasks.'\n",
      " 'Training counselors\\n' 'creative generation of digital art and images'\n",
      " 'recruiting candidates for business needs'\n",
      " '\"It is strongly recommended that this dataset be used only for research, keeping this in mind when using the dataset, and Kakao Brain does not recommend using this dataset as it is without special processing to clear inappropriate data to create commercial products.\"\\n'\n",
      " 'The intended use of the GPT-3 dataset is to train language models.'\n",
      " 'Evaluating code generation capabilities of models.\\n'\n",
      " 'Training language models on code.' 'Training multimodal vision models.'\n",
      " 'The intended use is to train speech models.'\n",
      " 'GPT-3 was intended to be use through the OpenAI API by developers for language applications. Other intended use of GPT-3 include researchers accessing the model through the API to study its paradigms [[Model Card]](https://github.com/openai/gpt-3/blob/master/model-card.md).\\n'\n",
      " 'Codex is intended to be used for coding related language modelling tasks.\\n'\n",
      " 'As stated in the model card: \"The intended direct users of InstructGPT are developers who access its capabilities via the OpenAI API. Through the OpenAI API, the model can be used by those who may not have AI development experience, to build and explore language modeling systems across a wide range of functions. We also anticipate that the model will continue to be used by researchers to better understand the behaviors, capabilities, biases, and constraints of large-scale language models\" [[Model Card]](https://github.com/openai/following-instructions-human-feedback/blob/main/model-card.md).\\n'\n",
      " 'Whisper is a general-purpose speech recognition model; it is a multi-task model that can perform multilingual speech recognition as well as speech translation and language identification.\\n'\n",
      " 'The model is intended to be used by AI researchers to better understand \"robustness, generalization, and other capabilities, biases, and constraints of computer vision models\" [[CLIP Model Card]](https://github.com/openai/CLIP/blob/main/model-card.md).\\n'\n",
      " '\"The model is intended for others to use for training their own generative models\" [[Model Card]](https://github.com/openai/DALL-E/blob/master/model_card.md).\\n'\n",
      " '\"The intended use of the DALL·E 2 Preview at this time is for personal, non-commercial exploration and research purposes by people who are interested in understanding the potential uses of these capabilities\" [[Use]] (https://github.com/openai/dalle-2-preview/blob/main/system-card.md#use).\\n'\n",
      " 'OpenAI API was designed to be used by developers to empower applications, and researchers to study language models [[Section 3]](https://openai.com/api/policies/terms/).\\n'\n",
      " 'The intended use of the DALL·E 3 Preview at this time is for personal, non-commercial exploration and research purposes by people who are interested in understanding the potential uses of these capabilities'\n",
      " 'Used to train the BloombergGPT model.'\n",
      " '\"This model will assist Bloomberg in improving existing financial NLP tasks, such as sentiment analysis, named entity recognition, news classification, and question answering, among others. Furthermore, BloombergGPT will unlock new opportunities for marshalling the vast quantities of data available on the Bloomberg Terminal to better help the firm\\'s customers, while bringing the full potential of AI to the financial domain.\"\\n'\n",
      " 'The dataset is intended to support open and reproducible AI research, enhancing accessibility, diversity, and democracy in AI by enabling everyone to explore large models.'\n",
      " 'to be used to help make the Nextdoor experience more positive for users'\n",
      " 'Pre-training of language models by DeepMind researchers [[Model Card]](https://arxiv.org/pdf/2112.11446.pdf#appendix.C).\\n'\n",
      " 'Pre-training of vision and language models by DeepMind researchers [[Datasheet]](https://arxiv.org/pdf/2204.14198.pdf#appendix.F).\\n'\n",
      " 'The intended uses are stated in the model card: \"The primary use is research on visual language models (VLM), including: research on VLM applications like classification, captioning or visual question answering, understanding how strong VLMs can contribute to AGI, advancing fairness and safety research in the area of multimodal research, and understanding limitations of current large VLMs.\" [[Model Card]](https://arxiv.org/pdf/2204.14198.pdf#appendix.E).\\n'\n",
      " 'The intended uses are stated in the Gopher model card: \"The primary use is research on language models, including: research on NLP applications like machine translation and question answering, understanding how strong language models can contribute to AGI, advancing fairness and safety research, and understanding limitations of current LLMs\" [[Model Card]](https://arxiv.org/pdf/2112.11446.pdf#appendix.B).\\n'\n",
      " 'The intended uses are stated in the Chinchilla model card: \"The primary use is research on language models, including: research on the scaling behaviour of language models along with those listed in Gopher paper\" [[Model Card]](https://arxiv.org/pdf/2203.15556.pdf#appendix.I).\\n'\n",
      " 'The intended uses are stated in the Gopher model card: \"Learn to accomplish a wide variety of tasks from expert demonstrations, such as playing video games, controlling simulated embodiments, and real world block stacking.\" [[Model Card]](https://openreview.net/pdf?id=1ikK0kHjvj#appendix.A).\\n'\n",
      " '\"Provided you keep to these rules, the University grants you (the researcher) a non-exclusive and non-transferable licence to use the content free of charge strictly for non-commercial research (i.e., whose output artefacts are not incorporated in commercial products) for 12 months.\"\\n'\n",
      " 'Sana is intended to be used by employers to provide a learning service for their employees.\\n'\n",
      " 'Developing various NLP-based AI services such as Q&A, chatbot, summarization, information extraction'\n",
      " '\"The primary intended use is to further research into large language models. These models can be used as a foundation model for NLP, applications, ethics, and alignment research. Our primary intended users are researchers who are working to improve LLMs and practitioners seeking reference implementations, training setups, hyperparameters, or pre-trained models. We release these models with a fully permissive Apache license for the community to use freely.\" [[Uses and Limitations]](https://github.com/Cerebras/modelzoo/tree/main/modelzoo/transformers/pytorch/gpt3/configs/Cerebras_GPT#uses-and-limitations).\\n'\n",
      " 'Jais is released with the aim to stimulate research and development in the Arabic NLP community.'\n",
      " 'Jais Chat is released with the aim to stimulate research and development in the Arabic NLP community.'\n",
      " 'Moirai can be used for time series forecasting in multiple domains. It offers robust zero-shot forecasting capabilities and eliminates the need for additional data, extensive computational resources, and expert input for achieving accurate forecasts.'\n",
      " 'pre-training Large Time Series Models'\n",
      " 'Jurassic-1 Instruct was trained specifically to handle instructions-only prompts (\"zero-shot\") without examples (\"few-shot\"). It is the most natural way to interact with language models, and it is the best way to get a sense of the optimal output for your task without any examples.'\n",
      " 'The intended uses are text completion, rewriting, and summarization.'\n",
      " 'The intended uses are text paraphrasing.'\n",
      " 'The Wordtune assistant is a writing assistant'\n",
      " 'intended for use as a foundation layer for fine tuning, training'\n",
      " 'Speech recognition' 'The model is intended for research purposes only.'\n",
      " 'You can use the raw model for many NLP tasks like text generation or fine-tune it to a downstream task.'\n",
      " 'The model can be used for reasoning tasks and is especially tailored for coding and math following specific prompts.'\n",
      " 'Training and evaluating language models on prompt ranking tasks and as a dataset that can be filtered only to include high-quality prompts. These can serve as seed data for generating synthetic prompts and generations.'\n",
      " 'Per the [[HuggingFace repository]](https://huggingface.co/facebook/flava-full), \"The model is intended to serve as a reproducible research artifact for research communities in the light of models whose exact reproduction details are never released such as CLIP and SimVLM.\"\\n'\n",
      " 'Llama 2 is intended for commercial and research use in English. Tuned models are intended for assistant-like chat, whereas pretrained models can be adapted for a variety of natural language generation tasks.'\n",
      " 'SA-1B is intended to be used for research purposes only. It allows access to a privacy protecting and copyright friendly large-scale image dataset. Researchers can use it to train and evaluate generic object segmentation models.'\n",
      " '\"SAM is intended to be used for any prompt-based segmentation task. We explored its use in segmenting objects from a point, edge detection, segmenting all objects, and segmenting detected objects. We explored how SAM can integrate with other vision models to segment objects from text.\"\\n'\n",
      " 'adapting LLMs to work with collaborative writing and updating.'\n",
      " 'The primary use of MusicGen is research on AI-based music generation'\n",
      " 'The primary use of AudioGen is research on AI-based audio generation.'\n",
      " 'Code Llama and its variants is intended for commercial and research use in English and relevant programming languages.'\n",
      " 'Llama 3 is intended for a broad range of use cases, including AI assistance, content creation, learning, and analysis.'\n",
      " 'HyperWrite is intended to be used as a writing assistant.\\n'\n",
      " 'It is expected to be used for various research purposes.'\n",
      " 'The dataset was intended and released for research purposes.\\n'\n",
      " 'Intended to be used by crowdworkers who are tasked with ranking model answers.\\n'\n",
      " 'Claude 2 tends to perform well at general, open-ended conversation; search, writing, editing, outlining, and summarizing text; coding; and providing helpful advice about a broad range of subjects. Claude 2 is particularly well suited to support creative or literary use cases. They can take direction on tone and “personality,” and users have described them as feeling steerable and conversational.'\n",
      " 'as an integrated AI assistant in Google Sheets'\n",
      " 'Claude models excel at open-ended conversation and collaboration on ideas, and also perform exceptionally well in coding tasks and when working with text - whether searching, writing, editing, outlining, or summarizing.'\n",
      " 'To empower large-scale monolingual and multilingual modeling projects with both the data and the processing tools, as well as stimulate research around this large multilingual corpus.'\n",
      " 'Multitask finetuning of language models.'\n",
      " 'You can use the models to perform inference on tasks by specifying your query in natural language, and the models will generate a prediction.'\n",
      " 'This model is being created in order to enable public research on large language models (LLMs). LLMs are intended to be used for language generation or as a pretrained base model that can be further fine-tuned for specific tasks. Use cases below are not exhaustive.'\n",
      " 'We recommend using the model to perform tasks expressed in natural language.'\n",
      " 'generating text from a prompt'\n",
      " '\"Dolly is intended exclusively for research purposes and is not licensed for commercial use.\" [[Limitations]](https://github.com/databrickslabs/dolly#limitations).\\n'\n",
      " 'DBRX models are open, general-purpose LLMs intended and licensed for both commercial and research applications. They can be further fine-tuned for various domain-specific natural language and coding tasks.'\n",
      " 'research on LLMs and chatbots'\n",
      " 'The model can be used for text generation tasks in both Japanese and English.'\n",
      " 'academic research'\n",
      " 'In conjunction with a LLM to improve its capability for using API calls.'\n",
      " 'Academic research and free commercial usage'\n",
      " 'Research on large language models; as a foundation for further specialization for specific use cases.'\n",
      " 'The authors recommend using the dataset \"for research purposes\" and warn that \"this large-scale dataset is non-curated. It was built for research purposes to enable testing model training on larger scale for broad researcher and other interested communities, and is not meant for any real-world production or application.\"'\n",
      " 'The authors recommend using the dataset \"for research purposes\" and \"do not recommend using it for creating ready-to-go industrial products, as the basic research about general properties and safety of such large-scale models, which we would like to encourage with this release, is still in progress\"'\n",
      " 'academic research purposes'\n",
      " 'The model can be used for fast, high-quality text-to-image generation. It supports 1-step, 2-step, 4-step, and 8-step distilled models which provide varying generation quality.'\n",
      " 'GitHub CoPilot is intended to be used as a coding assistant.\\n'\n",
      " 'Search engine' 'Providing document insights to users.'\n",
      " 'Suggesting email replies.'\n",
      " 'Security Copilot is designed to enhance the capabilities of cybersecurity professionals. It leverages machine speed and scale to accelerate response to security incidents, discover and process threat signals, and assess risk exposure within minutes.'\n",
      " 'analyzing, writing, and connecting business documents and data'\n",
      " 'Creating large amounts of instruction data, particularly with high complexity'\n",
      " 'training and evaluation in the field of natural language processing.'\n",
      " 'Phi-1.5 is best suited for answering prompts using the QA format, the chat format, and the code format.'\n",
      " 'Orca 2 is built for research purposes only. The main purpose is to allow the research community to assess its abilities and to provide a foundation for building better frontier models.'\n",
      " \"The model's primary use cases are for commercial and research purposes that require capable reasoning in memory or compute constrained environments and latency-bound scenarios. It can also serve as a building block for generative AI-powered features.\"\n",
      " \"The intended use of the dataset is to train Cohere's language models.\\n\"\n",
      " 'On the model card, the intended uses are stated as \"interactive autocomplete, augmenting human writing processes, summarization, text rephrasing, and other text-to-text tasks in non-sensitive domains\" [[Model Card]](https://docs.cohere.ai/generation-card).\\n'\n",
      " 'The intended uses are stated as \"estimating semantic similarity between two sentences, choosing a sentence which is most likely to follow another sentence, sentiment analysis, topic extraction, or categorizing user feedback\" on the Cohere model card [[Model Card]](https://docs.cohere.ai/representation-card).\\n'\n",
      " 'Intended to be used by developers who would like to incorporate NLP into their applications [[Cohere Website]](https://cohere.ai/).\\n'\n",
      " 'Efficient enterprise search and retrieval.'\n",
      " 'Grok-1 is intended to be used as the engine behind Grok for natural language processing tasks including question answering, information retrieval, creative writing and coding assistance.'\n",
      " 'Grok-1.5V can be used for understanding documents, science diagrams, charts, screenshots, photographs. It can also translate diagrams into Python code.'\n",
      " 'Alpaca is intended and licensed for research use only.'\n",
      " 'The model is intended for general task and conversation capabilities, function calling, and JSON structured outputs.'\n",
      " 'The model is intended for instruction-generation, creating questions involving complex scenarios and generating reasoning steps for those questions.'\n",
      " 'Advancing future research in multilingual LLMs'\n",
      " 'Integration into other instruction-tuned LLMs to further enhance arithmetic reasoning abilities in solving math word problems.'\n",
      " 'Research on evaluating the performance of large language models and chatbots.'\n",
      " 'To be used to conduct instruction-tuning for language models and make the language model able to judge open-ended answer pairs.'\n",
      " 'The model is intended for multilingual tasks such as knowledge retrieval, math reasoning, and instruction following. Also, it could be used to provide multilingual assistance.'\n",
      " 'Language model pretraining, finetuning, and evaluation.'\n",
      " 'to support open and collaborative AI research by making the full LLM training process transparent.']\n",
      "Column: prohibited_uses\n",
      "Unique values: [nan\n",
      " 'Not suitable for creating realistic photos or for users who expect high-quality results from short or simple prompts.'\n",
      " \"Prohibited uses are listed in the Terms of Service [[Terms of Service]](https://www.askviable.com/terms-of-service). The terms don't include statements specific to the use of the content generated by the system or GPT-3.\\n\"\n",
      " 'Prohibited from deploying in production environments for natural language generation or any professional health and medical purposes.'\n",
      " 'unknown'\n",
      " 'Using the model to generate representations of real-world people or events.'\n",
      " \"The model was not trained to be factual or true representations of people or events, and therefore using the model to generate such content is out-of-scope for the abilities of this model. The model should not be used in any way that violates Stability AI's Acceptable Use Policy.\"\n",
      " 'The model should not be used for generating factual or true representations of people or events, or in any way that violates Stability AIs Acceptable Use Policy.'\n",
      " 'Uploading copyrighted material for transformation.'\n",
      " 'Production use without adequate assessment of risks and mitigation; any use cases which may be considered irresponsible or harmful.'\n",
      " 'Illegal or abusive activity, security violations, network abuse\\n'\n",
      " 'No explicit prohibited uses stated, though it is noted that users should undertake thorough safety testing.'\n",
      " 'See BigCode Open RAIL-M license and FAQ'\n",
      " 'See https://huggingface.co/datasets/bigcode/the-stack'\n",
      " 'Users are encouraged to use the large language model responsibly and ethically. By using this model, you agree not to use it for purposes that promote hate speech, discrimination, harassment, or any form of illegal or harmful activities.'\n",
      " \"Prohibited use cases aren't specifically spelled out for Google search, but several illegal and discouraged use cases are shared in the Respect Others section of the [[Term of Service]](https://policies.google.com/terms).\\n\"\n",
      " \"The prohibited uses for Infiniset weren't specifically listed, but the Google AI principles inspired safety objectives in [[Appendix A.1]](https://arxiv.org/pdf/2201.08239.pdf#subsection.A.1) advises avoiding harm, unjust impact and misinformation, among others.\\n\"\n",
      " \"The prohibited uses of LaMDA weren't specifically listed, but the Google AI principles inspired safety objectives in [[Appendix A.1]](https://arxiv.org/pdf/2201.08239.pdf#subsection.A.1) advises avoiding harm, unjust impact and misinformation, among others.\\n\"\n",
      " '\"... should not be used for any of the unacceptable language model use cases, e.g., generation of toxic speech\" [[Datasheet]](https://arxiv.org/pdf/2204.02311.pdf#appendix.D).\\n'\n",
      " 'The model \"should not be used for downstream applications without further analysis on factors in the proposed downstream application [[Model Card]](https://arxiv.org/pdf/2204.02311.pdf#appendix.E)\"\\n'\n",
      " 'becoming part of a general-purpose service or product or use within specific downstream applications without prior assessment'\n",
      " 'Prohibited uses are specified in the Gemma Prohibited Use Policy here https://ai.google.dev/gemma/prohibited_use_policy'\n",
      " 'Unfit for real-world deployment in the safety-critical medical domain.'\n",
      " 'AI/ML training, attempting to create abusive, illegal, or confidential content.'\n",
      " 'The data must not be utilized for malicious or harmful purposes towards humanity.'\n",
      " 'Authors note the following limitations: \"Kakao Brain tried to construct a \"Safe\" dataset when building the COYO dataset. However, despite these efforts, this large-scale dataset was not hand-picked by humans to avoid the risk due to its very large size (over 700M). Keep in mind that the unscreened nature of the dataset means that the collected images can lead to strongly discomforting and disturbing content for humans. The COYO dataset may contain some inappropriate data, and any problems resulting from such data are the full responsibility of the user who used it.\"\\n'\n",
      " 'Access to GPT-3 is governed by Open AI API Usage Guidelines and API Terms of Use, prohibiting the use of the API in a way that causes societal harm. [[Usage Guidelines]] (https://beta.openai.com/docs/usage-guidelines/content-policy) [[Terms of Use]](https://openai.com/api/policies/terms/). The list of disallowed applications can be found in the usage guidelines [[Disallowed Applications]] (https://beta.openai.com/docs/usage-guidelines/disallowed-applications).\\n'\n",
      " 'Access to InstructGPT is governed by Open AI API Usage Guidelines and API Terms of Use, prohibiting the use of the API in a way that causes societal harm. [[Usage Guidelines]] (https://beta.openai.com/docs/usage-guidelines/content-policy) [[Terms of Use]](https://openai.com/api/policies/terms/). The list of disallowed applications can be found in the usage guidelines [[Disallowed Applications]] (https://beta.openai.com/docs/usage-guidelines/disallowed-applications).\\n'\n",
      " '\"Any deployed use case of the model - whether commercial or not - is currently out of scope. Non-deployed use cases such as image search in a constrained environment, are also not recommended unless there is thorough in-domain testing of the model with a specific, fixed class taxonomy. This is because our safety assessment demonstrated a high need for task specific testing especially given the variability of CLIP’s performance with different class taxonomies. This makes untested and unconstrained deployment of the model in any use case currently potentially harmful.\\nCertain use cases which would fall under the domain of surveillance and facial recognition are always out-of-scope regardless of performance of the model. This is because the use of artificial intelligence for tasks such as these can be premature currently given the lack of testing norms and checks to ensure its fair use.\\nSince the model has not been purposefully trained in or evaluated on any languages other than English, its use should be limited to English language use cases\" [[Model Card]](https://github.com/openai/CLIP/blob/main/model-card.mdlicen).\\n'\n",
      " 'Use of the model is governed by the OpenAI Content Policy, which prohibits posting of G rated content. Users are not allowed to utilize the model in commercial products in the preview version [[Content Policy]] (https://github.com/openai/dalle-2-preview/blob/main/system-card.md#policies-and-enforcement).'\n",
      " 'OpenAI API Terms of Use prohibits the use of the API in a way violating the applicable law, including: (i) \"Illegal activities, such as child pornography, gambling, cybercrime, piracy, violating copyright, trademark or other intellectual property laws\"; (ii) \"Accessing or authorizing anyone to access the APIs from an embargoed country, region, or territory as prohibited by the U.S. government\"; (iii) \"Threatening, stalking, defaming, defrauding, degrading, victimizing or intimidating anyone for any reason\". The usage requirements are detailed in the Terms of Use [[Section 3]](https://openai.com/api/policies/terms/).\\n'\n",
      " 'Use of the model is governed by the OpenAI Content Policy, which prohibits posting of G rated content. Users are not allowed to utilize the model in commercial products in the preview version.'\n",
      " 'It should not be used for tasks that infringe on copyright laws.'\n",
      " 'The model card lists the following as out of scope uses of the model: \"Uses of the model for visually conditioned language generation in harmful or deceitful settings. Broadly speaking, the model should not be used for downstream applications without further safety and fairness mitigations specific to each application.\" [[Model Card]](https://arxiv.org/pdf/2204.14198.pdf#appendix.E).\\n'\n",
      " 'The model card lists the following as out of scope uses of the model: \"for language generation in harmful or deceitful settings. More generally, the model should not be used for downstream applications without further safety and fairness mitigations\" [[Model Card]](https://arxiv.org/pdf/2112.11446.pdf#appendix.B).\\n'\n",
      " 'The model card lists the following as out of scope uses of the model: \"for language generation in harmful or deceitful settings. More generally, the model should not be used for downstream applications without further safety and fairness mitigations\" [[Model Card]](https://arxiv.org/pdf/2203.15556.pdf#appendix.I).\\n'\n",
      " 'The model card lists the following as out of scope uses of the model: \"Not intended for commercial or production use. Military uses are strictly prohibited.\" [[Model Card]](https://openreview.net/pdf?id=1ikK0kHjvj#appendix.A).\\n'\n",
      " '- Authors note the following prohibited uses: \"You must not use the content other than for the Permitted Purpose in strict conformity with these terms and any other reasonable instructions of the University. You must not, except as may be strictly necessary for carrying out the Permitted Purpose, provide or otherwise make available content to any third party or allow use of it or them by or on behalf of any third party, in whole or in part, whether by way of sale, resale, loan, transfer, hire or any other form of exploitation; or attempt to identify any living or deceased individual from the content.\" [[Terms of Access]](https://github.com/m-bain/webvid/blob/main/TERMS.md)\\n- Authors also note the following limitations of the dataset: \"We note that data sourced from the web may be prone to biases and may contain graphic content. Please be careful of unintended societal, gender, racial and other biases when training or deploying models trained on this data.\" [[Disclaimer]](https://github.com/m-bain/webvid#disclaimer-%EF%B8%8F)\\n'\n",
      " 'Authors note the following limitations of the model: \"Cerebras-GPT models are trained on the Pile, with English language only, and are not suitable for machine translation tasks. Cerebras-GPT models have not been tuned for human-facing dialog applications like chatbots and will not respond to prompts in a similar way to models that have received instruction tuning or reinforcement learning from human feedback (RLHF) like Flan-T5 or ChatGPT.\" [[Uses and Limitations]](https://github.com/Cerebras/modelzoo/tree/main/modelzoo/transformers/pytorch/gpt3/configs/Cerebras_GPT#out-of-scope-use).\\n'\n",
      " 'Generating or endorsing hate speech, disseminating false information, engaging in illegal activities, managing sensitive data, attempting language generalization beyond Arabic and English, and making critical decisions with high stakes.'\n",
      " 'Illegal activities, such as hate speech, gambling, child pornography or violating intellectual property rights; Harassment, victimization, intimidation, fraud or spam; Creation or dissemination of misinformation, promotion of self-harm, glorification of violent events or incitement of violence.'\n",
      " 'No uses are explicitly prohibited by the authors. They note the following limitations of the dataset: \"We note that the distribution of identities and activities in the HowTo100M dataset may not be representative of the global human population and the diversity in society. Please be careful of unintended societal, gender, racial and other biases when training or deploying models trained on this data.\"\\n'\n",
      " 'The model was not trained to be factual or true representations of people or events, and therefore using the model to generate such content is out-of-scope for the abilities of this model.'\n",
      " 'This dataset only contains rankings for prompts, not prompt/response pairs so it is not suitable for direct use for supervised fine-tuning of language models.'\n",
      " 'Per the [[HuggingFace repository]](https://huggingface.co/facebook/flava-full), \"Any deployed use case of the model - whether commercial or not\" - is currently out of scope.\\n'\n",
      " 'Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in languages other than English. Use in any other way that is prohibited by the Acceptable Use Policy and Licensing Agreement for Llama 2.'\n",
      " 'Authors note the following limitations of the dataset:\\n  The masks are generated by a segmentation model, so there may be errors\\nor inconsistencies in the masks.\\n  While no two images are the same, there are instances of images of the same\\nsubject taken close together in time.\\n  The dataset contains scenes of protests, or other gatherings that may suggest\\nreligious beliefs, political opinions or union memberships that may be offensive.\\n'\n",
      " 'For out-of-scope use cases see terms of use in [[LICENSE]](https://github.com/facebookresearch/segment-anything/blob/main/LICENSE). Authors also discuss the following limitations of the model: \"While SAM performs well in general, it is not perfect. It can miss fine structures, hallucinates small disconnected components at times, and does not produce boundaries as crisply as more computationally intensive methods that “zoom-in”, e.g. [18]. In general, we expect dedicated interactive segmentation methods to outperform SAM when many points are provided, e.g. [67]. Unlike these methods, SAM is designed for generality and breadth of use rather than high IoU interactive segmentation. Moreover, SAM can process prompts in real-time, but nevertheless SAM\\'s overall performance is not real-time when using a heavy image encoder. Our foray into the text-to-mask task is exploratory and not entirely robust, although we believe it can be improved with more effort. While SAM can perform many tasks, it is unclear how to design simple prompts that implement semantic and panoptic segmentation. Finally, there are domain-specific tools, such as [7], that we expect to outperform SAM in their respective domains.\"\\n'\n",
      " 'The model should not be used on downstream applications without further risk evaluation and mitigation. The model should not be used to intentionally create or disseminate music pieces that create hostile or alienating environments for people. This includes generating music that people would foreseeably find disturbing, distressing, or offensive; or content that propagates historical or current stereotypes.'\n",
      " 'The model should not be used on downstream applications without further risk evaluation and mitigation. The model should not be used to intentionally create or disseminate audio pieces that create hostile or alienating environments for people. This includes generating audio that people would foreseeably find disturbing, distressing, or offensive; or content that propagates historical or current stereotypes.'\n",
      " 'Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in languages other than English. Use in any other way that is prohibited by the Acceptable Use Policy and Licensing Agreement for Code Llama and its variants.'\n",
      " 'It cannot be used for commercial purposes.'\n",
      " 'Claude 2 should not be used on their own in high stakes situations where an incorrect answer would cause harm.'\n",
      " 'Prohibited uses include, but are not limited to, political campaigning or lobbying, surveillance, social scoring, criminal justice decisions, law enforcement, and decisions related to financing, employment, and housing.'\n",
      " \"Using the model in high-stakes settings is out of scope for this model (e.g. biomedical/political/legal/finance domains, evaluating or scoring individuals). The model is not designed for critical decisions nor uses with any material consequences on an individual's livelihood or wellbeing. The model outputs content that appears factual but may not be correct. Misuse. Intentionally using the model for harm, violating human rights, or other kinds of malicious activities, is a misuse of this model (e.g. spam generation, disinformation, disparagement, deception, surveillance).\"\n",
      " 'Authors note the following limitations of the model: \"The Dolly model family is under active development, and so any list of shortcomings is unlikely to be exhaustive, but we include known limitations and misfires here as a means to document and share our preliminary findings with the community. In particular, dolly-6b struggles with syntactically complex prompts, mathematical operations, factual errors, dates and times, open-ended question answering, hallucination, enumerating lists of specific length, and stylistic mimicry.\" [[Limitations]](https://github.com/databrickslabs/dolly#limitations).\\n'\n",
      " 'DBRX models are not intended to be used out-of-the-box in non-English languages, and do not support native code execution, function calling or any use that violates applicable laws or regulations or is otherwise prohibited by the Databricks Open Model License and Databricks Open Model Acceptable Use Policy.'\n",
      " 'irresponsible or harmful use or production use without adequate assessment of risks and mitigation.'\n",
      " 'No uses are explicitly prohibited by the license. Users are warned from using LAION-400M for any real-world production or application.'\n",
      " 'No uses are explicitly prohibited by the license. Users are warned from using LAION-5B for non-research purposes.'\n",
      " 'No uses are explicitly prohibited by the license. Users are warned from using LAION-2B-en for non-research purposes.'\n",
      " 'commercial use'\n",
      " 'Access to GPT-3 is governed by GitHub Acceptable Use Policies and Terms of Service, both of which list a set of prohibited uses [[Use Policies]] (https://docs.github.com/en/site-policy/acceptable-use-policies/github-acceptable-use-policies) [[Terms of Service]] (https://docs.github.com/en/site-policy/github-terms/github-terms-of-service).\\n'\n",
      " 'Any purposes other than research.'\n",
      " 'The model should not be used for high-risk scenarios without adequate evaluation and mitigation techniques for accuracy, safety, and fairness.'\n",
      " 'The usage of the model is bound by the Cohere usage guidelines [[Usage Guidelines]](https://docs.cohere.ai/usage-guidelines). A non-comprehensive list of specific application violating these guidelines are: astroturfing, generation of misinformation and other harmful content, and \"generation of text about people, places, or events without a human-in-the-loop\" [[Model Card]](https://docs.cohere.ai/generation-card).\\n'\n",
      " 'The usage of the model is bound by the Cohere usage guidelines [[Usage Guidelines]](https://docs.cohere.ai/usage-guidelines). A non-comprehensive list of specific application violating these guidelines are: extraction of identity and demographic information, building purposefully opaque text classification systems, and \"building downstream classifiers that serve as automated decision-making systems that have real-world consequences on people, where those decisions are made without a human-in-the-loop\" [[Model Card]](https://docs.cohere.ai/representation-card).\\n'\n",
      " 'The usage of the API is bound by the Cohere usage guidelines. Disallowed use cases include violence and threats, antisocial and antidemocratic uses, deceit, attacks on security or privacy, unsafe unsupervised uses, decision-making, high-Risk generations among others [[Usage Guidelines]](https://docs.cohere.ai/usage-guidelines).\\n'\n",
      " 'The model should not be used in a way that could lead to inaccurate, misleading or potentially harmful generation. Users should comply with local laws and regulations when deploying the model.'\n",
      " 'Any tasks which may considered irresponsible or harmful.'\n",
      " 'SambaLingo should not be used for mission-critical applications, applications involving the safety of others, and highly critical decisions.']\n",
      "Column: monitoring\n",
      "Unique values: [nan 'unknown' 'unknokwn'\n",
      " 'Advanced content recognition is used to maintain compliance and prevent copyright infringement.'\n",
      " 'At will monitoring by the provider'\n",
      " 'It is implied that Google scan uses of its products for spam, malware and illegal content in the [[Term of Service]](https://policies.google.com/terms).\\n'\n",
      " 'Google internal monitoring'\n",
      " 'OpenAI reviews all use cases of the model [[Model Card]](https://github.com/openai/gpt-3/blob/master/model-card.md).\\n'\n",
      " \"Uses of the model are monitored. In the preview version, any user can flag content. The specific policies for monitoring are not disclosed, but possible measures include disabling of accounts violating the content policies [[Monitoring and Reporting]] (https://github.com/openai/dalle-2-preview/blob/main/system-card.md#monitoring-and-reporting).\\n'\"\n",
      " 'OpenAI may monitor the API use to ensure \"quality and improve OpenAI systems, products and services; perform research; and ensure compliance\" with the Terms of Service and all applicable laws. Users of the API will give OpenAI reasonable access to their application to monitor compliance with the terms listed in the Terms of Service [[Section 5(b)]](https://openai.com/api/policies/terms/). Apps using the OpenAI API should submit an application once they are deployed to real users. The review form takes 10 minutes to complete and over 97% of the applications are directly accepted or conditionally accepted. The applicants are notified of the decision within 2 business days [[App Review Guidelines]] (https://beta.openai.com/docs/usage-guidelines/app-review).\\n'\n",
      " 'Uses of the model are monitored. In the preview version, any user can flag content. The specific policies for monitoring are not disclosed, but possible measures include disabling of accounts violating the content'\n",
      " 'Unknown'\n",
      " 'Quality filtration, deduplication, and risk mitigation via logistic qualifiers and regular expressions used.'\n",
      " 'The dataset will be hosted at https://ai.facebook.com/datasets/segment-anything and maintained by Meta AI. \"If a user observes objectionable image(s) in the dataset, we invite them to report the image(s) at segment-anything at meta.com for removal\" \"To aid reproducibility of research using SA-1B, the only updates (to the dataset) will be to remove reported images.\" \"We encourage users to gather further annotations for SA-1B. Any users who generate annotations will be liable for hosting and distributing their annotations.\"\\n'\n",
      " 'Extensive internal and external performance evaluation and red-teaming approach for safety testing.'\n",
      " 'unknkown'\n",
      " 'value: unknown explanation: >\\n  There may be internal monitoring mechanisms unknown to the public.\\n'\n",
      " 'Issues like allocation, high-risk scenarios, misinformation, generation of harmful content and misuse should be monitored and addressed.'\n",
      " 'The usage of the model is monitored by Cohere [[Model Card]](https://docs.cohere.ai/generation-card).\\n'\n",
      " 'The usage of the model is monitored by Cohere [[Model Card]](https://docs.cohere.ai/representation-card).\\n'\n",
      " 'All applications developed using the Cohere API is subject to review by Cohere.\\n'\n",
      " \"Governed by the laws of China, without regard to conflict of law principles, and the UN Convention on Contracts for the International Sale of Goods does not apply to this Agreement. And The People's Courts in Hangzhou City shall have exclusive jurisdiction over any dispute arising out of this Agreement.\"]\n",
      "Column: feedback\n",
      "Unique values: [nan\n",
      " 'https://huggingface.co/time-series-foundation-models/Lag-Llama/discussions'\n",
      " 'https://huggingface.co/ibm-nasa-geospatial/Prithvi-100M/discussions'\n",
      " 'https://huggingface.co/cagliostrolab/animagine-xl-3.1/discussions'\n",
      " 'unknown' 'https://huggingface.co/spaces/suno/bark/discussions'\n",
      " 'https://huggingface.co/togethercomputer/Llama-2-7B-32K-Instruct/discussions'\n",
      " 'Feedback can be sent to Together via https://www.together.ai/contact'\n",
      " 'https://huggingface.co/togethercomputer/StripedHyena-Hessian-7B/discussions'\n",
      " 'https://huggingface.co/togethercomputer/StripedHyena-Nous-7B/discussions'\n",
      " 'https://huggingface.co/epfl-llm/meditron-7b/discussions'\n",
      " 'https://huggingface.co/xverse/XVERSE-65B/discussions'\n",
      " 'Feedback can be sent to authors via poplpr@bit.edu.cn'\n",
      " 'https://huggingface.co/GeneZC/MiniMA-3B/discussions'\n",
      " 'https://huggingface.co/internlm/internlm-20b/discussions'\n",
      " 'https://huggingface.co/BioMistral/BioMistral-7B/discussions'\n",
      " 'https://huggingface.co/GreenBitAI/LLaMA-30B-2bit-groupsize8/discussions'\n",
      " 'https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha/discussions'\n",
      " 'https://huggingface.co/HuggingFaceM4/idefics-80b-instruct/discussions'\n",
      " 'https://huggingface.co/datasets/HuggingFaceM4/OBELICS/discussions'\n",
      " 'https://huggingface.co/TurkuNLP/gpt3-finnish-13B/discussions'\n",
      " 'https://huggingface.co/TurkuNLP/bloom-finnish-176b/discussions'\n",
      " 'https://huggingface.co/datasets/HuggingFaceTB/cosmopedia/discussions'\n",
      " 'https://huggingface.co/HuggingFaceM4/idefics2-8b/discussions'\n",
      " 'https://huggingface.co/datasets/HuggingFaceM4/the_cauldron/discussions'\n",
      " 'https://huggingface.co/DeepFloyd/IF-I-XL-v1.0/discussions'\n",
      " 'https://huggingface.co/CompVis/stable-diffusion/discussions'\n",
      " 'https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt/discussions'\n",
      " 'https://huggingface.co/stabilityai/stablelm-2-1_6b/discussions'\n",
      " 'https://huggingface.co/stabilityai/stable-cascade/discussions'\n",
      " 'https://huggingface.co/stabilityai/sv3d/discussions'\n",
      " 'https://huggingface.co/FuseAI/FuseChat-7B-VaRM/discussions'\n",
      " 'https://huggingface.co/moreh/MoMo-72B-lora-1.8.7-DPO/discussions'\n",
      " 'https://huggingface.co/mistralai/Mistral-7B-v0.1/discussions'\n",
      " 'https://github.com/ReexpressAI/support'\n",
      " 'https://huggingface.co/cognitivecomputations/dolphin-2_2-yi-34b/discussions'\n",
      " 'https://huggingface.co/cognitivecomputations/WizardLM-30B-Uncensored/discussions'\n",
      " 'https://huggingface.co/vilm/vulture-180b/discussions'\n",
      " 'Feedback can be given by emailing the authors at contact at eleuther.ai.\\n'\n",
      " 'Feedback can be provided using the  # 20b channel in EleutherAI Discord group [[EleutherAI Blog Post]](https://blog.eleuther.ai/announcing-20b/). Find the Discord link in the FAQ page [[FAQ]](https://www.eleuther.ai/faq/).\\n'\n",
      " 'Email support'\n",
      " 'https://huggingface.co/EleutherAI/pythia-6.9b/discussions'\n",
      " 'https://huggingface.co/EleutherAI/llemma_34b/discussions'\n",
      " 'https://huggingface.co/datasets/EleutherAI/proof-pile-2/discussions'\n",
      " 'https://huggingface.co/openbmb/UltraLM-13b/discussions'\n",
      " 'https://huggingface.co/datasets/stingning/ultrachat/discussions'\n",
      " 'https://huggingface.co/NinedayWang/PolyCoder-2.7B/discussion'\n",
      " 'https://huggingface.co/OpenAssistant/llama2-70b-oasst-sft-v10/discussions'\n",
      " 'https://huggingface.co/RWKV/rwkv-4-world-7b/discussions'\n",
      " 'https://huggingface.co/RWKV/rwkv-4-14b-pile/discussions'\n",
      " 'https://huggingface.co/RWKV/rwkv-5-world-3b/discussions'\n",
      " 'https://huggingface.co/amazon/FalconLite2/discussions'\n",
      " 'https://github.com/amazon-science/chronos-forecasting/discussions'\n",
      " 'https://huggingface.co/NucleusAI/nucleus-22B-token-500B/discussions'\n",
      " 'https://huggingface.co/IDEA-CCNL/Taiyi-Stable-Diffusion-XL-3.5B/discussions'\n",
      " 'https://huggingface.co/apple/OpenELM-3B-Instruct/discussions'\n",
      " 'https://huggingface.co/bigcode/starcoder/discussions'\n",
      " 'https://huggingface.co/bigcode/santacoder/discussions'\n",
      " 'https://huggingface.co/datasets/bigcode/the-stack/discussions'\n",
      " 'https://huggingface.co/bigcode/starcoder2-15b/discussions'\n",
      " 'https://huggingface.co/bigcode/starcoder2-7b/discussions'\n",
      " 'https://huggingface.co/bigcode/starcoder2-3b/discussions'\n",
      " 'https://huggingface.co/h2oai/h2ogpt-oasst1-512-20b/discussions'\n",
      " 'https://huggingface.co/h2oai/h2o-danube-1.8b-base/discussions'\n",
      " 'https://huggingface.co/datasets/c4/discussions'\n",
      " 'Feedback can be provided by creating an issue in the [[Conceptual Captions GitHub repository]](https://github.com/google-research-datasets/conceptual-captions) or by emailing at conceptual-captions at google.com'\n",
      " 'Feedback can be provided by creating an issue in the [[Conceptual 12M GitHub repository]](https://github.com/google-research-datasets/conceptual-12m) or by emailing at conceptual-captions at google.com'\n",
      " 'https://huggingface.co/t5-large/discussions'\n",
      " 'Feedback can be sent to Google Feedback using the product interface [[Google Feedback]](https://www.google.com/tools/feedback).\\n'\n",
      " 'Contact the authors.'\n",
      " 'https://huggingface.co/google/flan-t5-xxl/discussions'\n",
      " 'Specific queries provided by annotators'\n",
      " 'https://huggingface.co/google/gemma-7b/discussions'\n",
      " 'https://huggingface.co/datasets/uonlp/CulturaX/discussions'\n",
      " 'https://huggingface.co/Skywork/Skywork-13B-base/discussions'\n",
      " 'Feedback can be given by emailing at coyo at kakaobrain.com'\n",
      " 'https://huggingface.co/kotoba-tech/kotoba-speech-v0.1/discussions'\n",
      " 'Email the authors [[Codex Paper]](https://arxiv.org/pdf/2107.03374.pdf).\\n'\n",
      " 'Feedback for GPT-3 can be provided on the feedback form linked in the model card [[Model Card]](https://github.com/openai/gpt-3/blob/master/model-card.md). The form is especially meant to collect feedback on concerns about misuse, synthetic text detection, bias, and risk of generative language models.\\n'\n",
      " 'Email the authors [[InstructGPT Paper]](https://arxiv.org/pdf/2203.02155.pdf).\\n'\n",
      " 'The discussions page of the codebase is not formally cited as a place for feedback, but is being used in this way [[Discussions page]](https://github.com/openai/whisper/discussions)\\n'\n",
      " 'Questions can be shared at the feedback form linked in the CLIP model card [[Model Card]](https://github.com/openai/CLIP/blob/main/model-card.mdlicen).'\n",
      " 'Contact the paper author(s) specified on the paper [[Paper]](https://arxiv.org/pdf/2102.12092.pdf).\\n'\n",
      " 'Feedback can be provided at support at openai.com.'\n",
      " 'Feedback can be provided at openai.com'\n",
      " 'https://huggingface.co/garage-bAInd/Platypus2-13B/discussions'\n",
      " 'The feedback for the model can be provided at the email linked in the model card, geoffreyi at google.com [[Model Card]](https://arxiv.org/pdf/2112.11446.pdf#appendix.B).\\n'\n",
      " 'The feedback for the model can be provided at the email linked in the model card, {jordanhoffmann, sborgeaud, amensch,sifre} at deepmind.com [[Model Card]](https://arxiv.org/pdf/2203.15556.pdf#appendix.I).\\n'\n",
      " 'The feedback for the model can be provided at the email linked in the model card, reedscot at google.com [[Model Card]](https://openreview.net/pdf?id=1ikK0kHjvj#appendix.A).\\n'\n",
      " 'Feedback can be given by emailing at maxbain at robots.ox.ac.uk'\n",
      " 'https://huggingface.co/allenai/cosmo-xl/discussions'\n",
      " 'https://huggingface.co/datasets/allenai/tulu-v2-sft-mixture/discussions'\n",
      " 'https://huggingface.co/allenai/tulu-2-70b/discussions'\n",
      " 'https://huggingface.co/allenai/tulu-2-dpo-70b/discussions'\n",
      " 'https://huggingface.co/allenai/codetulu-2-13b/discussions'\n",
      " 'https://huggingface.co/allenai/OLMo-7B/discussions'\n",
      " 'https://huggingface.co/datasets/allenai/MADLAD-400/discussions'\n",
      " 'https://huggingface.co/cerebras/btlm-3b-8k-base/discussions'\n",
      " 'https://huggingface.co/datasets/cerebras/SlimPajama-627B/discussions'\n",
      " 'https://huggingface.co/Salesforce/blip2-opt-2.7b/discussions'\n",
      " 'https://huggingface.co/Salesforce/moirai-1.0-R-large/discussions'\n",
      " 'https://huggingface.co/datasets/Salesforce/lotsa_data/discussions'\n",
      " 'Feedback can be given by emailing at info at ai21.com'\n",
      " 'https://huggingface.co/ai21labs/Jamba-v0.1/discussions'\n",
      " 'https://huggingface.co/Xwin-LM/Xwin-LM-70B-V0.1/discussions'\n",
      " 'https://huggingface.co/01-ai/Yi-34B/discussions'\n",
      " 'https://huggingface.co/01-ai/Yi-VL-34B/discussions'\n",
      " 'https://huggingface.co/OpenLemur/lemur-70b-v1/discussions'\n",
      " 'https://huggingface.co/OpenLemur/lemur-70b-chat-v1/discussions'\n",
      " 'https://huggingface.co/adept/fuyu-8b/discussions'\n",
      " 'https://huggingface.co/openbmb/cpm-bee-10b/discussions'\n",
      " 'https://huggingface.co/datasets/openbmb/UltraFeedback/discussions'\n",
      " 'https://huggingface.co/openbmb/MiniCPM-V/discussions'\n",
      " 'https://huggingface.co/openbmb/Eurus-70b-nca/discussions'\n",
      " 'https://huggingface.co/datasets/DIBT/10k_prompts_ranked/discussions'\n",
      " 'https://huggingface.co/facebook/flava-full/discussions'\n",
      " 'Feedback can be given via the feedback form on their website [segment-anything.com](https://segment-anything.com/) or by emailing at segment-anything at meta.com.'\n",
      " 'https://huggingface.co/spaces/facebook/MusicGen/discussions'\n",
      " 'https://huggingface.co/facebook/audiogen-medium/discussions'\n",
      " 'Feedback is encouraged from users to improve the model, but the feedback mechanism is not explicitly described.'\n",
      " 'https://huggingface.co/KT-AI/midm-bitext-S-7B-inst-v1/discussions'\n",
      " 'Email the authors [[Paper]](https://arxiv.org/pdf/2204.05862.pdf).\\n'\n",
      " 'Reviews on https://workspace.google.com/marketplace/app/claude_for_sheets/909417792257'\n",
      " 'https://huggingface.co/spaces/bigscience-data/roots-search/discussions'\n",
      " 'Point of Contact is [Victor Sanh](https://huggingface.co/datasets/bigscience/P3)'\n",
      " 'Point of Contact is [Niklas Muennighoff](https://huggingface.co/datasets/bigscience/xP3)'\n",
      " 'https://huggingface.co/bigscience/T0pp/discussions'\n",
      " 'https://huggingface.co/bigscience/bloom/discussions'\n",
      " 'https://huggingface.co/bigscience/bloomz/discussions'\n",
      " 'https://huggingface.co/VAGOsolutions/SauerkrautLM-7b-HerO/discussions'\n",
      " 'https://huggingface.co/Writer/palmyra-base/discussions'\n",
      " 'https://huggingface.co/Writer/camel-5b-hf/discussions'\n",
      " 'https://github.com/databrickslabs/dolly/issues'\n",
      " 'https://huggingface.co/databricks/dbrx-base/discussions'\n",
      " 'https://huggingface.co/Rakuten/RakutenAI-7B/discussions'\n",
      " 'https://huggingface.co/OpenBA/OpenBA-LM/discussions'\n",
      " 'https://huggingface.co/TheBloke/koala-7B-GPTQ-4bit-128g/discussions'\n",
      " 'https://huggingface.co/deepnight-research/saily_100b/discussions'\n",
      " 'https://huggingface.co/deepseek-ai/deepseek-llm-67b-base/discussions'\n",
      " 'https://huggingface.co/deepseek-ai/deepseek-llm-67b-chat/discussions'\n",
      " 'https://huggingface.co/deepseek-ai/deepseek-coder-33b-base/discussions'\n",
      " 'https://huggingface.co/berkeley-nest/Starling-LM-7B-alpha/discussions'\n",
      " 'https://huggingface.co/tiiuae/falcon-40b/discussions'\n",
      " 'https://huggingface.co/tiiuae/falcon-180b/discussions'\n",
      " 'https://huggingface.co/MSIIP/SALMONN/discussions'\n",
      " 'https://huggingface.co/ByteDance/SDXL-Lightning/discussions'\n",
      " 'Feedback can be provided in the CoPilot feedback project [[CoPilot feedback]] (https://github.com/github/feedback/discussions/categories/copilot-feedback).\\n'\n",
      " 'Feedback can be submitted at [bing.com](bing.com).\\n'\n",
      " 'https://huggingface.co/datasets/WizardLM/evol_instruct_70k/discussions'\n",
      " 'https://huggingface.co/WizardLM/WizardCoder-Python-34B-V1.0/discussions'\n",
      " 'https://huggingface.co/Open-Orca/LlongOrca-7B-16k/discussions'\n",
      " 'https://huggingface.co/microsoft/phi-1_5/discussions'\n",
      " 'https://huggingface.co/microsoft/Orca-2-13b/discussions'\n",
      " 'https://huggingface.co/microsoft/Phi-3-mini-128k-instruct/discussions'\n",
      " 'https://huggingface.co/TigerResearch/tigerbot-180b-base-v2/discussions'\n",
      " 'General feedback as well as the violations of the usage guidelines can be reported to Cohere at responsibility at cohere.ai [[Usage Guidelines]](https://docs.cohere.ai/usage-guidelines).\\n'\n",
      " 'https://huggingface.co/Cohere/Cohere-embed-english-v3.0/discussions'\n",
      " 'https://huggingface.co/CohereForAI/aya-101/discussions'\n",
      " 'https://huggingface.co/CohereForAI/c4ai-command-r-v01/discussions'\n",
      " 'https://huggingface.co/datasets/CohereForAI/aya_dataset/discussions'\n",
      " 'https://huggingface.co/zjunlp/OceanGPT-7b/discussions'\n",
      " 'Feedback can be provided on [[GitHub Issues]](https://github.com/tatsu-lab/stanford_alpaca/issues).'\n",
      " 'https://huggingface.co/datasets/math-ai/AutoMathText/discussions'\n",
      " 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO/discussions'\n",
      " 'https://huggingface.co/NousResearch/Yarn-Llama-2-70b-32k/discussions'\n",
      " 'https://huggingface.co/NousResearch/Nous-Capybara-34B/discussions'\n",
      " 'https://huggingface.co/NousResearch/Yarn-Mistral-7b-128k/discussions'\n",
      " 'https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B/discussions'\n",
      " 'https://huggingface.co/NousResearch/Hermes-2-Pro-Mistral-7B/discussions'\n",
      " 'https://huggingface.co/NousResearch/Genstruct-7B/discussions'\n",
      " 'https://huggingface.co/James-WYang/BigTrans/discussions'\n",
      " 'https://huggingface.co/wenge-research/yayi2-30b/discussions'\n",
      " 'https://huggingface.co/OrionZheng/openmoe-base/discussions'\n",
      " 'https://huggingface.co/BAAI/JudgeLM-13B-v1.0/discussions'\n",
      " 'https://huggingface.co/datasets/BAAI/JudgeLM-100K/discussions'\n",
      " 'https://huggingface.co/BAAI/bge-m3/discussions'\n",
      " 'https://huggingface.co/BAAI/EVA-CLIP-8B-448/discussions'\n",
      " 'https://huggingface.co/Qwen/Qwen1.5-72B/discussions'\n",
      " 'https://huggingface.co/Qwen/Qwen1.5-MoE-A2.7B/discussions'\n",
      " 'https://huggingface.co/SeaLLMs/SeaLLM-7B-v2.5/discussions'\n",
      " 'https://huggingface.co/datasets/open-web-math/open-web-math/discussions'\n",
      " 'https://huggingface.co/OrionStarAI/Orion-14B-Base/discussions'\n",
      " 'https://huggingface.co/sambanovasystems/SambaLingo-Arabic-Base/discussions'\n",
      " 'https://huggingface.co/SciPhi/SciPhi-Mistral-7B-32k/discussions'\n",
      " 'https://huggingface.co/argilla/notus-7b-v1/discussions'\n",
      " 'https://huggingface.co/LLM360/Amber/discussions'\n",
      " 'https://huggingface.co/LLM360/CrystalCoder/discussions']\n",
      "Column: model_card\n",
      "Unique values: [nan 'https://huggingface.co/time-series-foundation-models/Lag-Llama'\n",
      " 'https://huggingface.co/ibm-nasa-geospatial/Prithvi-100M'\n",
      " 'https://huggingface.co/cagliostrolab/animagine-xl-3.1'\n",
      " 'https://github.com/suno-ai/bark/blob/main/model-card.md'\n",
      " 'https://huggingface.co/togethercomputer/Llama-2-7B-32K-Instruct'\n",
      " 'https://huggingface.co/togethercomputer/StripedHyena-Hessian-7B'\n",
      " 'https://huggingface.co/togethercomputer/StripedHyena-Nous-7B'\n",
      " 'https://huggingface.co/epfl-llm/meditron-70b'\n",
      " 'https://huggingface.co/xverse/XVERSE-65B'\n",
      " 'https://github.com/Luodian/Otter/blob/main/docs/model_card.md'\n",
      " 'https://huggingface.co/GeneZC/MiniMA-3B'\n",
      " 'https://huggingface.co/internlm/internlm-20b'\n",
      " 'https://huggingface.co/BioMistral/BioMistral-7B'\n",
      " 'https://huggingface.co/GreenBitAI/LLaMA-30B-2bit-groupsize8'\n",
      " 'https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha'\n",
      " 'https://huggingface.co/HuggingFaceM4/idefics-80b-instruct'\n",
      " 'https://huggingface.co/TurkuNLP/gpt3-finnish-13B'\n",
      " 'https://huggingface.co/TurkuNLP/bloom-finnish-176b'\n",
      " 'https://huggingface.co/HuggingFaceM4/idefics2-8b'\n",
      " 'https://huggingface.co/DeepFloyd/IF-I-XL-v1.0'\n",
      " 'https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt'\n",
      " 'https://huggingface.co/stabilityai/stablelm-2-1_6b'\n",
      " 'https://huggingface.co/stabilityai/stable-cascade'\n",
      " 'https://huggingface.co/stabilityai/sv3d'\n",
      " 'https://huggingface.co/FuseAI/FuseChat-7B-VaRM'\n",
      " 'https://huggingface.co/moreh/MoMo-72B-lora-1.8.7-DPO'\n",
      " 'https://huggingface.co/mistralai/Mistral-7B-v0.1'\n",
      " 'https://huggingface.co/cognitivecomputations/dolphin-2_2-yi-34b'\n",
      " 'https://huggingface.co/cognitivecomputations/WizardLM-30B-Uncensored'\n",
      " 'https://huggingface.co/vilm/vulture-180b'\n",
      " 'https://deci.ai/model-zoo/decilm-7b/'\n",
      " 'https://mystic.the-eye.eu/public/AI/models/GPT-NeoX-20B/20B_model_card.md'\n",
      " 'https://huggingface.co/EleutherAI/pythia-12b'\n",
      " 'https://huggingface.co/EleutherAI/llemma_34b'\n",
      " 'https://huggingface.co/openbmb/UltraLM-13b'\n",
      " 'https://huggingface.co/NinedayWang/PolyCoder-2.7B'\n",
      " 'https://huggingface.co/OpenAssistant/llama2-70b-oasst-sft-v10'\n",
      " 'https://huggingface.co/RWKV/rwkv-4-world-7b'\n",
      " 'https://huggingface.co/RWKV/rwkv-4-14b-pile'\n",
      " 'https://huggingface.co/RWKV/rwkv-5-world-3b'\n",
      " 'https://huggingface.co/amazon/FalconLite2'\n",
      " 'https://huggingface.co/amazon/chronos-t5-large'\n",
      " 'https://huggingface.co/NucleusAI/nucleus-22B-token-500B'\n",
      " 'https://huggingface.co/IDEA-CCNL/Taiyi-Stable-Diffusion-XL-3.5B'\n",
      " 'https://huggingface.co/MayaPH/GodziLLa2-70B'\n",
      " 'https://huggingface.co/apple/OpenELM-3B-Instruct'\n",
      " 'https://huggingface.co/bigcode/starcoder'\n",
      " 'https://huggingface.co/bigcode/santacoder'\n",
      " 'https://huggingface.co/bigcode/starcoder2-15b'\n",
      " 'https://huggingface.co/bigcode/starcoder2-7b'\n",
      " 'https://huggingface.co/bigcode/starcoder2-3b'\n",
      " 'https://huggingface.co/h2oai/h2ogpt-oasst1-512-20b'\n",
      " 'https://huggingface.co/h2oai/h2o-danube-1.8b-base'\n",
      " 'https://huggingface.co/t5-base' 'unknown'\n",
      " 'https://arxiv.org/pdf/2210.11416.pdf'\n",
      " 'https://arxiv.org/pdf/2204.02311.pdf#appendix.E'\n",
      " 'https://ai.google/static/documents/palm2techreport.pdf'\n",
      " 'https://cloud.google.com/static/vertex-ai/docs/generative-ai/medlm/MedLM-model-card.pdf'\n",
      " 'https://huggingface.co/google/gemma-7b'\n",
      " 'https://huggingface.co/Skywork/Skywork-13B-base'\n",
      " 'https://huggingface.co/kotoba-tech/kotoba-speech-v0.1'\n",
      " 'https://github.com/openai/gpt-2/blob/master/model_card.md'\n",
      " 'https://github.com/openai/gpt-3/blob/master/model-card.md'\n",
      " 'https://github.com/openai/following-instructions-human-feedback/blob/main/model-card.md'\n",
      " 'https://github.com/openai/whisper/blob/main/model-card.md'\n",
      " 'https://github.com/openai/CLIP/blob/main/model-card.md'\n",
      " 'https://github.com/openai/DALL-E/blob/master/model_card.md'\n",
      " 'https://github.com/openai/dalle-2-preview/blob/main/system-card.md'\n",
      " 'https://huggingface.co/garage-bAInd/Platypus2-13B'\n",
      " 'https://arxiv.org/pdf/2204.14198.pdf#appendix.E'\n",
      " 'https://arxiv.org/pdf/2112.11446.pdf#appendix.B'\n",
      " 'https://arxiv.org/pdf/2203.15556.pdf'\n",
      " 'https://openreview.net/pdf?id=1ikK0kHjvj#appendix.B'\n",
      " 'https://huggingface.co/allenai/cosmo-xl'\n",
      " 'https://huggingface.co/allenai/tulu-2-70b'\n",
      " 'https://huggingface.co/allenai/tulu-2-dpo-70b'\n",
      " 'https://huggingface.co/allenai/codetulu-2-13b'\n",
      " 'https://huggingface.co/allenai/OLMo-7B'\n",
      " 'https://huggingface.co/cerebras/Cerebras-GPT-13B'\n",
      " 'https://inceptioniai.org/jais/docs/Technicalpaper.pdf'\n",
      " 'https://huggingface.co/cerebras/btlm-3b-8k-base'\n",
      " 'https://huggingface.co/Salesforce/blip2-opt-2.7b'\n",
      " 'https://huggingface.co/Salesforce/moirai-1.0-R-large'\n",
      " 'https://huggingface.co/ai21labs/Jamba-v0.1'\n",
      " 'https://huggingface.co/Xwin-LM/Xwin-LM-70B-V0.1'\n",
      " 'https://huggingface.co/01-ai/Yi-34B'\n",
      " 'https://huggingface.co/01-ai/Yi-VL-34B'\n",
      " 'https://huggingface.co/OpenLemur/lemur-70b-v1'\n",
      " 'https://huggingface.co/OpenLemur/lemur-70b-chat-v1'\n",
      " 'https://huggingface.co/adept/fuyu-8b'\n",
      " 'https://huggingface.co/openbmb/cpm-bee-10b'\n",
      " 'https://huggingface.co/openbmb/MiniCPM-V'\n",
      " 'https://huggingface.co/openbmb/Eurus-70b-nca'\n",
      " 'https://huggingface.co/facebook/flava-full'\n",
      " 'https://huggingface.co/facebook/galactica-6.7b'\n",
      " 'https://arxiv.org/pdf/2205.01068.pdf'\n",
      " 'Can be found at appendix of paper at https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/'\n",
      " 'https://arxiv.org/pdf/2304.02643.pdf#page=28'\n",
      " 'https://github.com/facebookresearch/audiocraft/blob/main/model_cards/MUSICGEN_MODEL_CARD.md'\n",
      " 'https://github.com/facebookresearch/audiocraft/blob/main/model_cards/AUDIOGEN_MODEL_CARD.md'\n",
      " 'https://huggingface.co/codellama/CodeLlama-34b-hf'\n",
      " 'https://huggingface.co/facebook/metaclip-b32-400m'\n",
      " 'https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md'\n",
      " 'https://huggingface.co/KT-AI/midm-bitext-S-7B-inst-v1'\n",
      " 'https://www-files.anthropic.com/production/images/Model-Card-Claude-2.pdf'\n",
      " 'https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf'\n",
      " 'https://huggingface.co/bigscience/T0pp'\n",
      " 'https://huggingface.co/bigscience/bloomz'\n",
      " 'https://huggingface.co/CausalLM/14B'\n",
      " 'https://huggingface.co/VAGOsolutions/SauerkrautLM-7b-HerO'\n",
      " 'https://huggingface.co/Writer/palmyra-base'\n",
      " 'https://huggingface.co/Writer/camel-5b-hf'\n",
      " 'https://huggingface.co/databricks/dbrx-base'\n",
      " 'https://huggingface.co/lmsys/vicuna-13b-delta-v0'\n",
      " 'https://huggingface.co/Rakuten/RakutenAI-7B'\n",
      " 'https://huggingface.co/OpenBA/OpenBA-LM'\n",
      " 'https://huggingface.co/TheBloke/koala-7B-GPTQ-4bit-128g'\n",
      " 'https://huggingface.co/deepnight-research/saily_100b'\n",
      " 'https://huggingface.co/deepseek-ai/deepseek-llm-67b-base'\n",
      " 'https://huggingface.co/deepseek-ai/deepseek-llm-67b-chat'\n",
      " 'https://huggingface.co/deepseek-ai/deepseek-coder-33b-base'\n",
      " 'https://huggingface.co/berkeley-nest/Starling-LM-7B-alpha'\n",
      " 'https://huggingface.co/tiiuae/falcon-40b'\n",
      " 'https://huggingface.co/tiiuae/falcon-180B'\n",
      " 'https://github.com/mlfoundations/open_flamingo/blob/main/MODEL_CARD.md'\n",
      " 'https://huggingface.co/MSIIP/SALMONN'\n",
      " 'https://huggingface.co/ByteDance/SDXL-Lightning'\n",
      " 'https://huggingface.co/WizardLM/WizardLM-13B-1.0'\n",
      " 'https://huggingface.co/WizardLM/WizardCoder-Python-34B-V1.0'\n",
      " 'https://huggingface.co/Open-Orca/LlongOrca-7B-16k'\n",
      " 'https://huggingface.co/microsoft/phi-1_5'\n",
      " 'https://huggingface.co/microsoft/Orca-2-13b'\n",
      " 'https://huggingface.co/microsoft/Phi-3-mini-128k-instruct'\n",
      " 'https://huggingface.co/TigerResearch/tigerbot-180b-base-v2'\n",
      " 'https://docs.cohere.ai/generation-card'\n",
      " 'https://docs.cohere.ai/representation-card'\n",
      " 'https://huggingface.co/Cohere/Cohere-embed-english-v3.0'\n",
      " 'https://huggingface.co/CohereForAI/aya-101'\n",
      " 'https://huggingface.co/CohereForAI/c4ai-command-r-v01'\n",
      " 'https://x.ai/model-card/' 'https://huggingface.co/zjunlp/OceanGPT-7b'\n",
      " 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO'\n",
      " 'https://huggingface.co/NousResearch/Yarn-Llama-2-70b-32k'\n",
      " 'https://huggingface.co/NousResearch/Nous-Capybara-34B'\n",
      " 'https://huggingface.co/NousResearch/Yarn-Mistral-7b-128k'\n",
      " 'https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B'\n",
      " 'https://huggingface.co/NousResearch/Hermes-2-Pro-Mistral-7B'\n",
      " 'https://huggingface.co/NousResearch/Genstruct-7B'\n",
      " 'https://huggingface.co/James-WYang/BigTrans'\n",
      " 'https://huggingface.co/wenge-research/yayi2-30b'\n",
      " 'https://huggingface.co/OrionZheng/openmoe-base'\n",
      " 'https://huggingface.co/BAAI/JudgeLM-13B-v1.0'\n",
      " 'https://huggingface.co/BAAI/bge-m3'\n",
      " 'https://huggingface.co/BAAI/EVA-CLIP-8B-448'\n",
      " 'https://huggingface.co/Qwen' 'https://huggingface.co/Qwen/Qwen1.5-72B'\n",
      " 'https://huggingface.co/Qwen/Qwen1.5-MoE-A2.7B'\n",
      " 'https://huggingface.co/SeaLLMs/SeaLLM-7B-v2.5'\n",
      " 'https://huggingface.co/OrionStarAI/Orion-14B-Base'\n",
      " 'https://huggingface.co/sambanovasystems/SambaLingo-Arabic-Base'\n",
      " 'https://huggingface.co/SciPhi/SciPhi-Mistral-7B-32k'\n",
      " 'https://huggingface.co/argilla/notus-7b-v1'\n",
      " 'https://huggingface.co/LLM360/Amber'\n",
      " 'https://huggingface.co/LLM360/CrystalCoder']\n",
      "Column: training_emissions\n",
      "Unique values: [nan 'unknown' '11 tCO2eq' '2,276 kgCO2eq' '31.73 tCO2e'\n",
      " '16.68 tons of CO2eq' '124 kg of CO2eq' '29,622.83 kgCO2eq'\n",
      " '16,107.01 kgCO2eq' '26 tCO2e' 'Unknown' '271.43 tCO2' '552.1 tCO2e'\n",
      " '380 tCO2e' '75.05 tCo2eq' '75 tCO2e' '539 tCO2eq'\n",
      " '2.8 metric tons of carbon dioxide' '65.3 tCO2eq' '0.9 tCO2e' '25 tCO2e'\n",
      " '6.5 tCO2eq']\n",
      "Column: training_time\n",
      "Unique values: [nan 'unknown' 'Approximately 15 days, totaling over 350 GPU hours.'\n",
      " '50,000 GPU hours' '2.5 to 5 days' '54 hours' '4 days' '48 days'\n",
      " '92k GPU hours' '24,602 A100 GPU hours' 'few months' '3 days'\n",
      " '3000 A100 hours' '47.10 petaflop/s-day' 'Less than 1 V100-hour'\n",
      " '47k A100 hours' '2 million steps' '4096 A100 days' '6 weeks'\n",
      " '18 days according to [[the paper]](https://arxiv.org/abs/2210.15257)'\n",
      " '63 hours on p4d.24xlarge EC2 instance' 'less than 9 hours' '15 days'\n",
      " '35 days' '1 week' '2 weeks' '320,256 GPU hours' '14,284 GPU hours'\n",
      " '145,152 hours (cumulative)' '97,120 hours (cumulative)'\n",
      " '4108.80 petaflop/s-day' 'Unknown' '29600 petaflop/s-days' '130.4 days'\n",
      " '39 days' '3640 petaflop/s-days' '100-1000 petaflop/s-days'\n",
      " '60 petaflops/s-days' '71.12 petaflop/s-day' '4 weeks' '53 days'\n",
      " '5 hours' '11 days' '15 days on 1536 TPUs' '7303.24 petaflop/s-day'\n",
      " '4 days on a 16x16 TPU v3 slice' 'less than 9 days' 'Several months'\n",
      " '9.5 days' '6.79 days' '10 days'\n",
      " '24 days, according to [[the paper]](https://arxiv.org/pdf/2204.05999.pdf)'\n",
      " '68 hours' '750,000 iterations' '400K GPU hours' '27 hours'\n",
      " '7039 petaflop/s-days' '30 minutes' '3 months' '1 day' '38k GPU hours'\n",
      " '6 hours' '2 months' '9 months' 'Less than two weeks'\n",
      " '10,000 steps in 7 hours' '70 hours on 3 epochs' '37 hours' '8 days'\n",
      " '80 hours' '7 days' '60k training steps per day' '84 days' '13 days'\n",
      " '1000 epochs' '20,000 steps']\n",
      "Column: training_hardware\n",
      "Unique values: [nan 'A single NVIDIA Tesla-P100 GPU' 'unknown' '2x A100 80GB GPUs'\n",
      " '4 RTX-3090 GPUs' '8 A100 80G GPUs' 'Single A100 NVIDIA GPU'\n",
      " '8 A100 GPUs' 'A single 24 GB GPU' '4 80GB NVIDIA A40 GPUs'\n",
      " '32 NVIDIA A100 80GB GPUs' '32 A100 80GB GPUs'\n",
      " 'LUMI supercomputer, using 128 AMD MI250X GPUs' '16 x A100 (40GB)'\n",
      " '192 nodes, each consisting of 4 AMD Instinct MI250X GPUs, a single 64-core AMD Trento CPU and 512GB of memory.'\n",
      " '512 NVIDIA A100 40GB GPUs' 'Some number of A100 GPUs'\n",
      " 'thousands of GPUs' 'AMD’s MI250 GPU' '4 A100 GPUs' 'NVIDIA A10 GPUs'\n",
      " 'TRC (Unspecified # of TPU v3-8s)' '12 x 8 A100 GPUs'\n",
      " '1 NVIDIA Tesla K80 GPU' '64 A100 GPUs' '256 A100 40GB GPUs'\n",
      " 'THUDM 1536 Ascend 910 (32GB) Cluster' 'THUDM 96 DGX-A100 (40G) cluster'\n",
      " '8 NVIDIA RTX 8000' 'Single A6000 GPU' '5000 NVIDIA H100 GPUs'\n",
      " 'Baidu V100 Cluster, PengCheng Lab Ascend 910 NPU cluster'\n",
      " '320 A100 GPUs according to [[the paper]](https://arxiv.org/abs/2210.15257)'\n",
      " '8 NVIDIA A100 40G GPUs' '32 A100 GPUs' '32 NVIDIA A800 80GB GPUs'\n",
      " '256 NVIDIA A100 GPUs for 32 days, and 64 GPUs for 3 days'\n",
      " '32 80G NVIDIA A100 GPUs' '10 NVIDIA A5000 GPUs'\n",
      " '512 A100 80GB GPUs distributed across 64 nodes'\n",
      " '96 NVIDIA Tesla V100 GPUs' '1024 x H100 GPUs' '432 H100 GPUs'\n",
      " '160 A100 GPUs' 'unspecified number of 48GB A100 NVIDIA GPUs'\n",
      " '8x H100 GPUs on a single node' '1,024 TPU v3 chips (Cloud TPU Pods)'\n",
      " '1024 TPU-V3 chips' '512 v4 TPU Chips' '128 TPUv4' '128 TPU-v4'\n",
      " '256 TPU-v3' '6144 TPU v4 chips' 'TPU v4 (number unspecified)' 'TPUv5e'\n",
      " '1024 A100 GPUs' '512 A800-80GB GPUs' 'Azure' 'NVIDIA V100 GPUs'\n",
      " '510 V100s'\n",
      " '64 Amazon EC2 p4d.24xlarge instances each with 8 NVIDIA 40GB A100 GPUs (i.e. total 512 A100 GPUs)'\n",
      " '1 A100 GPU' '128 TPUv3 cores' 'TPU' 'TPUv3 pods' 'TPUv3/TPUv4 pods'\n",
      " '16x16 TPU v3 slice' 'v3-128 TPU accelerators with batch size 256'\n",
      " '27 nodes, with each node containing 8x NVIDIA A100-40GB GPUs provided by MosaicML'\n",
      " '16x Cerebras CS-2 wafer scale systems' 'Condor Galaxy Supercomputer'\n",
      " 'Condor Galaxy Supercomputer from Cerebras'\n",
      " 'Unspecified Salesforce Compute (TPU-V4s)' 'NVIDIA A100 40G GPUs'\n",
      " 'Over 800 A100 GPUs' '440 A100 40GB GPUs' '128 A100 NVIDIA GPUs'\n",
      " '128 NVIDIA A800 (80G) GPUs' 'TPUv4-512 pod'\n",
      " 'Meta AI Cluster. Trained on 1024 80GB A100 GPUs (128 8xA100 80GB nodes)'\n",
      " '248 V100 GPUs, according to [[the paper]](https://arxiv.org/pdf/2204.05999.pdf)'\n",
      " 'Meta AI cluster. Trained on 992 80GB A100 GPUs'\n",
      " 'NVIDIA A100-80GB GPUs (TDP of 350-400W)' '256 A100 GPUs'\n",
      " '32 GPUs of unspecified type' '64 GPUs' 'A100-80GB GPUs'\n",
      " '2 custom-built Meta 24K GPU clusters' 'Jean Zay (v3-512)'\n",
      " 'Jean Zay (48 * 8xA100 80GB nodes)'\n",
      " 'Jean Zay (288 A100 80GB GPUs with 8 GPUs per node (36 nodes) using NVLink 4 inter-gpu connects, 4 OmniPath links)'\n",
      " 'A single NDasrA100_v4 machine with 8x A100 40GB GPUs'\n",
      " '3072 NVIDIA H100s connected by 3.2Tbps Infiniband'\n",
      " '8 NVIDIA A100-80GB GPUs' '8 NVIDIA A100 GPUs and 8 NVIDIA H800 GPUs'\n",
      " '384 A100 40GB GPUs' '4096 A100 40GB GPUs' '64 A100 80G GPUs' '256 A100'\n",
      " '4480 A100s (560 x 8)' '16 V100 32GB GPUs'\n",
      " '8 NVIDIA Tesla V100 32GB GPUs' '8 V100 GPUs'\n",
      " '8x A6000-48GB (first-gen) GPUs' '32 A100-40G GPUs' '512 H100-80G GPUs'\n",
      " '6 A800 NVIDIA GPUs' 'one NVIDIA A40 GPU' '3072 A100 GPUs'\n",
      " '3072 H100 80GB SXM5 GPUs across 384 DGX H100 nodes'\n",
      " '16 A100 GPUs with 80 GB of RAM' 'over 1000 A800 GPUs'\n",
      " 'Yandex 800 A100 Cluster' '24 GB VRAM GPU' '1024 NVIDIA A800 GPUs'\n",
      " '8 A100 40GB NVIDIA GPUs' '4 NVIDIA A100 GPUs' '32 A100 40GB GPUs'\n",
      " '8 x A100 40GB GPUs'\n",
      " '56 DGX A100 nodes, each equipped with 4 80GB A100 GPUs'\n",
      " 'Trained on the Cerebras Condor Galaxy 1 (CG-1), a 4 exaFLOPS, 54 million core, 64-node cloud AI supercomputer.']\n",
      "Column: adaptation\n",
      "Unique values: [nan 'unknown'\n",
      " 'GPT-4 adapted to run autonomously by chaining together LLM \"thoughts\"'\n",
      " 'Fine-tuning'\n",
      " 'The API exposes the models fairly direclty with a range of hyperparameters (e.g. temperature scaling).'\n",
      " 'Customized GPT-3, fine-tuned on private data [[Sana GPT-3 Demo]](https://gpt3demo.com/apps/sanalabs).\\n'\n",
      " 'The Playground provides direct access to the language models (Complete API) as well as wrapped for Rewrite and Summarize.'\n",
      " 'The AI21 language models are further specialized to the task of paraphrasing.'\n",
      " 'The AI21 language models are further specialized to the task of summarization.'\n",
      " \"Security Copilot combines OpenAI's GPT-4 generative AI with a security-specific model from Microsoft. This security-specific model in turn incorporates a growing set of security-specific skills and is informed by Microsoft's unique global threat intelligence and more than 65 trillion daily signals.\"]\n",
      "Column: output_space\n",
      "Unique values: [nan 'deployed AI models' 'generative AI apps'\n",
      " 'Question and answer, summarization, sentiment analysis, topic identification'\n",
      " 'text' 'image' 'data analyses'\n",
      " 'Chatbot output in response to user queries'\n",
      " 'Text Generation, Text Completion' 'natural language text responses'\n",
      " 'foundation models made accessible via an API'\n",
      " 'AI-generated chat conversations' 'generated images' 'web page ranking'\n",
      " 'Dialogue' 'AI-generated creations' 'job candidate matches'\n",
      " 'Given a prompting text, the OpenAI API provides access to text completions, and log probabilities. The support for text and code embeddings were added on 2022-01-25 [[OpenAI Blog Post]] (https://openai.com/blog/introducing-text-and-code-embeddings/).\\n'\n",
      " 'natural language text guidance'\n",
      " 'question and answer, summarization, sentiment analysis,topic identification'\n",
      " 'Generation'\n",
      " 'UI allowing users to indicate their preference for the model responses shown.\\n'\n",
      " 'AI-generated text from prompt' 'text and code' 'Code completions'\n",
      " 'Search results' 'Document level insights for users.' 'Suggested emails.'\n",
      " 'Actionable responses to security-related questions (text and image). Security event, incident or threat reports (PowerPoint slide).'\n",
      " 'generation and embeddings' 'generation' 'embedding'\n",
      " 'The text models provide text outputs given text inputs. The multimodal models provide text completions given text and image inputs.']\n",
      "Column: terms_of_service\n",
      "Unique values: [nan\n",
      " 'https://www.ibm.com/docs/en/watsonx-as-a-service?topic=models-terms-use'\n",
      " 'https://portkey.ai/terms' 'https://www.askviable.com/terms-of-service'\n",
      " 'https://stability.ai/terms-of-use'\n",
      " 'https://mistral.ai/terms/#terms-of-use' 'hhttps://re.express/tos.html'\n",
      " 'https://blog.perplexity.ai/legal/terms-of-service'\n",
      " 'https://goose.ai/docs/tos' 'https://quizlet.com/tos'\n",
      " 'https://aws.amazon.com/service-terms/' 'https://beta.character.ai/tos'\n",
      " 'https://policies.google.com/terms' 'unknown'\n",
      " 'https://www.adobe.com/legal/licenses-terms/adobe-gen-ai-user-guidelines.html'\n",
      " 'https://www.moonhub.ai/terms' 'https://openai.com/api/policies/terms/'\n",
      " 'https://openai.com/policies/terms-of-use'\n",
      " 'https://www.sanalabs.com/legal/' 'https://neeva.com/terms'\n",
      " 'https://www.ai21.com/terms-of-use'\n",
      " 'https://www.wordtune.com/terms-of-use'\n",
      " 'https://play.aidungeon.io/main/termsOfService'\n",
      " 'https://www.assemblyai.com/legal/terms-of-service'\n",
      " 'https://hyperwriteai.com/terms' 'https://claude.ai/legal'\n",
      " 'https://www.transformify.ai/legal-stuff'\n",
      " 'https://www.spotify.com/us/legal/end-user-agreement/'\n",
      " 'https://poe.com/tos' 'https://snap.com/terms'\n",
      " 'https://www.brex.com/legal/user-terms'\n",
      " 'https://docs.github.com/en/site-policy/github-terms/github-terms-of-service\\n'\n",
      " 'https://www.microsoft.com/legal/terms-of-use'\n",
      " 'https://azure.microsoft.com/en-us/support/legal/'\n",
      " 'https://cohere.ai/terms-of-use'\n",
      " 'https://yandex.com/legal/browser_agreement/'\n",
      " 'https://github.com/continuedev/continue/blob/main/LICENSE'\n",
      " 'https://www.aleph-alpha.com/terms-conditions'\n",
      " 'https://www.robinai.co.uk/terms' 'https://www.duolingo.com/terms']\n",
      "Column: monthly_active_users\n",
      "Unique values: [nan 'unknown' '100M'\n",
      " 'GitHub Copilot reportedly has over 1 million sign-ups [[Tweet Source]](https://twitter.com/sama/status/1539737789310259200?s=21&t=YPaYd0ZueJzrR6rLslUqzg).\\n']\n",
      "Column: user_distribution\n",
      "Unique values: [nan 'unknown' 'crowdworkers']\n",
      "Column: failures\n",
      "Unique values: [nan 'unknown']\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loop through each DataFrame and print unique values\n",
    "unique_values = get_unique_values(df)\n",
    "for column, values in unique_values.items():\n",
    "    print(f\"Column: {column}\")\n",
    "    print(f\"Unique values: {values}\")\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary data exported to summary_data.json\n"
     ]
    }
   ],
   "source": [
    "# Convert summary to JSON format\n",
    "json = df.to_json(orient='records', date_format='iso')\n",
    "\n",
    "# Save JSON to a file\n",
    "with open('LLM.json', 'w') as f:\n",
    "    f.write(json)\n",
    "\n",
    "print(\"Summary data exported to summary_data.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Digital_Futures_Learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
